{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from bounce.utils import state2str, array_in_list, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "> Deffinition of the different kind of agents that can be implemented. The agents are the entities that perform the learning process to, ultimately, obtain the optimal set of constraints. \n",
    "\n",
    "We consider agents based on different techniques of artificial intelligence: \n",
    "- Agents based on reinforcement learning (Q-learning)\n",
    "- Agents based on blind-search (breadth first search)\n",
    "- Agents based on Monte-Carlo sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "The agents based on reinforcement learning implement a value-based algorithm called Q-learning. More precisely, the agent implemented in this framework is based on [deep double Q-learning](https://ojs.aaai.org/index.php/AAAI/article/view/10295)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DQNAgent:\n",
    "    def __init__(self, model, learning_rate=1e-3, criterion=None, optimizer=None, batch_size=128, \n",
    "                 target_update=5, gamma=0.85, eps_0=1, eps_decay=0.999, eps_min=0.1):       \n",
    "        \"\"\"Agent based on a deep Q-Network (DQN):\n",
    "        Input: \n",
    "            - model: torch.nn.Module with the DQN model. Dimensions must be consistent\n",
    "            - criterion: loss criterion (e.g., torch.nn.SmoothL1Loss)\n",
    "            - optimizer: optimization algorithm (e.g., torch.nn.Adam)\n",
    "            - eps_0: initial epsilon value for an epsilon-greedy policy\n",
    "            - eps_decay: exponential decay factor for epsilon in the epsilon-greedy policy\n",
    "            - eps_min: minimum saturation value for epsilon\n",
    "            - gamma: future reward discount factor for Q-value estimation\"\"\"   \n",
    "        \n",
    "        # Model\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self._build_target_net()\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Parameters\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma    # discount factor\n",
    "        self.epsilon, self.epsilon_min, self.epsilon_decay = eps_0, eps_min, eps_decay   \n",
    "        self._get_criterion(criterion)\n",
    "        self._get_optimizer(optimizer)\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=10000) # Replay memory\n",
    "        self.transition = namedtuple('Transition', ('state', 'action', 'bound', 'cost', 'next_state'))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"Given a state, return actions ordered by priority.\"\n",
    "\n",
    "        if torch.rand(1) <= self.epsilon:\n",
    "            return torch.randperm(self.state_size)\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                Q = self.q_values(state)\n",
    "                return torch.argsort(Q, descending=True)\n",
    "            \n",
    "    def q_values(self, state):\n",
    "        \"Returns the Q values of each action given a state.\"\n",
    "        state = T(state).reshape(1, self.state_size).to(self.device)\n",
    "        return self.model(state).squeeze()\n",
    "             \n",
    "    def replay(self, env):\n",
    "        \"Learn from past events in the memory.\"\n",
    "        batch_size = min(len(self.memory), self.batch_size)\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = self.transition(*zip(*transitions))    \n",
    "        \n",
    "        state_batch, action_batch = torch.cat(batch.state), torch.cat(batch.action)\n",
    "        next_states = torch.cat(batch.next_state) \n",
    "        bound_batch, cost_batch = torch.cat(batch.bound), torch.cat(batch.cost)\n",
    "        reward_batch = env.reward_fun(bound_batch, cost_batch)\n",
    "        \n",
    "        if torch.isnan(reward_batch).any(): \n",
    "            shit_idx = torch.isnan(reward_batch)\n",
    "            raise ValueError(f\"Shit went wrong, got nan in {next_states[shit_idx]}\"+\n",
    "                             f\"\\nBound {bound_batch[shit_idx]}\"+\n",
    "                             f\"\\nCost {cost_batch[shit_idx]}\"+\n",
    "                             f\"\\nReward {reward_batch[shit_idx]}\"+\n",
    "                             f\"\\nReward batch {reward_batch}\")\n",
    "\n",
    "        # Q-values\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch.reshape(batch_size, 1))\n",
    "        # Expected Q-values\n",
    "        next_state_values = self.target_net(next_states).max(1)[0].detach()*self.gamma + reward_batch\n",
    "                \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(state_action_values, next_state_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
    "        if self.epsilon < self.epsilon_min: self.epsilon = self.epsilon_min\n",
    "            \n",
    "    def memorize(self, state, action, bound, cost, next_state):  \n",
    "        \"Remember a state-action-state-reward transition.\"\n",
    "        info = [torch.FloatTensor(state).reshape(1, self.state_size).to(self.device), \n",
    "                torch.tensor([action], device = self.device), \n",
    "    #                 torch.FloatTensor([reward]).to(self.device), \n",
    "                torch.FloatTensor([bound]).to(self.device),\n",
    "                torch.FloatTensor([cost]).to(self.device),\n",
    "                torch.FloatTensor(next_state).reshape(1, self.state_size).to(self.device)\n",
    "                ]\n",
    "        self.memory.append(self.transition(*info))\n",
    "            \n",
    "    def _build_target_net(self):\n",
    "        model_params = list(self.model.parameters())\n",
    "        self.state_size = model_params[0].size()[-1]\n",
    "        self.action_size = model_params[-1].size()[0]\n",
    "        self.target_net = self.model.__class__(self.state_size, self.action_size).to(self.device)   \n",
    "        self.target_net.load_state_dict(self.model.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "    def _get_criterion(self, criterion=None):\n",
    "        if criterion is None: self.criterion = nn.SmoothL1Loss(reduction='sum')\n",
    "        else:                 self.criterion = criterion\n",
    "            \n",
    "    def _get_optimizer(self, optimizer=None):\n",
    "        if optimizer is None: self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        else:                 self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a default architecture for the neural network that encodes the Q-values, usually referred to as deep Q-Network (DQN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 3*state_size)\n",
    "        self.fc2 = nn.Linear(3*state_size, 2*action_size)\n",
    "        self.fc3 = nn.Linear(2*action_size, 2*action_size)\n",
    "        self.fc4 = nn.Linear(2*action_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind-search\n",
    "\n",
    "The agents based on tree search currently only implement blind-search techniques, such as breadth first search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BrFSAgent:\n",
    "    def __init__(self, initial_state):\n",
    "        \"Agent based on Breadth First Search (BrFS).\"\n",
    "        self.state_size = len(initial_state)\n",
    "        self.open = deque([initial_state])\n",
    "        self.closed = set()\n",
    "        \n",
    "    def expand(self):\n",
    "        \"Expands the first node of the open\"\n",
    "        try :\n",
    "            state = self.open.popleft()\n",
    "            self.add_closed(state)\n",
    "            return state, np.random.permutation(self.state_size)\n",
    "        except:\n",
    "            return [], []\n",
    "    \n",
    "    def in_open(self, state):\n",
    "        \"Boolean indicating whether state is in open\"\n",
    "        return array_in_list(state, self.open)\n",
    "    \n",
    "    def in_closed(self, state):\n",
    "        \"Boolean indicating whether state is in closed\"\n",
    "        return state2str(state) in self.closed\n",
    "    \n",
    "    def add_open(self, state): \n",
    "        \"Adds state to open\"\n",
    "        self.open.append(state)\n",
    "    \n",
    "    def add_closed(self, state):\n",
    "        \"Adds state to closed\"\n",
    "        self.closed.update([state2str(state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BrFSAgent(3, np.array([1, 1, 1, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 1, 0, 0, 0]),\n",
       " array([1, 0, 1, 0, 0, 0]),\n",
       " array([1, 1, 0, 0, 0, 0]),\n",
       " array([1, 1, 1, 1, 0, 0]),\n",
       " array([1, 1, 1, 0, 1, 0]),\n",
       " array([1, 1, 1, 0, 0, 1])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.expand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo\n",
    "\n",
    "The agents based on Monte-Carlo sampling follow the Metropolis-Hastings algorithm to move between states. A random action (new state) is proposed and the move is accepted or rejected with a certain probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MCAgent:\n",
    "    def __init__(self, beta=0.1):\n",
    "        self.beta = beta\n",
    "        self.accepted = 0\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Try random actions.\"\n",
    "        return np.random.permutation(len(state))\n",
    "    \n",
    "    def accept(self, r1, r2):\n",
    "        \"Boolean indicating whether to accept or not the movement from 1 to 2.\"\n",
    "        accept = np.random.random() <= min(1, np.exp((r2-r1)/self.beta))\n",
    "        self.accepted += accept\n",
    "        return accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
