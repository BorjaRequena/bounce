{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from bounce.utils import (state2int, state2str, add_subgraph_size, simplify_layout, \n",
    "                          fill_layout, dist_poly, get_memory_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "> Definition of the environment with which the agent interacts. The environment handles the execution of the actions and provides the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SdPEnvironment:\n",
    "    \"Environment for constraint-space exploration.\"\n",
    "    \n",
    "    def __init__(self, problem, sdp_solver, budget, reward_criterion=\"bound_norm\", \n",
    "                 layout_basis=None, max_basis_size=None, initial_size=1, bound_tol=1e-3):\n",
    "        \n",
    "        self.N = problem.graph.n_nodes\n",
    "        self.problem = problem\n",
    "        self.solver = sdp_solver\n",
    "        self.budget = budget\n",
    "        \n",
    "        # Agent state-action basis         \n",
    "        self.layout_basis = (self._get_layout_basis(max_basis_size) \n",
    "                             if layout_basis is None else layout_basis)\n",
    "        self.contained_map = self._get_contained_map()\n",
    "        self.basis_sizes = np.array([l.shape[0] for l in self.layout_basis])\n",
    "        \n",
    "        # Reward function\n",
    "        self.reward_fun = getattr(self, reward_criterion+\"_reward\")\n",
    "        self.dist_d = 5\n",
    "        \n",
    "        # Memory of visited states. It is a lookup table for computation speedup.\n",
    "        self.memory_limit = 2e6\n",
    "        self._get_memory()\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.initial_size = initial_size\n",
    "        self.reset()        \n",
    "        \n",
    "        # Memories           \n",
    "        self.bound_tol = bound_tol\n",
    "        self.max_bound = -np.inf   # Maximum bound ever obtained\n",
    "        self.min_bound = np.inf    # Minimum bound ever obtained\n",
    "        self.max_cost  = -np.inf   # Maximum amount of costs ever obtained\n",
    "        self.min_cost  = np.inf    # Minimum amount of costs ever obtained\n",
    "        self.best = np.array([-np.inf, np.inf, -np.inf])  # Maximum bound, best and worst cost\n",
    "        self.best_layout = deepcopy(self.layout)\n",
    "        \n",
    "        # Initial state reference\n",
    "        bound, cost, err = self.get_values()\n",
    "        if not err: self._min_max_update(bound, cost)\n",
    "        else:       raise ValueError(f\"Something went wrong. Initial state {self.state} provides error.\")\n",
    "        \n",
    "        \n",
    "    def reset(self):       \n",
    "        '''Resets the environment state to the simplest possible relaxation.'''\n",
    "        self.state = np.zeros_like(self.layout_basis, dtype=int)\n",
    "        self.state[self.basis_sizes == self.initial_size] = 1\n",
    "        self._fill_contained()\n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def layout(self):\n",
    "        layout = simplify_layout([np.array(sites) \n",
    "                                  for sites in self.layout_basis[self.state.astype(bool)]])\n",
    "        return fill_layout(layout, self.N)\n",
    "    \n",
    "    @property\n",
    "    def bound(self): return self.get_values()[0]\n",
    "            \n",
    "    def show_constraints(self, state=None):\n",
    "        if state is None: state = self.state       \n",
    "        for size in np.unique(self.basis_sizes):\n",
    "            print(f\"{size}: {state[self.basis_sizes == size]}\")\n",
    "            \n",
    "    ## agent - environment interaction ##\n",
    "    def step(self, actions):\n",
    "        \"\"\"Receives a list of actions (priority ordered) and executes them until one succeeds.\n",
    "        Input:  - actions: collection of actions (iterable of ints).\n",
    "        Output: - next_state: new state after performing the chosen action.\n",
    "                - action: action that was actually performed among the input ones.\n",
    "                - bound: bound associated to the new state.\n",
    "                - cost: cost associated to the new state.\n",
    "                - err: error code (should be zero).\"\"\"\n",
    "                \n",
    "        state_0 = deepcopy(self.state)\n",
    "        mask = self.action_mask()\n",
    "        for a in actions:\n",
    "            if mask[a]:\n",
    "                next_state, bound, cost, err  = self.perform_action(a) # Try action\n",
    "                if err: \n",
    "                    self.state = deepcopy(state_0)\n",
    "                    _, _, err = self.get_values()\n",
    "                    if err: raise Exception(f\"Error found undoing an action. Returning from \" +\n",
    "                                            f\"\\n{next_state}\\nto\\n{self.state}\\nwith action {a}.\" +\n",
    "                                            \"\\nRef state\\n{state_0}\")\n",
    "                else: \n",
    "                    break \n",
    "        return next_state, a, bound, cost, err\n",
    "    \n",
    "    def perform_action(self, action):\n",
    "        ''' Perform action over the current state and returns the resulting associated values.\n",
    "        Inputs: - action: integer indicating the index of the state to be flipped\n",
    "        Outputs: - Resulting state\n",
    "                 - Solution of the associated SdP:\n",
    "                    - Resulting bound\n",
    "                    - Associated problem cost\n",
    "                    - Error code'''   \n",
    "        \n",
    "        if action < len(self.state):   self.state[action] = -self.state[action] + 1\n",
    "        elif action > len(self.state): raise ValueError(f\"Action {action} exceeds state\" +\n",
    "                                                        f\" size {len(self.state)}\")\n",
    "        # Case that action == len(self.state) the action is to remain in the current state\n",
    "        \n",
    "        self._fill_contained() # Include the smaller contained constraints\n",
    "        bound, cost, err = self.get_values() # Calculate the features\n",
    "        if not err: self._min_max_update(bound, cost)         \n",
    "                \n",
    "        return self.state, bound, cost, err  \n",
    "    \n",
    "    ## SdP results ## \n",
    "    def get_values(self):\n",
    "        \"Solve the associated SdP to the state and return the results.\"\n",
    "        binary = state2int(self.state)\n",
    "        if binary in self.memory.keys():\n",
    "            bound, cost, err = self._remember(binary)\n",
    "            bound, cost, err = self._check_current_limit(binary, bound, cost, err)  \n",
    "        else:\n",
    "            bound, cost, err = self._solve_sdp()\n",
    "            if len(self.memory) < self.memory_limit:\n",
    "                self._memorize(binary, [bound, cost, err])\n",
    "\n",
    "        return bound, cost, err\n",
    "    \n",
    "    def _solve_sdp(self):\n",
    "        \"Solves the associated SdP to the current sate and returns the output.\"\n",
    "        cost = self.solver.ojimetro(self.layout)\n",
    "        if cost > self.budget: \n",
    "            bound = None\n",
    "            err = 2\n",
    "        else:\n",
    "            bound = self.solver.solve(self.problem.to_sdp(), self.layout)\n",
    "            if bound == None: err = 1\n",
    "            else:             err = 0\n",
    "        return bound, cost, err\n",
    "    \n",
    "    def _check_current_limit(self, binary, bound, cost, err):\n",
    "        \"Checks whether pre-computed results fit in the current conditions.\"\n",
    "        if not err and cost > self.budget:\n",
    "            # Pre-computed costs are larger than current limit\n",
    "            err, bound = 2, 0.\n",
    "        elif err == 2 and cost <= self.budget or err==1: \n",
    "            # If the error was due to excess of costs but it fits now, recompute the SdP\n",
    "            bound, cost, err = self._solve_sdp()\n",
    "            self._memorize(binary, [bound, cost, err])\n",
    "                \n",
    "        return bound, cost, err\n",
    "\n",
    "    def _min_max_update(self, bound, cost):\n",
    "        \"\"\"Given a set of bound and cost, compares them to the previous max and min references\n",
    "        and updates them accordingly.\"\"\"\n",
    "        if cost < self.min_cost:   self.min_cost  = cost\n",
    "        if cost > self.max_cost:   self.max_cost  = cost\n",
    "        if bound < self.min_bound: self.min_bound = bound\n",
    "        if bound > self.max_bound: self.max_bound = bound\n",
    "        \n",
    "        # Recall in self.best we have [best_bound, best_cost, worst_cost]    \n",
    "        if bound > self.best[0] and np.abs(bound-self.best[0]) > self.bound_tol:\n",
    "            # If bound beyond threshold, keep it all\n",
    "            self.best = np.array([bound, cost, cost])\n",
    "            self.best_layout = deepcopy(self.layout)\n",
    "            \n",
    "        elif np.abs(bound-self.best[0]) < self.bound_tol:\n",
    "            # If bound within threshold\n",
    "            if   cost < self.best[1]: \n",
    "                self.best[0], self.best[1] = bound, cost \n",
    "                self.best_layout = deepcopy(self.layout)\n",
    "            elif cost > self.best[2]: \n",
    "                self.best[2] = cost\n",
    "    \n",
    "    ## State and action space methods ##\n",
    "    def contained_constraints(self, state=None):\n",
    "        \"Returns an array indicating which constraints are contained by larger ones in the state.\"\n",
    "        state = self.state if state is None else state\n",
    "        return self.contained_map[state.astype(bool)].sum(0).astype(bool)\n",
    "    \n",
    "    def action_mask(self, state=None):\n",
    "        \"Returns a boolean mask indicating which actions can be performed in the current state.\"\n",
    "        return np.concatenate((~self.contained_constraints(state), np.array([True])))\n",
    "    \n",
    "    def _fill_contained(self):\n",
    "        \"Fills state vector to account for the constraints contained in larger ones.\"\n",
    "        self.state[self.contained_constraints()] = 1\n",
    "        \n",
    "    def _get_layout_basis(self, max_size=None):\n",
    "        \"Builds layout basis relating the physical constraints with the state vector.\"\n",
    "        graph = self.problem.graph\n",
    "        subgraphs = [graph.edges]\n",
    "        constr_cost = self.solver.ojimetro(fill_layout([subgraphs[-1][0]], graph.n_nodes))\n",
    "        while constr_cost <= self.budget:\n",
    "            subgraphs = add_subgraph_size(subgraphs)\n",
    "            constr_cost = self.solver.ojimetro(fill_layout([subgraphs[-1][0]], graph.n_nodes))\n",
    "            if len(subgraphs[-1]) == 1: break\n",
    "            if len(subgraphs[-1][0]) == max_size: break\n",
    "        \n",
    "        basis = [c for sub in (subgraphs if constr_cost <= self.budget else subgraphs[:-1])\n",
    "                 for c in sub]\n",
    "        if len(basis) == 0: \n",
    "            raise ValueError(f\"Unable to fit a single 2-body constraint with cost {constr_cost} \" +\n",
    "                             f\"for budget {self.budget}. We'll need beefier computers for this!\")\n",
    "        return np.array(basis, dtype=object)\n",
    "    \n",
    "    def _get_contained_map(self):\n",
    "        \"\"\"Builds a map indicating which basis elements are contained into larger ones excluding\n",
    "        themselves. The map is a matrix such that `map[state.astype(bool)].sum(0)` returns the \n",
    "        boolean indicators of the contained elements.\"\"\"\n",
    "        size = len(self.layout_basis)\n",
    "        contained_map = np.zeros((size, size), dtype=bool)\n",
    "        for i, big_supp in enumerate(self.layout_basis):\n",
    "            for j, small_supp in enumerate(self.layout_basis[:i]):\n",
    "                common = np.intersect1d(big_supp, small_supp)\n",
    "                if len(common) == len(small_supp):\n",
    "                    contained_map[i, j] = True\n",
    "                    \n",
    "        return contained_map\n",
    "    \n",
    "    ## Reward functions ##\n",
    "    def bound_reward(self, bounds, costs, best_ref=None):\n",
    "        \"The reward is the bound of the state.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol\n",
    "        reward = deepcopy(bounds)\n",
    "        reward[bounds == 0] = self.min_bound*1.1                 # Errors\n",
    "        reward[costs > self.budget] = self.min_bound*1.1         # Over parameter limit\n",
    "        reward[thresh_mask] = costs[thresh_mask]/best[2].float() # Reweight threshold states\n",
    "        return reward\n",
    "    \n",
    "    def bound_norm_reward(self, bounds, costs, best_ref=None):\n",
    "        \"The reward is a normalized function from 0 to 1 as function of the bound and cost.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol\n",
    "        reward = dist_poly(deepcopy(bounds), best[0], self.min_bound, d=self.dist_d)\n",
    "        reward[bounds == 0] = 0                                  # Errors\n",
    "        reward[costs > self.budget] = 0                          # Over parameter limit\n",
    "        reward[thresh_mask] = best[2]/costs[thresh_mask].float() # Reweight threshold states\n",
    "        return reward*best[1]/best[2]\n",
    "    \n",
    "    def bound_improve_reward(self, bounds, costs, best_ref=None):\n",
    "        \"The reward is the bound improvement (+1) with respect to the minimum bound.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol\n",
    "        reward = bounds - self.min_bound + 1\n",
    "        reward[bounds == 0] = 0                                  # Errors\n",
    "        reward[costs > self.budget] = 0                          # Over parameter limit\n",
    "        reward[thresh_mask] = best[2]/costs[thresh_mask].float() # Reweight threshold states\n",
    "        return reward               \n",
    "         \n",
    "    ## Memory methods ##\n",
    "    def save_memory(self):\n",
    "        old_memory = self._read_memory()\n",
    "        full_memory = {**old_memory, **self.memory}\n",
    "        with open(self.memory_path, \"wb\") as f:\n",
    "            pickle.dump(full_memory, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        self.memory = self._read_memory()\n",
    "        \n",
    "    def _get_memory(self):\n",
    "        \"Reads the corresponding memory file\"\n",
    "        memory_dir = Path(\"../memories/\")\n",
    "        memory_dir.mkdir(exist_ok=True)\n",
    "        self.memory_path = memory_dir/f\"{get_memory_file_name(self.problem, self.solver)}.pkl\" \n",
    "        self.memory = self._read_memory()\n",
    "\n",
    "    def _memorize(self, state_idx, values):\n",
    "        \"Add to memory the state visited (associated binary index) and the values of the SdP.\"\n",
    "        bound, cost, err = values\n",
    "\n",
    "        if state_idx in self.memory.keys() and cost > self.budget and err != 2:\n",
    "            _, _, old_err = self._remember(state_idx)\n",
    "            if old_err != 1:\n",
    "                raise Exception(f\"Trying to memorize constraint with binary index {state_idx}\" +\n",
    "                                \" already in memory\")\n",
    "        elif not isinstance(state_idx, int):\n",
    "            raise ValueError(f\"Constraint is not a binary integer {state_idx}\")\n",
    "        else:\n",
    "            self.memory[state_idx] = values       \n",
    "    \n",
    "    def _remember(self, state_idx):\n",
    "        \"Recalls the results values associated to a previously visited state.\"         \n",
    "        return self.memory[state_idx]     \n",
    "    \n",
    "    def _read_memory(self):\n",
    "        \"Reads the memory corresponding to the environemnt's current problem.\"\n",
    "        try:\n",
    "            with open(self.memory_path, \"rb\") as f:\n",
    "                memory = pickle.load(f)\n",
    "        except: memory = {}\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains all the information of the problem at hand, and controls the way the agent can explore the state space. \n",
    "\n",
    "Hence, we must provide the `SdPEnvironment` with an instance of our problem, such as a `Hamiltonian`, an adequate solver that defines the objective, such as an `SdPEnergySolver`. \n",
    "\n",
    "Then, we need to provide information about the reinforcement learning task, such as the computational budget we can afford, and the reward function that we want to use. Optionally, we can provide a pre-defined layout basis, which conditions the kind of constraints that the agent can create. Otherwise, the environment generates its own basis directly from the problem instance and it underlying structure, e.g., the `Hamiltonian.graph`.\n",
    "\n",
    "## Create an environment\n",
    "Let's see an example. We will start by defining the problem parameters. In this case, we will try to find the ground state energy of a Heisenberg XX Hamiltonian on a one-dimensional chain. Furthermore, we will consider a moderate budget that only allows us to consider 3-body reduced density matrices to compute the bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from bounce.hamiltonian import XXHamiltonian, Chain1D\n",
    "from bounce.sdp import SdPEnergySolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "chain = Chain1D(N)\n",
    "B, J = 3, 1\n",
    "H = XXHamiltonian(chain, B, J)\n",
    "solver = SdPEnergySolver()\n",
    "budget = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can create our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SdPEnvironment(H, solver, budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment starts in the simplemost state, which corresponds to a trivial relaxation of the problem. This is represented by a state of zeros, indicating that no compatibility constraints are active at this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (env.state == np.zeros(2*N)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state vector serves as a boolean indicator of which constraints in the environment's basis are currently active, although the vector is not boolean itself because it has to be processed by the agents. \n",
    "\n",
    "In this case, the environment's basis is automatically generated from the `Hamiltonian.graph.edges` and the budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0, 1]), array([1, 2]), array([2, 3]), array([3, 4]),\n",
       "       array([0, 4]), array([0, 1, 2]), array([0, 1, 4]),\n",
       "       array([1, 2, 3]), array([2, 3, 4]), array([0, 3, 4])], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment associates a unique layout to every state which depends on the basis and the active constraints. In this case, the layout is made out of single-body reduced density matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([1]), array([2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment takes care of providing the solver the needed information of the problem. In this case, it feeds the `SdPEnergySolver` both the data from the Hamiltonian and the current layout. We can solve the associated semidefinite program (SdP) to the active constraints with the `get_values` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.99999999732747, 19, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound, cost, error = env.get_values()\n",
    "bound, cost, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bound == solver.solve(H.to_sdp(), env.layout)\n",
    "assert cost == solver.ojimetro(env.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we visit a new state, the environment keeps track of the active values. This way, it builds a reference of the best possible bounds that can be obtained. For now, given that we have not moved from the initial state, the best bound and layout are the only ones it has ever seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.99999999732747,\n",
       " [array([0]), array([1]), array([2]), array([3]), array([4])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_bound, env.best_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess which is the best layout, it accounts for both the best possible bound and the lowest possible cost with which it has ever been achieved. This is stored in `best` which contains the best possible bound together with the maximum an minimum costs it was ever achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.,  19.,  19.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bound == env.max_bound\n",
    "assert env.best_layout == env.layout\n",
    "assert (env.best == np.array([bound, cost, cost])).all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can move to new states by performing an action. Actions are, in essence, very simple. They are integers that indicate which bit we flip in the state vector. Doing so, adds or removes constraints in the associated SdP.\n",
    "\n",
    "In order to test this out, let's add the first 3-body constraint, indexed by `N` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 0, 0, 1, 0, 0, 0, 0]), -20.999999995348936, 71)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, bound, cost, error = env.perform_action(N)\n",
    "new_state, bound, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that now we have a higher energy bound. In this case, the higher the better, so it's nice! However, the associated cost is also much larger than before. \n",
    "\n",
    "Additionally, despite performing a single action, we observe that now there are three \"ones\" in our state vector. This is because the basis element in the `N`-th position, contained two other elements of smaller size. We can see the state vector by size with the method `show_constraints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 0 0 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] contains [0 1] and [1 2].\n"
     ]
    }
   ],
   "source": [
    "print(f\"{env.layout_basis[N]} contains {env.layout_basis[0]} and {env.layout_basis[1]}.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these two smaller ones are not present in the laoyut, as they would be redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this with the `contained_constraints` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.contained_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we ensure a consistent exploration through state space. Now, if we removed the larger element, the two smaller ones would remain, ensuring that there are no big leaps in the exploration and limiting the actions that can be performed. The agent cannot attempt to add or remove elements contained in larger ones. We can see this with the `action_mask` method, which indicates the state vector positions that can be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the action mask is one element longer than the state vector because it accounts for the action of \"remaining still\". \n",
    "\n",
    "Since we have moved to a state that provides a better bound for our problem, the references have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.999999995348936, [array([0, 1, 2]), array([3]), array([4])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_bound, env.best_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (new_state == env.state).all()\n",
    "assert bound == env.max_bound\n",
    "assert (bound, cost, 0) == env.get_values()\n",
    "assert (env.best == np.array([bound, cost, cost])).all()\n",
    "assert np.all([(l_best == l).all() for l_best, l in zip(env.best_layout, env.layout)])\n",
    "assert (env.contained_constraints() == np.array([True, True] + [False]*(2*N-2))).all()\n",
    "assert (env.action_mask() == np.array([False, False] + [True]*(2*N-1))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the action, we remove the larger element and the two smaller ones remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "new_state, bound, cost, _ = env.perform_action(N)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1]), array([1, 2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so, we have reduced the associated cost of the state. However, the bound has remained the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.99999999834299, 36)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the main motivations behind the work [Certificates of quantum many-body properties assisted by machine learning](https://arxiv.org/abs/2103.03830). We can see this reflected in the environment's references, which now shows the minimum and maximum costs with which the best bound has ever been obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-21.,  36.,  71.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration boundaries and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to exceed the allowed computational budget with an action, we would obtain an error indicator. The erorr code is:\n",
    "- 0: all good! \n",
    "- 1: there could not be found a solution for the associated SdP \n",
    "- 2: the associated SdP cost exceeds the computational budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 1 1 1]\n",
      "3: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "env.perform_action(N+3)\n",
    "new_state, bound, cost, err = env.perform_action(N+4)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 132, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err, cost, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert err == 2\n",
    "assert cost > budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are the way through which we establish \"walls\" in the state space. This way, agents do not need to learn the boundaries of the feasible region nor need to keep track of it. \n",
    "\n",
    "The environment prevents threspassing the state space boundaries with the `step` method. Rather than taking a single action, such as the `perform_action` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SdPEnvironment.step\" class=\"doc_header\"><code>SdPEnvironment.step</code><a href=\"__main__.py#L64\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SdPEnvironment.step</code>(**`actions`**)\n",
       "\n",
       "Receives a list of actions (priority ordered) and executes them until one succeeds.\n",
       "Input:  - actions: collection of actions (iterable of ints).\n",
       "Output: - next_state: new state after performing the chosen action.\n",
       "        - action: action that was actually performed among the input ones.\n",
       "        - bound: bound associated to the new state.\n",
       "        - cost: cost associated to the new state.\n",
       "        - err: error code (should be zero)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SdPEnvironment.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step` method takes a list of priority-ordered actions. The method performes them in the given order until one succeeds, ensuring that we never land on an error-flagged state.\n",
    "\n",
    "In order to test this out, let's first reset the environment to the initial state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performed action was 5.\n",
      "2: [1 1 0 0 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "new_state, action, bound, cost, error = env.step([5, 6, 7, 0, 1])\n",
    "print(f\"The performed action was {action}.\")\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert action == 5\n",
    "assert error == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now try to add an additional large constraint, we will exceed the computational budget. However, the step method will perform the first suitable action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performed action was 3.\n",
      "2: [1 1 0 1 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "actions_to_try = [8, 6, 3, 5]\n",
    "new_state, action, bound, cost, error = env.step(actions_to_try)\n",
    "print(f\"The performed action was {action}.\")\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert action == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step` method has skipped the first two actions in the priority list to avoid exceeding the budget. In the extreme case in which none of the actions provided can be executed, the function returns the resutls for the last action it tried, although the environment has not advanced. This should never happen if we provide all possible actions ranked and the state-space is properly designed. Intuitively, there should never be rabbit holes the agent can't escape from.\n",
    "\n",
    "## Custom basis\n",
    "\n",
    "We can have a major impact in the state space and its boundaries by specifying a layout basis of our choice. For instance, we may be interested in finding bounds to a problem using a certain kind of constraints.\n",
    "\n",
    "Let's take the previous example and set a specific basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_basis = np.array([np.array([0, 1]), np.array([2, 3]), np.array([3, 4, 5]), np.array([0, 1, 4, 5])], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SdPEnvironment(H, solver, budget, layout_basis=layout_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0, 1]), array([2, 3]), array([3, 4, 5]),\n",
       "       array([0, 1, 4, 5])], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all([(lb == le).all() for lb, le in zip(layout_basis, env.layout_basis)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have basis elements of different sizes with various overlaps. For instance, in this case there are no 2-body elements contained in the 3-body one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0]\n",
      "3: [1]\n",
      "4: [0]\n"
     ]
    }
   ],
   "source": [
    "env.step([2])\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some times we may want to use a fully customized basis, some times we simply care about truncating it. For instance, we may have a budget that allows us to reach up to four-body elements, but we wish to invest the resources into using exclusively as many three-body elements as possible. In these cases, we can specify a maximum size for the basis elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 300\n",
    "max_basis_size = 3\n",
    "env_unrestricted = SdPEnvironment(H, solver, budget)\n",
    "env_restricted = SdPEnvironment(H, solver, budget, max_basis_size=max_basis_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n",
      "4: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env_unrestricted.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env_restricted.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max([len(l) for l in env_restricted.layout_basis]) == max_basis_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Besides providing a consistent state space and exploration rules, the environment is in charge of providing feedback to the agent. When we initialize the environment, we choose the desired criterion for the reward. By default, we use the `bound_norm_reward`, with which we have obtained the best results so far and we have performed all the calculations in [[1]](https://arxiv.org/abs/2103.03830)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bound_norm_reward'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_fun.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env.reward_fun.__name__ == 'bound_norm_reward'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that, in general, we do not know the optimal relaxation, the environment relies on the references it has gathered during the exploration to provide the rewards. We can choose among:\n",
    "- **bound_reward**: the reward is the obtained bound, regardless of the cost. It can take any arbitrary value depending on the problem.\n",
    "- **bound_norm_reward**: the reward is a normalized function between zero and one which accounts for both the value of the bound and the associated cost. \n",
    "- **bound_improve_reward**: the reward is the improvement of a bound with respect to the minimum one ever observed. \n",
    "\n",
    "We select the criterion by providing the name of the function excluding the `'_reward'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The environment implements a memory that stores the SdP solution of all the visited states. This way, we do not need to instantiate the SdP and solve it every time we revisit a state, speeding up the whole process. We can save the memory with the `save_memory` method and it will be automatically loaded when dealing with the same problem. The limit stored solutions in the memory is 1e6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../memories/memory_Energy_xx_N5_supports_0_1_2_3_4_01_12_23_34_04_terms_3·z_3·z_3·z_3·z_3·z_x⊗x + y⊗y_x⊗x + y⊗y_x⊗x + y⊗y_x⊗x + y⊗y_x⊗x + y⊗y.pkl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.memory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] B. Requena, G. Muñoz-Gil, M. Lewenstein, V. Dunjko, J. Tura. [Certificates of quantum many-body properties assisted by machine learning](https://arxiv.org/abs/2103.03830). *arXiv:2103.03830 (2021)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
