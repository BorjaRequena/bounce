{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from bounce.sdp import solve_sdp, ojimetro\n",
    "from bounce.utils import state2int, state2str, contained_constraints, simplify_layout, fill_layout, dist_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "> Definition of the environment with which the agent interacts. The environment handles the execution of the actions and provides the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SDPEnvironment:\n",
    "    \"Environment for constraint exploration.\"\n",
    "    \n",
    "    def __init__(self, N, H, param_profile, reward_criterion=\"energy_norm\", energy_threshold=1e-3):\n",
    "        \n",
    "        self.N = N # Number of sites\n",
    "        self.H = H # Hamiltonian\n",
    "        \n",
    "        # Parameter profile\n",
    "        self.param_profile = param_profile\n",
    "        self.param_limit = param_profile(0)\n",
    "        \n",
    "        # Reward function\n",
    "        self.reward_fun = getattr(self, reward_criterion+\"_reward\")\n",
    "        self.dist_d = 5\n",
    "        \n",
    "        # Memory of visited states. It is a lookup table for computation speedup.\n",
    "        self.memory_limit = 1e6\n",
    "        self._get_memory()\n",
    "        \n",
    "        # Creating the agent basis \n",
    "        self.agent_basis = self._get_agent_basis()         \n",
    "        self._get_layout_basis()\n",
    "        self._constrain_basis()     \n",
    "        \n",
    "        # Initialize the environment\n",
    "#         self._find_initial_state()\n",
    "        self.reset()        \n",
    "        \n",
    "        # Memories           \n",
    "        self.E_threshold = energy_threshold\n",
    "        self.max_energy  = -np.inf  # Maximum energy ever obtained\n",
    "        self.min_energy  = np.inf   # Minimum energy ever obtained\n",
    "        self.max_params  = -np.inf  # Maximum amount of parameters ever obtained\n",
    "        self.min_params  = np.inf   # Minimum amount of parameters ever obtained\n",
    "        self.best = np.array([-np.inf, np.inf, -np.inf])  # Maximum energy, best and worst params\n",
    "        self.best_layout = deepcopy(self.layout)\n",
    "        \n",
    "        # Initial state reference\n",
    "        energy, params, err = self.get_values()\n",
    "        if not err: self._min_max_update(energy, params)\n",
    "        else:       raise ValueError(f\"Something went wrong. Initial state {self.state} provides error.\")\n",
    "        \n",
    "        \n",
    "    def reset(self):       \n",
    "        ''' Resets the environment. State goes either to random or to all zeros.\n",
    "        Outputs: - State in env form. '''\n",
    "        # Reset state with lowest energy state\n",
    "        self.state = np.zeros(len(self.layout_basis), dtype=int)\n",
    "#         self.state[:self.N] = 1                             \n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def constraints(self):\n",
    "        return [self.state[i:i+self.N] for i in range(0, len(self.state), self.N)]\n",
    "    \n",
    "    @property\n",
    "    def layout(self):\n",
    "        layout = simplify_layout([np.array(sites) for sites in self.layout_basis[self.state.astype(bool)]])\n",
    "        return fill_layout(layout, self.N)\n",
    "            \n",
    "    def show_constraints(self, state=None):\n",
    "        if state is None: state = self.state\n",
    "        sc = [state[i:i+self.N] for i in range(0, len(state), self.N)]\n",
    "        for k,c in enumerate(sc): print(f\"{k+2}: {np.array2string(c)[1:-1]}\")  \n",
    "            \n",
    "    ## agent - environment interaction ##\n",
    "    def perform_action(self, actions, it):\n",
    "        \"Receives a list of actions (priority ordered) and repeatedly tries to execute them until one is accepted.\"\n",
    "        state_0 = deepcopy(self.state)\n",
    "        for a in actions:\n",
    "            next_state, energy, params, err  = self.explorative_step(a, it) # Try action\n",
    "            if err: \n",
    "                self.state = deepcopy(state_0)\n",
    "                _, _, err = self.get_values()\n",
    "                if err: raise Exception(f\"Error found undoing an action. Going back from \"+\n",
    "                                        f\"{next_state} to {self.state} with action {a}. Ref state is {state_0}\")\n",
    "            else: \n",
    "                break \n",
    "#         _, next_state, _ = self._simplify_constraints() # In case we want to try with simplified states on the NN\n",
    "        return next_state, a, energy, params, err\n",
    "    \n",
    "    def explorative_step(self, action, it):\n",
    "        ''' Perform action over the current state and calculate/recalls the features\n",
    "        of the resulting state. Then calculates the reward.\n",
    "        Inputs: - action: integer indicating the index of the state to be flipped\n",
    "                - it: training iteration (episode in main code)\n",
    "        Outputs: - Resulting state\n",
    "                 - Parameters of the SDP:\n",
    "                    - Parameters given the new constraints\n",
    "                    - Energy of the new state\n",
    "                    - Error'''   \n",
    "        \n",
    "        if action < len(self.state):   self.state[action] = -self.state[action] + 1\n",
    "        elif action > len(self.state): raise ValueError(f\"Action {action} exceeds maximum index {len(self.state)}\")\n",
    "        # Case that action == len(self.state) the action is to remain in the current state\n",
    "        \n",
    "        self.state[contained_constraints(self.state, self.N)] = 1 # Include the smaller contained constraints\n",
    "        self.param_limit = self.param_profile(it)\n",
    "        energy, params, err = self.get_values() # Calculate the features\n",
    "        if not err: self._min_max_update(energy, params)         \n",
    "                \n",
    "        return self.state, energy, params, err  \n",
    "    \n",
    "    \n",
    "    ## Reward functions ##\n",
    "    def energy_reward(self, energies, parameters, best_ref=None):\n",
    "        \"The reward is the energy of the state.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(energies - best[0]) < self.E_threshold\n",
    "        reward = deepcopy(energies)\n",
    "        reward[energies == 0] = self.min_energy*1.1                   # Errors\n",
    "        reward[parameters > self.param_limit] = self.min_energy*1.1   # Over parameter limit\n",
    "        reward[thresh_mask] = parameters[thresh_mask]/best[2].float() # Reweight threshold states\n",
    "        return reward\n",
    "    \n",
    "    def energy_norm_reward(self, energies, parameters, best_ref=None):\n",
    "        \"The reward is a normalized function from 0 to 1 as function of the energy and parameters.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(energies - best[0]) < self.E_threshold\n",
    "        reward = dist_poly(deepcopy(energies), best[0], self.min_energy, d=self.dist_d)\n",
    "        reward[energies == 0] = 0                                     # Errors\n",
    "        reward[parameters > self.param_limit] = 0                     # Over parameter limit\n",
    "        reward[thresh_mask] = best[2]/parameters[thresh_mask].float() # Reweight threshold states\n",
    "        return reward*best[1]/best[2]\n",
    "    \n",
    "    def energy_improve_reward(self, energies, parameters, best_ref=None):\n",
    "        \"The reward is the energy improvement (+1) with respect to the minimum energy.\"\n",
    "        best = self.best if best_ref is None else best_ref\n",
    "        thresh_mask = torch.abs(energies - best[0]) < self.E_threshold\n",
    "        reward = energies - self.min_energy + 1\n",
    "        reward[energies == 0] = 0                                     # Errors\n",
    "        reward[parameters > self.param_limit] = 0                     # Over parameter limit\n",
    "        reward[thresh_mask] = best[2]/parameters[thresh_mask].float() # Reweight threshold states\n",
    "        return reward               \n",
    "    \n",
    "    ## SDP results ## \n",
    "    def get_values(self):\n",
    "        \"Solve the associated SDP to the state and return the results.\"\n",
    "        binary = state2int(self.state)\n",
    "        if binary in self.memory.keys():\n",
    "            energy, params, err = self._remember(binary)\n",
    "            energy, params, err = self._check_current_limit(binary, energy, params, err)  \n",
    "        else:\n",
    "            energy, params, err = self.get_sdp_results()\n",
    "            if len(self.memory) < self.memory_limit:\n",
    "                self._memorize(binary, [energy, params, err])\n",
    "\n",
    "        return energy, params, err\n",
    "            \n",
    "    def get_params(self):\n",
    "        \"Estimates the free parameters needed to solve the SDP\"\n",
    "        binary = state2int(self.state)\n",
    "        if binary in self.memory.keys(): _, params, _ = self._remember(binary)\n",
    "        else:                            params = ojimetro(self.layout)\n",
    "        return params\n",
    "    \n",
    "    def get_sdp_results(self):\n",
    "        \"Computes the energy bound solving the associated SDP to the sate\"\n",
    "        energy = solve_sdp(self.layout, self.H)\n",
    "        params = ojimetro(self.layout)\n",
    "        if energy == 0:                 err = 1\n",
    "        elif params > self.param_limit: err = 2\n",
    "        else:                           err = 0\n",
    "        return energy, params, err\n",
    "  \n",
    "        \n",
    "    def _SDP_graph(self, L=None):\n",
    "        \"\"\"SDP speedup for graph states by avoiding matlab queries. L is optional input for generality purposes, does not\n",
    "        even need simplified versions, matlab will do with the simplification.\"\"\"\n",
    "        err = self._check_support(3, L=L)\n",
    "        if err: return 0, 0, err, 0.\n",
    "        else:   \n",
    "            params = self.get_params() # visits matlab\n",
    "            if params > self.param_limit: return params, len(self.L)-1, 2, 0.   # Error for excess of parameters (arbitrary numer of blocks)\n",
    "            else:                        return params, len(self.L)-1, err, 1. # SDP values (arbitrary number of blocks)\n",
    "    \n",
    "    def _check_support(self, min_support, L=None):\n",
    "        \"\"\"L is optional input for generality purposes\"\"\"\n",
    "        self.L = self.matlab_basis[self.state.astype(bool) if L is None else L.astype(bool)]\n",
    "        str_layout = '|'.join(map(lambda x: ' '.join([str(x[i]) for i in range(len(x))]), self.L))\n",
    "        \n",
    "        err = 0\n",
    "        for constraint in self.agent_basis[min_support-2]: \n",
    "            if constraint not in str_layout: err = 1; break                \n",
    "        return err  \n",
    "    \n",
    "    def _check_current_limit(self, binary, energy, params, err):\n",
    "        if not err and params > self.param_limit:\n",
    "            # Pre-computed parameters are larger than current limit\n",
    "            err, energy = 2, 0.\n",
    "        elif err == 2 and params <= self.param_limit or err==1: \n",
    "            # If the error was due to excess of parameters but it fits now, recompute the SDP\n",
    "            energy, params, err = self.get_sdp_results()\n",
    "            self._memorize(binary, [energy, params, err])\n",
    "                \n",
    "        return energy, params, err\n",
    "\n",
    "    def _min_max_update(self, energy, params):\n",
    "        \"\"\"Given a a new obtained set of energy and parameters, check whether they are higher or lower than the max and min\n",
    "        values obtained previously and update them.\"\"\"\n",
    "        if params < self.min_params: self.min_params = params\n",
    "        if params > self.max_params: self.max_params = params\n",
    "        if energy < self.min_energy: self.min_energy = energy\n",
    "        if energy > self.max_energy: self.max_energy = energy\n",
    "        \n",
    "        # Recall in self.best we have [best_energy, best_params, worst_params]    \n",
    "        if energy > self.best[0] and np.abs(energy-self.best[0]) > self.E_threshold:\n",
    "            # If energy beyond threshold, keep it all\n",
    "            self.best = np.array([energy, params, params])\n",
    "            self.best_layout = deepcopy(self.layout)\n",
    "            \n",
    "        elif np.abs(energy-self.best[0]) < self.E_threshold:\n",
    "            # If energy within threshold\n",
    "            if   params < self.best[1]: \n",
    "                self.best[0], self.best[1] = energy, params \n",
    "                self.best_layout = deepcopy(self.layout)\n",
    "            elif params > self.best[2]: \n",
    "                self.best[2] = params\n",
    "        \n",
    "    ## State vector methods ##       \n",
    "    def _layout2state(self, L=None):\n",
    "        \"Formats layout to state\"\n",
    "        if L is None: L = self.layout\n",
    "        state = np.zeros(self.agent_basis.shape)\n",
    "        for constraint in L: \n",
    "            idx1 = len(constraint) - 2\n",
    "            const = np.sort(constraint)\n",
    "            const_str = ' '.join(map(str, const))\n",
    "            idx2 = np.where(const_str == self.agent_basis[idx1])[0]\n",
    "            state[idx1, idx2] = 1\n",
    "        return state.reshape(self.state.shape)\n",
    "    \n",
    "    def _simplify_constraints(self):\n",
    "        '''Simplifies current state removing contained constraints.'''        \n",
    "        state_simp = deepcopy(self.state)\n",
    "        state_simp[contained_constraints(state_simp, self.N)] = 0\n",
    "        return state_simp, state2int(state_simp) \n",
    "         \n",
    "    ## Memory methods ##\n",
    "    def save_memory(self):\n",
    "        old_memory = self._read_memory()\n",
    "        full_memory = {**old_memory, **self.memory}\n",
    "        with open(self.memory_path, \"wb\") as f:\n",
    "            pickle.dump(full_memory, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        self.memory = self._read_memory()\n",
    "        \n",
    "    def _get_memory(self):\n",
    "        \"Reads the corresponding memory file\"\n",
    "        memory_dir = Path(\"../memories/\")\n",
    "        if self.H.model == \"graph\": \n",
    "            self.memory_path = memory_dir/f\"env_memory_{self.H.model}_N{self.N}.pkl\"\n",
    "        elif self.H.model == \"xy\":\n",
    "            self.memory_path = memory_dir/(f\"env_memory_{self.H.model}_g{self.H.g}_N{self.N}\" + \n",
    "                                    f\"_B{state2str(self.H.linear)}_J{state2str(self.H.quadratic)}.pkl\")\n",
    "        else: \n",
    "            self.memory_path = memory_dir/(f\"env_memory_{self.H.model}_N{self.N}\" + \n",
    "                                    f\"_B{state2str(self.H.linear)}_J{state2str(self.H.quadratic)}.pkl\")\n",
    "        self.memory = self._read_memory()\n",
    "\n",
    "    def _memorize(self, constraint, values):\n",
    "        \"Add to memory the states visited and the values of the SDP for each iteration\"\n",
    "        energy, params, err = values\n",
    "\n",
    "        if constraint in self.memory.keys() and params > self.param_limit and err != 2:\n",
    "            _, _, old_err = self._remember(constraint)\n",
    "            if old_err != 1:\n",
    "                raise Exception(f\"Trying to memorize constraint with binary index {constraint} already in memory\")\n",
    "        elif not isinstance(constraint, int):\n",
    "            raise ValueError(f\"Constraint is not a binary integer {constraint}\")\n",
    "        else:\n",
    "            self.memory[constraint] = values       \n",
    "    \n",
    "    def _remember(self, constraint):\n",
    "        \"Given a set of constraint, outputs the values of the SDP.\"         \n",
    "        return self.memory[constraint]     \n",
    "    \n",
    "    def _read_memory(self):\n",
    "        try:\n",
    "            with open(self.memory_path, \"rb\") as f:\n",
    "                memory = pickle.load(f)\n",
    "        except: memory = {}\n",
    "        return memory\n",
    "            \n",
    "    ## Agent action basis methods ##\n",
    "    def _get_agent_basis(self, local_hamiltonian = True): \n",
    "        \"Creates the basis of the different possible constraints given as a list of str\"\n",
    "        if local_hamiltonian: # only nearest neigbors\n",
    "            s = \"\"\n",
    "            agent_basis = []\n",
    "            for n1 in range(1,self.N-1): # Avoid computing 1-body and full system\n",
    "                aa = []\n",
    "                for n2 in range(self.N):                   \n",
    "                    l_c = np.arange(n1+1)+n2\n",
    "                    l_c[l_c >= self.N] -= self.N\n",
    "                    l_c = np.sort(l_c)\n",
    "                    num = ' '.join(map(str,l_c))                       \n",
    "                    aa.append(s.join(num))                                   \n",
    "                _, idx = np.unique(aa, return_index=True)\n",
    "                aa = np.array(aa)[np.sort(idx)].tolist()                \n",
    "                agent_basis.append(aa)                \n",
    "        else: # arbitrary connections\n",
    "            num = ' '.join(map(str,list(range(self.N))))        \n",
    "            s = \"\"\n",
    "            agent_basis = []       \n",
    "            for nn in range(self.N):            \n",
    "                ll  = np.sort(list(itertools.permutations(num, nn+1)))                \n",
    "                aa = []                \n",
    "                for l in ll:\n",
    "                    seq = l\n",
    "                    aa.append(s.join(seq))                     \n",
    "                agent_basis.append(np.unique(aa))            \n",
    "        return np.array(agent_basis)  \n",
    "    \n",
    "    def _get_layout_basis(self):   \n",
    "        \"Builds layout basis.\"\n",
    "        a = [item for sublist in self.agent_basis for item in sublist]\n",
    "        self.layout_basis = []        \n",
    "        for item in a:                \n",
    "            self.layout_basis.append([int(s) for s in item.split(sep=\" \") if s.isdigit()])\n",
    "        self.layout_basis = np.array(self.layout_basis)\n",
    "    \n",
    "    def _constrain_basis(self):\n",
    "        \"\"\"Given the maximum allowed of parameters, this function redefines the basis\n",
    "        by cutting down the unaccessible states from the state-vector\"\"\"\n",
    "        for k in range(self.N-2):\n",
    "            self.state = np.zeros(len(self.layout_basis))\n",
    "#             self.state[:self.N] = 1\n",
    "            self.state[k*self.N] = 1\n",
    "            p = self.get_params()\n",
    "            if p > self.param_profile.max_params:\n",
    "                self.agent_basis = self.agent_basis[:k]\n",
    "                self._get_layout_basis()\n",
    "                break\n",
    "\n",
    "        if k == 0: \n",
    "            raise ValueError(f\"Unable to fit minimum complexity state with {int(p)} parameters \"+\n",
    "        f\"for maximum allowed {self.param_profile.max_params}. Will need beefier computers for this!\")\n",
    "            \n",
    "    def _find_initial_state(self):\n",
    "        \"\"\"Finds a computable initial state. Starts with the lowest interaction and increases\n",
    "        it until it finds a suitable initial state.\"\"\"\n",
    "        self.n = 0\n",
    "        err = 1\n",
    "        while err:\n",
    "            self.n += 1\n",
    "            self.state = np.zeros(len(self.layout_basis))\n",
    "            self.state[:self.n*self.N] = 1 \n",
    "            _, params, _, err, _ = self.get_values()\n",
    "            if err == 2: raise ValueError(\"Unable to find computable initial state within parameter limit \"+\n",
    "                                          f\"{self.param_limit}. Currently trying with groups of {self.n+1} requiring \"+\n",
    "                                          f\"{params} parameters. Try increasing the computational budget.\")\n",
    "                \n",
    "    def _get_constraint_bounds(self):\n",
    "        \"Finds the maximum amount of each constraint that can be fit into the budget.\"\n",
    "        self.max_const_by_size = [self.N]\n",
    "        for size in range(1, len(self.agent_basis)):\n",
    "            for k in range(self.N):\n",
    "                self.state = np.zeros(len(self.layout_basis))\n",
    "                self.state[:self.N] = 1 # All pairs must fit for sure\n",
    "                self.state[size*self.N:size*self.N + k] = 1\n",
    "                params = self.get_params()\n",
    "                if params > self.param_profile.max_params: k-=1; break\n",
    "            self.max_const_by_size.append(k+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains the information of the physical problem, such as the number of sites `N` and the properties of the hamiltonian `H`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from bounce.hamiltonian import XXHamiltonian\n",
    "from bounce.budget_profiles import FlatProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "B, J = np.ones(N), np.ones(N)*2\n",
    "H = XXHamiltonian(N, B, J)\n",
    "\n",
    "profile = FlatProfile(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SDPEnvironment(N, H, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment provides several functionalities to ease the visualization of the current state. For instance, we can call the method `show_constraints` to display the current active constraints with their associated sites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0 0 0 0 0\n",
      "3: 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0 1 1 0 0\n",
      "3: 0 1 0 0 0\n"
     ]
    }
   ],
   "source": [
    "env.explorative_step(N+1, 0)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the current layout, which provides a list of the current active constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]), array([0]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It deals with solving the SDP when given a set of constraints from the agent. The method `get_values` provides the full solution of the SDP. Alternatively, for an estimation of the computational cost without solving the SDP, the method `get_params` provides the amount of free parameters estimated to be used by the solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.65685424078611, 71, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can always be reset to the initial state with `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0 0 0 0 0\n",
      "3: 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment implements a memory that stores the SDP solution of all the visited states in order to speed up the process. This memory can be saved with the method `save_memory` and will automatically be loaded when dealing with the same problem. The limit stored solutions in the memory is 1e6.  \n",
    "\n",
    "The environment deals with the state exploration through `perform_action`. It handles the state-space boundaries and provides the rewards according to a given criterion. To track the state exploration process, `show_constraints` provides a nice visualization of the current state. The reward criterion can be specified when instancing the environment by providing a string with the name of the reward function, e.g., `reward_criterion='energy_norm'` (the default). The naming convention for the reward functions is `f'{reward_criterion}_reward'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
