{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import picos\n",
    "import numpy as np\n",
    "from bounce.utils import state2str, simplify_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semidefinite programming \n",
    "\n",
    "> Toolkit to solve the semi-definite program defined by the set of constraints and Hamiltonian.\n",
    "\n",
    "## SdP formulation and optimization\n",
    "\n",
    "In order to formulate and solve the SdP there are two main needed items: \n",
    "* a Hamiltonian that can be expressed in terms of `picos.Constant` through a `Hamiltonian.to_sdp()` call.\n",
    "* a layout based in `np.array`, e.g., `L = [np.array([0, 1]), np.array([0, 1, 2])` determining the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def solve_sdp(layout, hamiltonian):\n",
    "    \"Solves the SDP defined by the given layout and Hamiltonian.\"\n",
    "    layout = simplify_layout(layout)\n",
    "    H = hamiltonian.to_sdp()\n",
    "    problem = picos.Problem(solver = 'cvxopt')\n",
    "    variables = [(site, picos.HermitianVariable('rho'+','.join(map(str, site)), (2**len(site), 2**len(site)))) for site in layout]\n",
    "    problem.add_list_of_constraints([rho >> 0 for _, rho in variables])\n",
    "    problem.add_list_of_constraints([picos.trace(rho) == 1 for _, rho in variables])\n",
    "    \n",
    "    # Energy\n",
    "    objective = 0\n",
    "    for support, h in H:\n",
    "        supported = False\n",
    "        for sites, rho in variables:\n",
    "            common, _, idx = np.intersect1d(support, sites, return_indices=True)\n",
    "            if len(common) == len(support): \n",
    "                rdm = rho.partial_trace(complementary_system(idx, len(sites)))\n",
    "                objective += (rdm | h) # Tr(rdmÂ·H')\n",
    "                supported = True\n",
    "                break\n",
    "        if not supported:\n",
    "            eigenvalues, _ = np.linalg.eigh(picos2np(h))\n",
    "            objective += min(eigenvalues)\n",
    "                \n",
    "    problem.set_objective('min', objective)\n",
    "    \n",
    "    # Compatibility\n",
    "    compatibility_constraints = []\n",
    "    for k1, (sites1, rho1) in enumerate(variables):\n",
    "        for k2 in range(k1+1, len(variables)):\n",
    "            sites2, rho2 = variables[k2]\n",
    "            common, idx1, idx2 = np.intersect1d(sites1, sites2, return_indices=True)\n",
    "            if len(common) > 0:\n",
    "                partial_trace1 = rho1.partial_trace(complementary_system(idx1, len(sites1)))\n",
    "                partial_trace2 = rho2.partial_trace(complementary_system(idx2, len(sites2)))\n",
    "                constraint = partial_trace1 - partial_trace2 == 0\n",
    "                compatibility_constraints.append(constraint)\n",
    "    \n",
    "    problem.add_list_of_constraints(compatibility_constraints)\n",
    "    \n",
    "    try:    \n",
    "        problem.solve()\n",
    "        result = np.real(objective.value)\n",
    "    except: \n",
    "        print(problem)\n",
    "        result = 0.\n",
    "    return result\n",
    "    \n",
    "def complementary_system(subsystem, size):\n",
    "    \"Obtains the complementary system of subsystem.\"\n",
    "    return list(map(int, np.setdiff1d(np.arange(size), subsystem)))\n",
    "\n",
    "def picos2np(variable):\n",
    "    \"Converts picos variable (even sparse) to numpy matrix.\"\n",
    "    return picos.expressions.data.cvx2np(variable.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from bounce.hamiltonian import XXHamiltonian\n",
    "from bounce.utils import fill_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SdP is built imposing the compatibility contraints from the layout. There, we simply provide a list of `np.array` containing the support for each reduced density matrix (RDM) that we consider. In the following example, we solve the SdP given the example Hamiltonian and imposing compatibility over a single pair of sites. We will consider the RDMs $\\rho_0, \\rho_1, \\rho_2, \\rho_3, \\rho_4, \\rho_{50}$ in a ring with 6 sites, which is the same as the set of constraints $C=\\{\\{0\\},\\{1\\},\\{2\\},\\{3\\},\\{4\\},\\{5,0\\}\\}$. The function `fill_layout` allows us to skip the 1-body terms when providing the set of constraints, as it will fill any site that is not contained within the provided constraints with its corresponding 1-body term.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "B, J = [1]*N, [i%3 for i in range(N)]\n",
    "H = XXHamiltonian(N, B, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Hamiltonian, define the set of constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 5]), array([1]), array([2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_layout = fill_layout([np.array([0, N-1])], N)\n",
    "simple_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can solve the associated SdP to the Hamiltonian with the given set of constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.999999999012134"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_sdp(simple_layout, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tightening the constraints we can obtain a better energy bound. For instance, adding a 3-body constraint to the previous set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]), array([0, 5]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stronger_layout = fill_layout([np.array([1, 2, 3]), np.array([0, N-1])], N)\n",
    "stronger_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.4721359498209"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_sdp(stronger_layout, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost estimation\n",
    "\n",
    "In order to have an estimation of the computational cost required to solve a given SdP, we estimate the amount of free variables in the optimization problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ojimetro(L):\n",
    "    \"Estimates the amount of free parameters in the SDP associated to the layout.\"\n",
    "    # The old blocks are len(L)\n",
    "    L = simplify_layout(L)\n",
    "    all_variables = np.sum([2**(2*len(sites)) for sites in L])\n",
    "    intersections = []\n",
    "    for k, sites1 in enumerate(L[:-1]):\n",
    "        for sites2 in L[k+1:]:\n",
    "            intersections.append(np.intersect1d(sites1, sites2))\n",
    "    intersections = simplify_layout(intersections)\n",
    "    dep_variables = np.sum([2**(2*len(sites)) for sites in intersections])\n",
    "    return all_variables-dep_variables   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following with our previous example, the first set of constraints was rather loose, providing a low energy bound. However, the second set of constraints provided a tighter bound, although the SdP was harder to solve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ojimetro(simple_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ojimetro(stronger_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first set of constraints, the resulting SdP had 31 free variables to optimize, while the second SdP had to deal with 83. Tighter energy bounds usually come at the cost of higher computational costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "We implement two other methods to obtain lower bounds of many-body Hamiltonians. We rewrite the Anderson bound [[1]](https://journals.aps.org/pr/abstract/10.1103/PhysRev.83.1260) and the method introduced by Uskov and Lichovskiy in [[2]](https://iopscience.iop.org/article/10.1088/1742-6596/1163/1/012057) in SdP form to compare with the proposed methodology above. The SdP formulation ensures that the obtained result is the actual global minimum.\n",
    "\n",
    "The main limitation of these methods is not taking into account compatibility constraints between the reduced density matrices spanning the system. Furthermore, they rely on symmetries (such as translational invariance) and can, thus, not be used to solve inhomogeneous systems. Conversely, the approach we introduced above can be nautrally complemented by introducing additional constratints stemming from any previously known symmetry from the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def anderson_bound(hamiltonian, cluster_size=3):\n",
    "    \"Provides the corresponding Anderson bound to the Hamiltonian given a cluster size.\"\n",
    "    if hamiltonian.N % (cluster_size-1) != 0:\n",
    "        raise ValueError(f\"The number of qubits {hamiltonian.N} must be divisible by cluster_size - 1\")\n",
    "    else: \n",
    "        n_clusters = hamiltonian.N // (cluster_size-1) # Assumes 1D nearest-neighbor interaction\n",
    "    H = hamiltonian.to_sdp()\n",
    "    problem = picos.Problem(solver = 'cvxopt')\n",
    "    \n",
    "    sites = np.arange(cluster_size)\n",
    "    rho = picos.HermitianVariable('rho'+','.join(map(str, sites)), (2**cluster_size, 2**cluster_size))\n",
    "    problem.add_constraint(rho >> 0)\n",
    "    problem.add_constraint(picos.trace(rho) == 1)\n",
    "    \n",
    "    # Energy\n",
    "    objective = 0\n",
    "    for support, h in H:\n",
    "        common, _, idx = np.intersect1d(support, sites, return_indices=True)\n",
    "        if len(common) == len(support): \n",
    "            rdm = rho.partial_trace(complementary_system(idx, len(sites)))\n",
    "            objective += (rdm | h) # Tr(rdmÂ·H')\n",
    "    \n",
    "    problem.set_objective('min', objective)\n",
    "    \n",
    "    try:    \n",
    "        problem.solve()\n",
    "        result = np.real(objective.value)\n",
    "    except: \n",
    "        print(problem)\n",
    "        result = 0.\n",
    "    return n_clusters*result\n",
    "\n",
    "def uskov_lichkovskiy_bound(hamiltonian, cluster_size=3):\n",
    "    \"Provides the corresponding Uskov and Lichkovskiy bound to the Hamiltonian given a cluster size.\"\n",
    "    if hamiltonian.N % (cluster_size-1) != 0:\n",
    "        raise ValueError(f\"The number of qubits {hamiltonian.N} must be divisible by cluster_size - 1\")\n",
    "    else: \n",
    "        n_clusters = hamiltonian.N // (cluster_size-1) # Assumes 1D nearest-neighbor interaction\n",
    "    H = hamiltonian.to_sdp()\n",
    "    problem = picos.Problem(solver = 'cvxopt')\n",
    "    \n",
    "    sites = np.arange(cluster_size)\n",
    "    rho = picos.HermitianVariable('rho'+','.join(map(str, sites)), (2**cluster_size, 2**cluster_size))\n",
    "    problem.add_constraint(rho >> 0)\n",
    "    problem.add_constraint(picos.trace(rho) == 1)\n",
    "\n",
    "    # Symmetry constraints: TI and specular symmetry in the cluster\n",
    "    x = picos.Constant('x', [[0, 1], [1, 0]])\n",
    "    y = picos.Constant('y', [[0, -1j], [1j, 0]])\n",
    "    z = picos.Constant('z', [[1, 0], [0, -1]])\n",
    "    Id = picos.Constant('Id', [[1, 0], [0, 1]])\n",
    "    \n",
    "    def tensor_prod(idx, s):\n",
    "        \"Tensor product of `s` acting on indexes `idx`. Fills rest with Id.\"\n",
    "        matrices = [Id if k not in idx else s for k in range(cluster_size)]\n",
    "        prod = matrices[0]\n",
    "        for k in range(1, cluster_size): prod = prod @ matrices[k]\n",
    "        return prod\n",
    "    \n",
    "    def sigma_term(idx): return tensor_prod(idx, x) + tensor_prod(idx, y) + tensor_prod(idx, z)\n",
    "            \n",
    "    idx = [sites[i:i+2] for i in range(len(sites)-1)]\n",
    "    for i in range(len(idx)//2):\n",
    "        left, right = sigma_term(idx[i]), sigma_term(idx[-(i+1)])\n",
    "        problem.add_constraint((left | rho) == (right | rho)) # Tr(sigma_01*rho) = Tr(sigma_{n-1,n}*rho)\n",
    "        adj = sigma_term(idx[i+1])\n",
    "        problem.add_constraint((left | rho) == (adj | rho)) # Tr(sigma_01*rho) = Tr(sigma_12*rho)\n",
    "    \n",
    "    # Energy\n",
    "    objective = 0\n",
    "    for support, h in H:\n",
    "        common, _, idx = np.intersect1d(support, sites, return_indices=True)\n",
    "        if len(common) == len(support): \n",
    "            rdm = rho.partial_trace(complementary_system(idx, len(sites)))\n",
    "            objective += (rdm | h) # Tr(rdmÂ·H')\n",
    "    \n",
    "    problem.set_objective('min', objective)\n",
    "    \n",
    "    try:    \n",
    "        problem.solve()\n",
    "        result = np.real(objective.value)\n",
    "    except: \n",
    "        print(problem)\n",
    "        result = 0.\n",
    "    return n_clusters*result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from bounce.hamiltonian import XYZHamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from [[2]](https://iopscience.iop.org/article/10.1088/1742-6596/1163/1/012057) can be reproduced by properly adjusting the cluster size and using the following Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "linear, quadratic = [0]*N, [1]*N\n",
    "hamiltonian = XYZHamiltonian(N, linear, quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9278862525095595, -1.8685170845449277, -1.868517086761974)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_size = 5\n",
    "bound_anderson = anderson_bound(hamiltonian, cluster_size=cluster_size)\n",
    "bound_uskov = uskov_lichkovskiy_bound(hamiltonian, cluster_size=cluster_size)\n",
    "\n",
    "layout = [np.sort(np.arange(i, i+cluster_size)%N) for i in np.arange(0, N, cluster_size-3)]\n",
    "bound_ours = solve_sdp(layout, hamiltonian)\n",
    "\n",
    "bound_anderson/N, bound_uskov/N, bound_ours/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] P. W. Anderson. [Limits on the Energy of the Antiferromagnetic Ground State](https://journals.aps.org/pr/abstract/10.1103/PhysRev.83.1260). *Physical Review* **83**, 1260 (1951)\n",
    "\n",
    "[2] F. Uskov and O. Lychkovskiy. [A variational lower bound on the ground state of a many-body system and the squaring parametrization of density matrices](https://iopscience.iop.org/article/10.1088/1742-6596/1163/1/012057). *Journal of Physics: Conference Series* **1163** 012057 (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
