{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Definition of the environment with which the agent interacts. The environment\n",
    "  handles the execution of the actions and provides the rewards.\n",
    "output-file: environment.html\n",
    "title: Environment\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/environment.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SdPEnvironment\n",
       "\n",
       ">      SdPEnvironment (problem, sdp_solver, budget,\n",
       ">                      reward_criterion='bound_norm', layout_basis=None,\n",
       ">                      max_basis_size=None, initial_size=1, bound_tol=0.001)\n",
       "\n",
       "Environment for constraint-space exploration."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/environment.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SdPEnvironment\n",
       "\n",
       ">      SdPEnvironment (problem, sdp_solver, budget,\n",
       ">                      reward_criterion='bound_norm', layout_basis=None,\n",
       ">                      max_basis_size=None, initial_size=1, bound_tol=0.001)\n",
       "\n",
       "Environment for constraint-space exploration."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(SdPEnvironment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains all the information of the problem at hand, and controls the way the agent can explore the state space. \n",
    "\n",
    "Hence, we must provide the [`SdPEnvironment`](https://BorjaRequena.github.io/BOUNCE/environment.html#sdpenvironment) with an instance of our problem, such as a [`Hamiltonian`](https://BorjaRequena.github.io/BOUNCE/hamiltonian.html#hamiltonian), an adequate solver that defines the objective, such as an [`SdPEnergySolver`](https://BorjaRequena.github.io/BOUNCE/sdp.html#sdpenergysolver). \n",
    "\n",
    "Then, we need to provide information about the reinforcement learning task, such as the computational budget we can afford, and the reward function that we want to use. Optionally, we can provide a pre-defined layout basis, which conditions the kind of constraints that the agent can create. Otherwise, the environment generates its own basis directly from the problem instance and it underlying structure, e.g., the `Hamiltonian.graph`.\n",
    "\n",
    "## Create an environment\n",
    "Let's see an example. We will start by defining the problem parameters. In this case, we will try to find the ground state energy of a Heisenberg XX Hamiltonian on a one-dimensional chain. Furthermore, we will consider a moderate budget that only allows us to consider 3-body reduced density matrices to compute the bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from bounce.hamiltonian import XXHamiltonian, Chain1D\n",
    "from bounce.sdp import SdPEnergySolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "chain = Chain1D(N)\n",
    "B, J = 3, 1\n",
    "H = XXHamiltonian(chain, B, J)\n",
    "solver = SdPEnergySolver()\n",
    "budget = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can create our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "env = SdPEnvironment(H, solver, budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment starts in the simplemost state, which corresponds to a trivial relaxation of the problem. This is represented by a state of zeros, indicating that no compatibility constraints are active at this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert (env.state == np.zeros(2*N)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state vector serves as a boolean indicator of which constraints in the environment's basis are currently active, although the vector is not boolean itself because it has to be processed by the agents. \n",
    "\n",
    "In this case, the environment's basis is automatically generated from the `Hamiltonian.graph.edges` and the budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0, 1]), array([1, 2]), array([2, 3]), array([3, 4]),\n",
       "       array([0, 4]), array([0, 1, 2]), array([0, 1, 4]),\n",
       "       array([1, 2, 3]), array([2, 3, 4]), array([0, 3, 4])], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment associates a unique layout to every state which depends on the basis and the active constraints. In this case, the layout is made out of single-body reduced density matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([1]), array([2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment takes care of providing the solver the needed information of the problem. In this case, it feeds the [`SdPEnergySolver`](https://BorjaRequena.github.io/BOUNCE/sdp.html#sdpenergysolver) both the data from the Hamiltonian and the current layout. We can solve the associated semidefinite program (SdP) to the active constraints with the `get_values` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.99999999732747, 19, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound, cost, error = env.get_values()\n",
    "bound, cost, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert bound == solver.solve(H.to_sdp(), env.layout)\n",
    "assert cost == solver.ojimetro(env.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we visit a new state, the environment keeps track of the active values. This way, it builds a reference of the best possible bounds that can be obtained. For now, given that we have not moved from the initial state, the best bound and layout are the only ones it has ever seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.99999999732747,\n",
       " [array([0]), array([1]), array([2]), array([3]), array([4])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_bound, env.best_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess which is the best layout, it accounts for both the best possible bound and the lowest possible cost with which it has ever been achieved. This is stored in `best` which contains the best possible bound together with the maximum an minimum costs it was ever achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.,  19.,  19.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert bound == env.max_bound\n",
    "assert env.best_layout == env.layout\n",
    "assert (env.best == np.array([bound, cost, cost])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can move to new states by performing an action. Actions are, in essence, very simple. They are integers that indicate which bit we flip in the state vector. Doing so, adds or removes constraints in the associated SdP.\n",
    "\n",
    "In order to test this out, let's add the first 3-body constraint, indexed by `N` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 0, 0, 1, 0, 0, 0, 0]), -20.999999995348936, 71)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, bound, cost, error = env.perform_action(N)\n",
    "new_state, bound, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that now we have a higher energy bound. In this case, the higher the better, so it's nice! However, the associated cost is also much larger than before. \n",
    "\n",
    "Additionally, despite performing a single action, we observe that now there are three \"ones\" in our state vector. This is because the basis element in the `N`-th position, contained two other elements of smaller size. We can see the state vector by size with the method `show_constraints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 0 0 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] contains [0 1] and [1 2].\n"
     ]
    }
   ],
   "source": [
    "print(f\"{env.layout_basis[N]} contains {env.layout_basis[0]} and {env.layout_basis[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these two smaller ones are not present in the laoyut, as they would be redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this with the [`contained_constraints`](https://BorjaRequena.github.io/BOUNCE/utils.html#contained_constraints) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.contained_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we ensure a consistent exploration through state space. Now, if we removed the larger element, the two smaller ones would remain, ensuring that there are no big leaps in the exploration and limiting the actions that can be performed. The agent cannot attempt to add or remove elements contained in larger ones. We can see this with the [`action_mask`](https://BorjaRequena.github.io/BOUNCE/utils.html#action_mask) method, which indicates the state vector positions that can be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the action mask is one element longer than the state vector because it accounts for the action of \"remaining still\". \n",
    "\n",
    "Since we have moved to a state that provides a better bound for our problem, the references have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.999999995348936, [array([0, 1, 2]), array([3]), array([4])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_bound, env.best_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert (new_state == env.state).all()\n",
    "assert bound == env.max_bound\n",
    "assert (bound, cost, 0) == env.get_values()\n",
    "assert (env.best == np.array([bound, cost, cost])).all()\n",
    "assert np.all([(l_best == l).all() for l_best, l in zip(env.best_layout, env.layout)])\n",
    "assert (env.contained_constraints() == np.array([True, True] + [False]*(2*N-2))).all()\n",
    "assert (env.action_mask() == np.array([False, False] + [True]*(2*N-1))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the action, we remove the larger element and the two smaller ones remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "new_state, bound, cost, _ = env.perform_action(N)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1]), array([1, 2]), array([3]), array([4])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so, we have reduced the associated cost of the state. However, the bound has remained the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.99999999834299, 36)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the main motivations behind the work [Certificates of quantum many-body properties assisted by machine learning](https://arxiv.org/abs/2103.03830). We can see this reflected in the environment's references, which now shows the minimum and maximum costs with which the best bound has ever been obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-21.,  36.,  71.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration boundaries and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to exceed the allowed computational budget with an action, we would obtain an error indicator. The erorr code is:\n",
    "- 0: all good! \n",
    "- 1: there could not be found a solution for the associated SdP \n",
    "- 2: the associated SdP cost exceeds the computational budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 1 1 1]\n",
      "3: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "env.perform_action(N+3)\n",
    "new_state, bound, cost, err = env.perform_action(N+4)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 132, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err, cost, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert err == 2\n",
    "assert cost > budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are the way through which we establish \"walls\" in the state space. This way, agents do not need to learn the boundaries of the feasible region nor need to keep track of it. \n",
    "\n",
    "The environment prevents threspassing the state space boundaries with the [`step`](https://BorjaRequena.github.io/BOUNCE/training.html#step) method. Rather than taking a single action, such as the `perform_action` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/environment.py#L83){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SdPEnvironment.step\n",
       "\n",
       ">      SdPEnvironment.step (actions)\n",
       "\n",
       "Receives a list of actions (priority ordered) and executes them until one succeeds.\n",
       "Input:  - actions: collection of actions (iterable of ints).\n",
       "Output: - next_state: new state after performing the chosen action.\n",
       "        - action: action that was actually performed among the input ones.\n",
       "        - bound: bound associated to the new state.\n",
       "        - cost: cost associated to the new state.\n",
       "        - err: error code (should be zero)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/environment.py#L83){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SdPEnvironment.step\n",
       "\n",
       ">      SdPEnvironment.step (actions)\n",
       "\n",
       "Receives a list of actions (priority ordered) and executes them until one succeeds.\n",
       "Input:  - actions: collection of actions (iterable of ints).\n",
       "Output: - next_state: new state after performing the chosen action.\n",
       "        - action: action that was actually performed among the input ones.\n",
       "        - bound: bound associated to the new state.\n",
       "        - cost: cost associated to the new state.\n",
       "        - err: error code (should be zero)."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(SdPEnvironment.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`step`](https://BorjaRequena.github.io/BOUNCE/training.html#step) method takes a list of priority-ordered actions. The method performes them in the given order until one succeeds, ensuring that we never land on an error-flagged state.\n",
    "\n",
    "In order to test this out, let's first reset the environment to the initial state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performed action was 5.\n",
      "2: [1 1 0 0 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "new_state, action, bound, cost, error = env.step([5, 6, 7, 0, 1])\n",
    "print(f\"The performed action was {action}.\")\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert action == 5\n",
    "assert error == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now try to add an additional large constraint, we will exceed the computational budget. However, the step method will perform the first suitable action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performed action was 3.\n",
      "2: [1 1 0 1 0]\n",
      "3: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "actions_to_try = [8, 6, 3, 5]\n",
    "new_state, action, bound, cost, error = env.step(actions_to_try)\n",
    "print(f\"The performed action was {action}.\")\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert action == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`step`](https://BorjaRequena.github.io/BOUNCE/training.html#step) method has skipped the first two actions in the priority list to avoid exceeding the budget. In the extreme case in which none of the actions provided can be executed, the function returns the resutls for the last action it tried, although the environment has not advanced. This should never happen if we provide all possible actions ranked and the state-space is properly designed. Intuitively, there should never be rabbit holes the agent can't escape from.\n",
    "\n",
    "## Initial state\n",
    "\n",
    "As we've seen before, the exploration starts at the zero state by default. However, in some cases it may be more convenient to start somewhere else. We can specify the initial constraint size to the [`SdPEnvironment`](https://BorjaRequena.github.io/BOUNCE/environment.html#sdpenvironment) such that the initial state comprises all the basis elements of such size.\n",
    "\n",
    "Let's see how this works in the current example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [1 1 1 1 1]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env = SdPEnvironment(H, solver, budget, initial_size=2)\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert (env.state[:N] == 1).all()\n",
    "assert (env.state[N:] == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom basis\n",
    "\n",
    "We can have a major impact in the state space and its boundaries by specifying a layout basis of our choice. For instance, we may be interested in finding bounds to a problem using a certain kind of constraints.\n",
    "\n",
    "Let's take the previous example and set a specific basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "layout_basis = np.array([np.array([0, 1]), np.array([2, 3]), np.array([3, 4, 5]), np.array([0, 1, 4, 5])], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "env = SdPEnvironment(H, solver, budget, layout_basis=layout_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0, 1]), array([2, 3]), array([3, 4, 5]),\n",
       "       array([0, 1, 4, 5])], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.layout_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert np.all([(lb == le).all() for lb, le in zip(layout_basis, env.layout_basis)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have basis elements of different sizes with various overlaps. For instance, in this case there are no 2-body elements contained in the 3-body one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0]\n",
      "3: [1]\n",
      "4: [0]\n"
     ]
    }
   ],
   "source": [
    "env.step([2])\n",
    "env.show_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some times we may want to use a fully customized basis, some times we simply care about truncating it. For instance, we may have a budget that allows us to reach up to four-body elements, but we wish to invest the resources into using exclusively as many three-body elements as possible. In these cases, we can specify a maximum size for the basis elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "budget = 300\n",
    "max_basis_size = 3\n",
    "env_unrestricted = SdPEnvironment(H, solver, budget)\n",
    "env_restricted = SdPEnvironment(H, solver, budget, max_basis_size=max_basis_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n",
      "4: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env_unrestricted.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [0 0 0 0 0]\n",
      "3: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "env_restricted.show_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert max([len(l) for l in env_restricted.layout_basis]) == max_basis_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Besides providing a consistent state space and exploration rules, the environment is in charge of providing feedback to the agent. When we initialize the environment, we choose the desired criterion for the reward. By default, we use the `bound_norm_reward`, with which we have obtained the best results so far and we have performed all the calculations in [[1]](https://arxiv.org/abs/2103.03830)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bound_norm_reward'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_fun.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert env.reward_fun.__name__ == 'bound_norm_reward'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that, in general, we do not know the optimal relaxation, the environment relies on the references it has gathered during the exploration to provide the rewards. We can choose among:\n",
    "- **bound_reward**: the reward is the obtained bound, regardless of the cost. It can take any arbitrary value depending on the problem.\n",
    "- **bound_norm_reward**: the reward is a normalized function between zero and one which accounts for both the value of the bound and the associated cost. \n",
    "- **bound_improve_reward**: the reward is the improvement of a bound with respect to the minimum one ever observed. \n",
    "\n",
    "We select the criterion by providing the name of the function excluding the `'_reward'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The environment implements a memory that stores the SdP solution of all the visited states. This way, we do not need to instantiate the SdP and solve it every time we revisit a state, speeding up the whole process. We can save the memory with the `save_memory` method and it will be automatically loaded when dealing with the same problem. The limit stored solutions in the memory is 1e6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../memories/memory_Energy_xx_N5_Chain1D_terms_3.00_3.00_3.00_3.00_3.00_1.00_1.00_1.00_1.00_1.00.pkl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.memory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] B. Requena, G. Mu√±oz-Gil, M. Lewenstein, V. Dunjko, J. Tura. [Certificates of quantum many-body properties assisted by machine learning](https://arxiv.org/abs/2103.03830). *arXiv:2103.03830 (2021)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
