{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The agents are the entities that perform the learning process to, ultimately,\n",
    "  accomplish a task.\n",
    "output-file: agents.html\n",
    "title: Agents\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "The agents based on reinforcement learning implement a value-based algorithm called Q-learning. More precisely, the agent implemented in this framework is based on [deep double Q-learning](https://ojs.aaai.org/index.php/AAAI/article/view/10295)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DQNAgent\n",
       "\n",
       ">      DQNAgent (model, learning_rate=0.001, criterion=None, optimizer=None,\n",
       ">                batch_size=128, target_update=5, gamma=0.85, eps_0=1,\n",
       ">                eps_decay=0.999, eps_min=0.1)\n",
       "\n",
       "Agent based on a deep Q-Network (DQN):\n",
       "Input: \n",
       "    - model: torch.nn.Module with the DQN model. Dimensions must be consistent\n",
       "    - criterion: loss criterion (e.g., torch.nn.SmoothL1Loss)\n",
       "    - optimizer: optimization algorithm (e.g., torch.nn.Adam)\n",
       "    - eps_0: initial epsilon value for an epsilon-greedy policy\n",
       "    - eps_decay: exponential decay factor for epsilon in the epsilon-greedy policy\n",
       "    - eps_min: minimum saturation value for epsilon\n",
       "    - gamma: future reward discount factor for Q-value estimation"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DQNAgent\n",
       "\n",
       ">      DQNAgent (model, learning_rate=0.001, criterion=None, optimizer=None,\n",
       ">                batch_size=128, target_update=5, gamma=0.85, eps_0=1,\n",
       ">                eps_decay=0.999, eps_min=0.1)\n",
       "\n",
       "Agent based on a deep Q-Network (DQN):\n",
       "Input: \n",
       "    - model: torch.nn.Module with the DQN model. Dimensions must be consistent\n",
       "    - criterion: loss criterion (e.g., torch.nn.SmoothL1Loss)\n",
       "    - optimizer: optimization algorithm (e.g., torch.nn.Adam)\n",
       "    - eps_0: initial epsilon value for an epsilon-greedy policy\n",
       "    - eps_decay: exponential decay factor for epsilon in the epsilon-greedy policy\n",
       "    - eps_min: minimum saturation value for epsilon\n",
       "    - gamma: future reward discount factor for Q-value estimation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DQNAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a default architecture for the neural network that encodes the Q-values, usually referred to as deep Q-Network (DQN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DQN\n",
       "\n",
       ">      DQN (state_size, action_size)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super(Model, self).__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DQN\n",
       "\n",
       ">      DQN (state_size, action_size)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super(Model, self).__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind-search\n",
    "\n",
    "The agents based on tree search currently only implement blind-search techniques, such as breadth first search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BrFSAgent\n",
       "\n",
       ">      BrFSAgent (initial_state)\n",
       "\n",
       "Agent based on Breadth First Search (BrFS)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BrFSAgent\n",
       "\n",
       ">      BrFSAgent (initial_state)\n",
       "\n",
       "Agent based on Breadth First Search (BrFS)."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(BrFSAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "agent = BrFSAgent(np.array([1, 1, 1, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 1, 0, 0, 0]),\n",
       " array([1, 0, 1, 0, 0, 0]),\n",
       " array([1, 1, 0, 0, 0, 0]),\n",
       " array([1, 1, 1, 1, 0, 0]),\n",
       " array([1, 1, 1, 0, 1, 0]),\n",
       " array([1, 1, 1, 0, 0, 1])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.expand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo\n",
    "\n",
    "The agents based on Monte-Carlo sampling follow the Metropolis-Hastings algorithm to move between states. A random action (new state) is proposed and the move is accepted or rejected with a certain probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MCAgent\n",
       "\n",
       ">      MCAgent (beta=0.1)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/BorjaRequena/BOUNCE/tree/master/blob/master/BOUNCE/agents.py#L174){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MCAgent\n",
       "\n",
       ">      MCAgent (beta=0.1)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MCAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_environment.ipynb.\n",
      "Converted 01_agents.ipynb.\n",
      "Converted 02_budget_profiles.ipynb.\n",
      "Converted 03_hamiltonian.ipynb.\n",
      "Converted 04_training.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted 06_sdp.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
