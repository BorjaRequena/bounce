[
  {
    "objectID": "tutorial/benchmarking.html",
    "href": "tutorial/benchmarking.html",
    "title": "Benchmarking agents",
    "section": "",
    "text": "In the first tutorial, we have seen how to define a problem and optimize its possible relaxations. We also mentioned that we can perform the optimization in different ways. Here, we show how to implement different optimization processes and how to compare them in a consistent way.\nMore precisely, we study how the reinforcement learning (RL) optimization compares to a breadth first search (BFS) and a Monte Carlo (MC) optimization."
  },
  {
    "objectID": "tutorial/benchmarking.html#common-problem",
    "href": "tutorial/benchmarking.html#common-problem",
    "title": "Benchmarking agents",
    "section": "Common problem",
    "text": "Common problem\nIn order to compare the optimizers, we need something to optimize! So, the first step is to define a problem. We will continue with the same example of the first tutorial: finding an approximation to the ground state energy of the Heisenberg XX model.\n\n# Hamiltonian\nN = 6\nchain = Chain1D(N)\nb, j = 1., [0, 1, 2]\nH = XXHamiltonian(chain, b, j)\n\n# Solver\nsolver = SdPEnergySolver()\n\n# Environment\nbudget = 305\nenv = SdPEnvironment(H, solver, budget)"
  },
  {
    "objectID": "tutorial/benchmarking.html#evaluation-metric-towards-the-goal",
    "href": "tutorial/benchmarking.html#evaluation-metric-towards-the-goal",
    "title": "Benchmarking agents",
    "section": "Evaluation metric towards the goal",
    "text": "Evaluation metric towards the goal\nAnother key element is to find a proper evaluation metric for the agents. In this case, we exploit the clear properties of the system to find the optimal relaxation beforehand. This way, we can evaluate “how close” are the agents to the optimal solution at every step they take.\n\nH.draw()\n\n\n\n\nAs we had found in the first tutorial, the optimal relaxation provides a bound of -10.944 with 127 free SdP parameters. In this case, we can obtain the same bound investing all our resources, which involves 288 free parameters. These numbers clearly depend on the computational budget. We can provide this information to the trainers and they will compute the reward with respect to these parameters, which will allow us to compare the progress of the different algorithms towards the goal.\n\n# Training parameters\nepisodes = 400\ntime_steps = 6\nopt = (-10.9443, 127) \nbest_ref = np.array([*opt, 288])\nn_agents, jobs = 5, 5 # Number of agents to train and parallel threads"
  },
  {
    "objectID": "tutorial/benchmarking.html#run-the-different-algorithms",
    "href": "tutorial/benchmarking.html#run-the-different-algorithms",
    "title": "Benchmarking agents",
    "section": "Run the different algorithms",
    "text": "Run the different algorithms\nNow we’re all set and ready to run the different algorithms. We simply need to define the respective trainers for each of them. All the trainers have a similar sintax.\n\nlearning_rate = 5e-3\nbatch_size = 160\neps_decay = 0.95\n\ndqn = DQNTrainer(env, n_agents=n_agents, n_jobs=jobs, \n                 learning_rate=learning_rate, batch_size=batch_size, \n                 eps_decay=eps_decay)\n\nbrfs = BrFSTrainer(env, n_agents=n_agents, n_jobs=jobs)\n\nmc = MCTrainer(env, n_agents=n_agents, n_jobs=jobs)\n\nTrain!\n\n\n\nCPU times: user 4.09 s, sys: 233 ms, total: 4.32 s\nWall time: 21.2 s\n\n\n\nmax_states = int(0.4*episodes*time_steps)\n\nCPU times: user 119 ms, sys: 1.78 ms, total: 121 ms\nWall time: 731 ms\n\n\n\n\n\nCPU times: user 89.3 ms, sys: 0 ns, total: 89.3 ms\nWall time: 870 ms\n\n\nLet’s see the results!\n\nimport matplotlib.pyplot as plt\n\n\ndqn_expl = dqn_results[\"exploration\"]\ndqn_rewards = arrange_shape(dqn_expl['oracle_rewards'])\nbrfs_rewards = arrange_shape(brfs_results['oracle_rewards'])\nmc_rewards = arrange_shape(mc_results['oracle_rewards'])\n\nplt.figure(figsize=(10, 4))\nplt.plot(np.mean(dqn_rewards, axis=0), label=\"RL\")\nplt.plot(np.mean(brfs_rewards, axis=0), label=\"BFS\")\nplt.plot(np.mean(mc_rewards, axis=0), label=\"MC\")\nplt.grid()\nplt.legend(fontsize=16)\nplt.tick_params(labelsize=16)\nplt.xlabel(\"New visited state\", fontsize=20)\nplt.ylabel(\"Reward\", fontsize=20)\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(np.mean(best_so_far(dqn_rewards), axis=0), linewidth=2, label=\"RL\")\nplt.plot(np.mean(best_so_far(brfs_rewards), axis=0), linewidth=2, label=\"BFS\")\nplt.plot(np.mean(best_so_far(mc_rewards), axis=0), linewidth=2, label=\"MC\")\nplt.grid()\nplt.legend(fontsize=16)\nplt.tick_params(labelsize=16)\nplt.xlabel(\"New visited state\", fontsize=20)\nplt.ylabel(\"Best reward\", fontsize=20);\n\n\n\n\n\n\n\nIn these plots we see the progress that the different agents do towards the goal. In the first one, we see how close are the agents to the optimal relaxation, on average, with every new step they take. This allows us to see how the different agents explore the constraint-space. For instance, we see a clear collective trend in the BFS exploration, as they all follow a similar exploration strategy.\nThe second plot provides a clearer idea of the progress towards the goal. In this case, we take the mean over all agents of their best ever reward. Hence, we obtain monotonic lines that show the ensemble progress towards the optimal relaxation: once they reach unit value it means that all agents have found the optimal relaxation. In our work, we use this information to compare the different optimization strategies in multiple scenarios, which we show below."
  },
  {
    "objectID": "tutorial/transfer_learning.html",
    "href": "tutorial/transfer_learning.html",
    "title": "Transfer learning",
    "section": "",
    "text": "In this notebook, we show how to leverage transfer learning to improve our optimization results. Trasfer learning is a machine learning technique with which we use the experience obtained solving a determined problem to speed up the solution of a similar (yet different) problem.\nIn our case, we start by training an agent to find an optimal relaxation with a given Hamiltonian. Then, we can exploit the knowledge gathered by this agent to solve a similar problem, e.g., finding the optimal relaxation with a perturbation of the original Hamiltonian. In order to do so, instead of restarting the learning process from scratch, we take the trained agent from the first problem as starting point to solve the second problem.\nIn the first tutorial, we used a DQNTrainer to optimize our relaxations. This class provides two ways to do transfer learning. The most straightfoward method is by, simply, training the agents to solve a problem and, then, change the already existing environment and some of the agent hyper-parameters to solve the second problem. The second approach consists on instancing a new DQNTrainer for the second problem with an agent.model list containing the pre-trained newtorks. We recommend to use this second approach as it has less risk of having data leakages and we will usually save the trained models in the disk anyway.\nWith transfer learning, we can obtain advantages in three main aspects:"
  },
  {
    "objectID": "tutorial/transfer_learning.html#problem-definition",
    "href": "tutorial/transfer_learning.html#problem-definition",
    "title": "Transfer learning",
    "section": "Problem definition",
    "text": "Problem definition\nHere, we will define our two problems characterized by similar Hamiltonians: * The initial Hamiltonian H1. * The second Hamiltonian H2, similar to H1.\n\nN = 6\nchain = Chain1D(N)\n\n# Hamiltonian 1\nb, j = 1., [i%3 for i in range(N)]\nH1 = XXHamiltonian(chain, b, j)\n\n# Hamiltonian 2\nb, j = 1., [0, 0, 2, 0, 1, 2]\nH2 = XXHamiltonian(chain, b, j)\n\nLet’s see how they look!\n\nH1.draw()\n\n\n\n\n\nH2.draw()\n\n\n\n\nThey are fairly similar, as they share an entire equal section. If both Hamiltonians had the same ground state, the transfer learning would be completely trivial, provided that the optimal relaxation would be the same. As we show in our work, we can exploit this to explore the phase diagram of the Hamiltonians, but let’s not get ahead of ourselves here! We still need to finish defining our environment.\n\n# Solver\nsolver = SdPEnergySolver()\n\n# Environment\nbudget = 300\nenv1 = SdPEnvironment(H1, solver, budget)\nenv2 = SdPEnvironment(H2, solver, budget)"
  },
  {
    "objectID": "tutorial/transfer_learning.html#the-agent",
    "href": "tutorial/transfer_learning.html#the-agent",
    "title": "Transfer learning",
    "section": "The agent",
    "text": "The agent\nNow that we have our problem instances, we can define our agent to figure out the optimal relaxation for the task.\n\n# Agent parameters\nlearning_rate = 2e-3\nbatch_size = 200\neps_0 = 1.\neps_decay = 0.995\ntarget_update = 5\n\n# Training parameters\nepisodes = 750\ntime_steps = 6\nopt1 = (-10.9443, 127) \nopt2 = (-10.4721, 83)\nbest_ref2 = np.array([*opt2, 288]) \nn_agents, jobs = 10, 10 # Parallel trainings and cores"
  },
  {
    "objectID": "tutorial/transfer_learning.html#solve-the-first-problem",
    "href": "tutorial/transfer_learning.html#solve-the-first-problem",
    "title": "Transfer learning",
    "section": "Solve the first problem",
    "text": "Solve the first problem\nLet us define the DQNTrainer and train an agent to find the optimal solution to the first problem.\n\ndqn = DQNTrainer(env1, n_agents=n_agents, n_jobs=jobs,\n                learning_rate=learning_rate, batch_size=batch_size,\n                 eps_decay=eps_decay, eps_0=eps_0, target_update=target_update)\n\n\nplot_trainings(results1[\"training\"])"
  },
  {
    "objectID": "tutorial/transfer_learning.html#change-the-problem",
    "href": "tutorial/transfer_learning.html#change-the-problem",
    "title": "Transfer learning",
    "section": "Change the problem",
    "text": "Change the problem\nTo properly do transfer learning, we not only have to change the environment, but we also have to reset some of the agent attributes. For example, we need to reset the episodic memory for replay, and we can choose to change the \\(\\varepsilon\\) for exploration in the \\(\\varepsilon\\)-greedy policy. We can use the methods DQNTrainer.change_environment and DQNTrainer.set_agent_attrs.\n\ndqn.change_environment(problem=H2) # Change environment hamiltonian\ndqn.set_agent_attrs(epsilon=0.8*eps_0, memory=deque(maxlen=10000)) # Reset epsilon and memory\n\n\nplot_trainings(tl_results[\"training\"])"
  },
  {
    "objectID": "tutorial/transfer_learning.html#compare-against-directly-solving-the-problem",
    "href": "tutorial/transfer_learning.html#compare-against-directly-solving-the-problem",
    "title": "Transfer learning",
    "section": "Compare against directly solving the problem",
    "text": "Compare against directly solving the problem\nIn order to see whether we obtain any kind of advantage with transfer learing, we need to compare the result with respect to a training without any prior knowledge. Therefore, let’s create our second problem and solve it from scratch.\n\ndqn = DQNTrainer(env2, n_agents=n_agents, n_jobs=jobs,\n                 learning_rate=learning_rate, batch_size=batch_size,\n                 eps_decay=eps_decay, eps_0=eps_0, target_update=target_update)\n\n\nplot_trainings(base_results['training'])\n\n\n\n\nIn this case, we observe a clear jump-start advantage with transfer learning over a cold start. We also observe that some agents find the optimal relaxation sooner in the exploration with transfer learning and the convergence is faster. However, we do not see an asymptotic advantage.\nWe can also look at the exploration results of both approaches.\n\n\nCode\nbase_expl = base_results['exploration']\nbase_rewards = arrange_shape(base_expl['oracle_rewards'])\ntl_expl = tl_results[\"exploration\"]\ntl_rewards = arrange_shape(tl_expl['oracle_rewards'])\n\nplt.figure(figsize=(10, 4))\nplt.plot(np.mean(best_so_far(base_rewards), axis=0), linewidth=2, label=\"No pre-training\")\nplt.plot(np.mean(best_so_far(tl_rewards), axis=0), linewidth=2, label=\"Transfer learning\")\nplt.grid()\nplt.legend(fontsize=16)\nplt.tick_params(labelsize=16)\nplt.xlabel(\"New visited state\", fontsize=20)\nplt.ylabel(\"Best reward\", fontsize=20);\n\n\n\n\n\nIn this case, we see an early advantage for the transfer learning agents. Unfortunately, in this example we have found that the second transfer learning agent has got stuck heavily and it hinders their statistics. In order to properly compare both approaches, we need to gather more statistics than what we have in this small example :)\n\n\n\n\n\n\nWarning\n\n\n\nJust like transfer learning can be beneficial in some cases, it can be completely irrelevant in others. However, it can also be harmful, so be careful!"
  },
  {
    "objectID": "tutorial/transfer_learning.html#load-the-pre-trained-models",
    "href": "tutorial/transfer_learning.html#load-the-pre-trained-models",
    "title": "Transfer learning",
    "section": "Load the pre-trained models",
    "text": "Load the pre-trained models\nIn this case, we can load the models trained to solve H1. We use the function load_model which outpus a dictionary containing both the acutal pytorch model and the mdoel’s state_dict. The quantities can be accessed with the keys 'model' and 'state_dict'.\n\npre_trained_models = [load_model(H1, budget, ID)['model'] for ID in range(n_agents)]\nprint(f'We have loaded {len(pre_trained_models)} models')\nprint(pre_trained_models[0])\n\nWe have loaded 10 models\nDQN(\n  (fc1): Linear(in_features=18, out_features=54, bias=True)\n  (fc2): Linear(in_features=54, out_features=38, bias=True)\n  (fc3): Linear(in_features=38, out_features=38, bias=True)\n  (fc4): Linear(in_features=38, out_features=19, bias=True)\n)"
  },
  {
    "objectID": "tutorial/transfer_learning.html#train-the-models",
    "href": "tutorial/transfer_learning.html#train-the-models",
    "title": "Transfer learning",
    "section": "Train the models",
    "text": "Train the models\nLet’s now create the DQNTrainer to solve the second problem H2 with the pre-trained models. Whenever we input a collection of models, n_agents is automatically adjusted but we still need to choose the amount of parallel threads.\n\ndqn_from_models = DQNTrainer(env2, models=pre_trained_models, n_jobs=len(pre_trained_models),\n                             eps_0=0.8)\n\n\ntrain_results_models = results_from_models['training'] \nplot_trainings(train_results_models)"
  },
  {
    "objectID": "tutorial/transfer_learning.html#the-task",
    "href": "tutorial/transfer_learning.html#the-task",
    "title": "Transfer learning",
    "section": "The task",
    "text": "The task\nWe start by defining the source task H0 in which we will train our models for the first time. This is deep in the product state phase at \\(B/J=5\\). Then we define the tasks to which we will perform transfer learning Hs, which are points across the phase diagram \\(0\\leq B/J \\leq 4\\).\n\nN = 6\nchain = Chain1D(N)\n\n# Initial Hamiltonian\nB0, J0 = 5, 1\nH0 = XXHamiltonian(chain, B0, J0)\n\n# Hamiltonians to transfer\nBs = [np.round(0.1*k, 2) for k in range(20)] + [np.round(2+0.5*k, 2) for k in range(5)]\nHs = [XXHamiltonian(chain, b, J0) for b in Bs]\n\n#Computational budget\nbudget = 185\n\n# Solver\nsolver = SdPEnergySolver()\n\n# Environment\nenv = SdPEnvironment(H0, solver, budget)"
  },
  {
    "objectID": "tutorial/transfer_learning.html#the-agent-1",
    "href": "tutorial/transfer_learning.html#the-agent-1",
    "title": "Transfer learning",
    "section": "The agent",
    "text": "The agent\nNow we set our agent parameters and train it on the source task. The saved agents will be used as starting point for the other tasks.\n\n# Agent parameters\nlearning_rate = 5e-3\nbatch_size = 150\neps_decay = 0.996\ntarget_update = 5\n\nepisodes = 800\ntime_steps = 7\nn_agents, n_jobs = 60, 15\n\nRemember to only train the source agents once!\n\ndqn = DQNTrainer(env, n_agents=n_agents, n_jobs=n_jobs,\n                 learning_rate=learning_rate, batch_size=batch_size,\n                 eps_decay=eps_decay, target_update=target_update)\n\n\nplot_trainings(results['training'])\n\n\n\n\nWe see they have converged nicely! ::: {.callout-note} These results were obtained with BOUNCE v1, hence the different plot style. However, they are completely reproducible if you have a few hours and a nice GPU :P :::"
  },
  {
    "objectID": "tutorial/transfer_learning.html#explore-the-phase-diagram",
    "href": "tutorial/transfer_learning.html#explore-the-phase-diagram",
    "title": "Transfer learning",
    "section": "Explore the phase diagram",
    "text": "Explore the phase diagram\nWe can now find the optimal relaxation for all the different Hamiltonians both with transfer learning and starting from scratch. For this case, we will look at the convergence time and see whether we observe any advantage.\n\nTL_evaluation = {B[0]: dict.fromkeys(['tl', 'vanilla', 'time_ratio']) for B in Bs}\nfor B, H in tqdm(zip(Bs, Hs)):\n    env = SdPEnvironment(H, solver, budget)\n    pre_trained_models = [load_model(H0, budget, ID)['model']\n                          for ID in range(n_agents, 2*n_agents)]\n    \n    # Solve from scratch\n    dqn_cold_start = DQNTrainer(env, n_agents=len(pre_trained_models), n_jobs=n_jobs, \n                                learning_rate=learning_rate, batch_size=batch_size,\n                                eps_decay=eps_decay, target_update=target_update)\n    results = dqn_cold_start.train(episodes, time_steps=time_steps)\n    time = convergence_time(results)\n    TL_evaluation[B]['vanilla'] = deepcopy(results['training'])\n    del dqn_cold_start; del results\n    \n    # Solve with transfer learning\n    dqn_from_models = DQNTrainer(env, models=pre_trained_models, n_jobs=n_jobs, \n                                 learning_rate=learning_rate, batch_size=batch_size,\n                                 eps_0=eps_0, eps_decay=eps_decay, target_update=target_update)\n    results_tl = dqn_from_models.train(episodes, time_steps=time_steps, save=False)\n    time_tl = convergence_time(results_tl)\n    TL_evaluation[B]['tl'] = deepcopy(results_tl['training'])\n    del dqn_from_models; del results_tl\n    \n    time = convergence_time(TL_evaluation[B]['vanilla'])\n    TL_evaluation[B]['time_ratio'] = time_tl/time\n\n\n\n\nNow we can plot the ratio between the convergence time with transfer learning \\(t_{TL}\\), and the convergence time starting from scratch \\(t_0\\).\n\n\nCode\nBs, time_ratios, time_errs = get_indiv_times(TL_evaluation)\n\nplt.figure(figsize=(10, 5))\nplt.fill_between(Bs, time_ratios-time_errs, time_ratios+time_errs, alpha=0.25)\nplt.plot(Bs, time_ratios, 's-', ms=7)\nplt.grid()\nplt.tick_params(labelsize=16)\nplt.xlabel(\"B/J\", fontsize=20)\nplt.ylabel(r\"Time ratio $t_{TL}/t_0$\", fontsize=20);\n\n\n\n\n\nIf \\(t_{TL}/t_0 < 1\\), it means that the transfer learning provided an advantage. In this case, we see that there is a huge advantage for the product state phase, where we have done the pretraining. This is because the ground state is the same for the whole phase \\(B/J\\geq 2\\) and, thus, so is the optimal relaxation.\nAs we cross the phase transition at \\(B/J=2\\), we observe a sharp drop in convergence time. As we go deeper in the opposite phase, we observe that \\(t_{TL}/t_0\\to1\\), meaning that the convergence advantage banishes. For further details, see our work."
  },
  {
    "objectID": "tutorial/introduction.html",
    "href": "tutorial/introduction.html",
    "title": "Constraint optimization with reinforcement learning",
    "section": "",
    "text": "In this notebook, we show the basic principles to optimize relaxations based on semidefinite programming (SdP) with reinforcement learning. To ease the introduction, we fix an optimization problem that we aim to relax. In this case, we want to approximate the ground state energy of many-body quantum Hamiltonians. More precisely, we aim to find the optimal set of compatibility constraints, under a limited computational budget.\nThe optimal relaxation provides the best approximation to the system’s ground state energy \\(E_0\\) using the least amount of computational resources. In general, there is a tradeoff between such accuracy and the problem complexity: tighter relaxations provide better bounds to the ground state energy at the cost of higher computational requirements. However, some smart relaxations may provide comparable results (or even better!) than others incurring a larger cost, and finding these is the main goal of this work."
  },
  {
    "objectID": "tutorial/introduction.html#hamiltonian",
    "href": "tutorial/introduction.html#hamiltonian",
    "title": "Constraint optimization with reinforcement learning",
    "section": "Hamiltonian",
    "text": "Hamiltonian\nSo the first step is to define the Hamiltonian (see the Hamiltonian docs). We need two main ingredients: the topology and the interactions. We define the topology through a Graph, which in this case is a one-dimensional chain.\nIn the library, we have predefined some typical graphs, including the chain. This makes our life much easier now, as we simply need to choose the size!\n\nfrom bounce.hamiltonian import Chain1D\n\n\nN = 6\nchain = Chain1D(N)\n\nNow that we have the topology, we define the desired interactions of the Hamiltonian over the graph. Just like with the typical graphs, we also have some pre-defined Hamiltonians, including the XX model.\nWe simply need to specify the linear and quadratic terms, \\(B_i\\) and \\(J_i\\), respectively. This gives us full control over the problem parameters. We can do so by providing a list of values for each individual site, in case that we want to create a very specific Hamiltonian, or we can provide a smaller pattern that will be replicated over the system. Therefore, for a homogeneous Hamiltonian we can simply provide the value \\(J_i=J, B_i=B \\ \\forall i\\).\nTo illustrate the functionality, we will take a repeating pattern in the interaction \\(J_i\\) and a constant magnetic field \\(B\\).\n\nfrom bounce.hamiltonian import XXHamiltonian\n\n\nb, j = 1., [0, 1, 2]\nH = XXHamiltonian(chain, b, j)\n\nNow we can visualize our Hamiltonian to ensure that this is the actual system that we wanted.\n\nH.draw()\n\n\n\n\nNice! We see that all the nodes have the same color (indicating \\(B_i\\)) and the connections have different transparency, indicating the value of \\(J_i\\). We can see the details by looking at the interactions.\n\nH.interactions\n\n[(array([0]), <2×2 Real Constant: z>),\n (array([1]), <2×2 Real Constant: z>),\n (array([2]), <2×2 Real Constant: z>),\n (array([3]), <2×2 Real Constant: z>),\n (array([4]), <2×2 Real Constant: z>),\n (array([5]), <2×2 Real Constant: z>),\n (array([0, 1]), <4×4 Complex Constant: 0>),\n (array([1, 2]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([2, 3]), <4×4 Complex Constant: 2·(x⊗x + y⊗y)>),\n (array([3, 4]), <4×4 Complex Constant: 0>),\n (array([4, 5]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([0, 5]), <4×4 Complex Constant: 2·(x⊗x + y⊗y)>)]"
  },
  {
    "objectID": "tutorial/introduction.html#environment",
    "href": "tutorial/introduction.html#environment",
    "title": "Constraint optimization with reinforcement learning",
    "section": "Environment",
    "text": "Environment\nNow that we have the problem, we can define the environment. In reinforcement learning, the environment handles the “rules of the game”. In this case, it will host the physical problem that we tackle together with an associated solver to the problem and the maximum computational cost that we want to work with.\nFor every problem, we need an associated SdPSolver (see the SdP docs). The Hamiltonian defines the problem and the solver defines the objective. In this case, we look for the ground state energy, so we will use the SdPEnergySolver.\n\nfrom bounce.sdp import SdPEnergySolver\n\n\nsolver = SdPEnergySolver()\n\nWe now define a computational budget in terms of the solver.ojimetro method. In this case, the ojimetro estimates the computational cost of the problem based on the amount of free parameters. We will provide a maximum of 300 free parameters, which allow us to work with up to half of all the 3-body constraints.\n\nbudget = 300\n\nAnd we’re ready to define our environment!\n\nfrom bounce.environment import SdPEnvironment\n\n\nenv = SdPEnvironment(H, solver, budget)"
  },
  {
    "objectID": "tutorial/introduction.html#trainer",
    "href": "tutorial/introduction.html#trainer",
    "title": "Constraint optimization with reinforcement learning",
    "section": "Trainer",
    "text": "Trainer\nTherefore, we will use a DQNTrainer, which will train our agent based on a deep Q-network.\n\nfrom bounce.training import DQNTrainer\n\n\ntrainer = DQNTrainer(env)\n\nThis may seem quite basic, but we do have full control over the agent’s hyperparameters. We can provide the trainer with the base architecture for our model, we can provide pre-trained architectures to build our agents from, we can choose the learning rate, the loss function, etc. We’re just keeping it easy :)\nNow we simply tell our trainer to start training our agent. We need to specify the number of training episodes, which are the repetitions of the “game”, and their length.\n\nepisodes = 500\nepisode_length = 6\nresults = trainer.train(episodes, time_steps=episode_length)\n\n\n\n\nThe training should not take long and we can easily visualize the training results with some auxiliary functions.\n\nfrom bounce.utils import plot_trainings\n\n\nplot_trainings(results['training'])\n\n\n\n\nThe higher the reward, the better (max 1), so we see that our agent managed to find some decent relaxations. We can see which are the best layouts for our problem by looking at the best_layout property of the environment.\n\ntrainer.envs[0].best_layout\n\n[array([1, 2, 3]), array([0, 4, 5])]\n\n\nYay, we found something that makes sense! Given our Hamiltonian, the relaxation that provides the best bound with the least amount of parameters is the one that considers the two separate sub-systems. Adding more complexity to this relaxation will not improve the energy bound that we obtain, as the interconnection between the two sub-systems is completely meaningless.\nNotice here that we’re indexing over a list of environments. This is because our trainer can handle multiple parallel agent trainings and keeps them in a list with their corresponding list of environments. In this case, we only had a single agent.\n\nlen(trainer.agents), len(trainer.envs)\n\n(1, 1)\n\n\nWe can also see its energy value and its cost (in terms of the SdP free parameters). The SdPEnvironment.best property otputs the best bound together with the best and worst costs it has been found.\n\ntrainer.envs[0].best_layout\n\n[array([1, 2, 3]), array([0, 4, 5])]\n\n\n\ntrainer.envs[0].best\n\narray([-10.94427191, 127.        , 288.        ])"
  },
  {
    "objectID": "tutorial/introduction.html#parallel-training",
    "href": "tutorial/introduction.html#parallel-training",
    "title": "Constraint optimization with reinforcement learning",
    "section": "Parallel training",
    "text": "Parallel training\nUsually, when we face an optimization problem from which we do not know the solution, we want to launch several optimizers to maximize our chances of getting lucky. For this purpose, our trainer can handle the parallel training of various agents at once.\nLet’s train two agents in parallel.\n\ntrainer = DQNTrainer(env, n_agents=2, n_jobs=2)\n\n\nresults = trainer.train(episodes, time_steps=episode_length)\n\nNow we can look at the best relaxations found by each of them.\n\n[env.best_layout for env in trainer.envs]\n\n[[array([1, 2, 3]), array([0, 4, 5])], [array([1, 2, 3]), array([0, 4, 5])]]\n\n\nAyyy, they both found the optimal relaxation!"
  },
  {
    "objectID": "tutorial/entanglement_witnessing.html",
    "href": "tutorial/entanglement_witnessing.html",
    "title": "Energy-based entanglement witnesses",
    "section": "",
    "text": "A key property of quantum systems is entanglement. In order to determine whether a system is entangled or not, we need to find whether its state belongs to the set of separable states. However, fully characterizing this set is extremely hard and we rely on tools such as entanglement witnesses to solve this task.\nSuch witnesses often rely on the expectation value of an observable whose result signals the presence of entanglement. For instance, in the case of energy-based witnesses, the expectation value is over the Hamiltonian. In this case, if we are able to find the minimum possible energy within the set of separable states \\(E_{\\text{sep}}\\), we can build an entanglement witness in the following way. Let \\[\\Delta E = \\langle H\\rangle - E_{\\text{sep}}\\,,\\] where \\(\\langle H\\rangle=\\text{Tr}\\left[\\rho H\\right]\\) is the energy of an arbitrary state \\(\\rho\\). If \\(\\Delta E > 0\\), it means that \\(\\rho\\) lies within the set of separable states. Conversely, if \\(\\Delta E<0\\), the energy obtained with \\(\\rho\\) is lower than \\(E_{\\text{sep}}\\) and, therefore, \\(\\rho\\) lies outside of the set of separable states meaning that it is entangled.\nNevertheless, finding \\(E_{\\text{sep}}\\) is, in general, a difficult task. Although there are analytical expressions for some specific cases [1], we usually rely on approximate methods, such as relaxation methods to estimate \\(E_{\\text{sep}}\\). In this case, we relax the minimization problem of finding the minimum energy over the set of separable states to, instead, imposing that the states are positive under partial transposition (PPT) [2]. The set of PPT states is larger than the set of separable states and, therefore, we obtain a lower bound of \\(E_{\\text{sep}}\\). However, the tighter the bound, the better our witness as we can identify more entangled states."
  },
  {
    "objectID": "tutorial/entanglement_witnessing.html#quick-test",
    "href": "tutorial/entanglement_witnessing.html#quick-test",
    "title": "Energy-based entanglement witnesses",
    "section": "Quick test",
    "text": "Quick test\nIn such a random system, the contribution of larger constraints can vary significantly. We perform a simple test in which we allocate all the possible three-body constraints alone filling the rest with two-body ones.\n\nfor i in tqdm(range(18, len(env.state))):\n    env.reset()\n    env.state[:18] = 1\n    env.state[i] = 1\n    print(f\"{env.layout_basis[i]} {env.bound:.4f}\")\n\n\n\n\n[0 1 3] -9.6186\n[0 1 2] -8.9952\n[0 1 4] -9.6186\n[0 1 6] -9.6186\n[0 1 7] -9.6186\n[0 2 3] -9.6186\n[0 3 4] -9.6186\n[0 3 6] -9.5821\n[0 3 5] -9.6186\n[1 2 4] -9.6186\n[1 2 5] -9.6186\n[1 2 7] -9.6186\n[1 2 8] -9.6186\n[1 3 4] -9.6186\n[1 4 5] -9.6186\n[1 4 7] -9.5038\n[0 2 5] -9.6186\n[0 2 6] -9.6186\n[0 2 8] -9.6186\n[2 4 5] -9.6186\n[2 3 5] -9.6186\n[2 5 8] -9.4889\n[3 4 6] -9.6186\n[3 4 5] -9.4597\n[3 4 7] -9.6186\n[3 5 6] -9.6186\n[3 6 7] -9.6186\n[3 6 8] -9.6186\n[4 5 7] -9.6186\n[4 5 8] -9.6186\n[4 6 7] -9.6186\n[4 7 8] -9.6186\n[3 5 8] -9.6186\n[5 7 8] -9.6186\n[5 6 8] -9.6186\n[0 6 7] -9.6186\n[6 7 8] -9.5872\n[1 6 7] -9.6186\n[0 6 8] -9.6186\n[1 7 8] -9.6186\n[2 7 8] -9.6186\n[2 6 8] -9.6186\n\n\nMost of them barely have any effect, while just a few have a very high impact. Let’s explore this with an agent and see what is the best possible bound it can find.\n\nenv = SdPEnvironment(H, solver_sep, budget, max_basis_size=3)\n\n\ndqn = DQNTrainer(env)\n\n\nresults_dqn = dqn.train(300, time_steps=15)\n\n\n\n\n\nplot_exploration(results_dqn['exploration'], expl_optims=False)"
  },
  {
    "objectID": "tutorial/entanglement_witnessing.html#exhaustive-search",
    "href": "tutorial/entanglement_witnessing.html#exhaustive-search",
    "title": "Energy-based entanglement witnesses",
    "section": "Exhaustive search",
    "text": "Exhaustive search\nLet’s now throw several agents for various budgets and see what are the best separable bounds that we can find.\n\nsolver_sep = SdPWitnessSolver()\nbudgets = [390, 470, 490, 540]\neps_decays = [0.98, 0.98, 0.99, 0.99]\nepisodes = 600\nn_agents, n_jobs = 50, 15\nresults_dqn = []\nfor budget, decay in tqdm(zip(budgets, eps_decays)):\n    env = SdPEnvironment(H, solver_sep, budget, max_basis_size=3, initial_size=2)\n    dqn = DQNTrainer(env, n_agents=n_agents, n_jobs=n_jobs, eps_decay=decay, batch_size=256)\n    results_budget = dqn.train(episodes, time_steps=15, ckp=2)\n    results_dqn.append(results_budget)\n    \n#     with open(f'../benchmarks/witness_lattice_dqn_{budget}.pkl', 'wb') as f:\n#         pickle.dump(results_budget, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n\n\n\n# with open('../benchmarks/witness_lattice_dqn.pkl', 'wb') as f:\n#     pickle.dump(results_dqn, f, protocol=pickle.HIGHEST_PROTOCOL)"
  },
  {
    "objectID": "tutorial/entanglement_witnessing.html#result-analysis",
    "href": "tutorial/entanglement_witnessing.html#result-analysis",
    "title": "Energy-based entanglement witnesses",
    "section": "Result analysis",
    "text": "Result analysis\nFirst of all, we start by loading the data.\n\nwith open('../benchmarks/witness_lattice_dqn.pkl', 'rb') as f:\n    results_dqn = pickle.load(f)\n\n\nExploration results\nNow we can look at an exploration example for a given budget to get a better understanding of the optimization process.\n\n\nCode\nplot_exploration(results_dqn[2]['exploration'], expl_optims=False, highlight_max=True)\n\n\n\n\n\nLet’s plot the results for the different budgets\n\nfrom bounce.utils import arrange_shape, best_so_far\n\n\n\nCode\nlabels = [7, 8, 9, 10]\nfig, ax = plt.subplots(figsize=(10, 5))\nfor i, results in enumerate(results_dqn[:-1]):\n    bounds = arrange_shape(results['exploration']['bounds'])\n    best_bounds = best_so_far(bounds).T\n    best_mean, best_max = np.mean(best_bounds, axis=-1), np.max(best_bounds, axis=-1)\n    \n    ax.plot(best_max, color=f'C{i}', linewidth=3, label=labels[i])\n    ax.plot(best_mean, color=f'C{i}', linewidth=3, linestyle='dashed', alpha=0.6)\n    \nax.grid()\nax.legend(fontsize=17)\nax.tick_params(labelsize=17)\nax.set_ylabel(\"Separability bound\", fontsize=21)\nax.set_xlabel(\"New visited state\", fontsize=21)\nplt.savefig(\"../figures/witnesses_budget.pdf\", bbox_inches='tight')\n\n\n\n\n\nWe can see a quick early raise in the best obtained bound followed by a stagnation period. This is the natural random exploration process, which is quite inefficient. Then, after about 1500 states, we observe a collective raise in the best bound, which is when the agents start exploiting the gathered knowledge to their advantage. This lines up with the end of the exploration phase, as we exponentially decrease the value of \\(\\varepsilon\\) in the policy, reaching its minimum between 100 and 200 episodes.\nWe can have a look at the best layout that we have found for each budget.\n\nbest_by_budget = []\nfor results in results_dqn:\n    best_bound = -np.infty\n    for env in results['exploration']['envs']:\n        bound, layout = env.best[0], env.best_layout\n        if bound > best_bound:\n            best_bound = deepcopy(bound)\n            best_layout = deepcopy(layout)\n    best_by_budget.append((best_bound, best_layout))\n\n\nbest_by_budget\n\n[(-8.225026975813115,\n  [array([0, 6]),\n   array([0, 1, 2]),\n   array([0, 3, 5]),\n   array([1, 4, 7]),\n   array([0, 2, 5]),\n   array([2, 5, 8]),\n   array([3, 4, 5]),\n   array([6, 7, 8])]),\n (-8.167977031639149,\n  [array([0, 1, 2]),\n   array([0, 3, 6]),\n   array([0, 3, 5]),\n   array([1, 4, 7]),\n   array([0, 2, 5]),\n   array([2, 5, 8]),\n   array([3, 4, 5]),\n   array([6, 7, 8])]),\n (-8.148677351430782,\n  [array([3, 6]),\n   array([0, 1, 2]),\n   array([0, 1, 6]),\n   array([0, 2, 3]),\n   array([1, 4, 7]),\n   array([2, 3, 5]),\n   array([2, 5, 8]),\n   array([3, 4, 5]),\n   array([6, 7, 8]),\n   array([1, 6, 7])])]\n\n\n\n\n\n\n\n\nNote\n\n\n\nYep, this print right above here looks terrible and the information is rather hidden. We have used it to draw Figure 9 (b) in our paper, better have a look there!\n\n\n\n\nConstraint statistics: frequency\nIn order to get a deeper understanding of the problem, we can look at the similarities between the best results obtained by each agent. This way, we can see which are the most important constraints in our problem.\n\ndef element2str(element): return ''.join([str(i) for i in element])\nelement_index = {element2str(element): k for k, element in enumerate(env.layout_basis)}\n\n\nhist_by_budget = []\nfor results in results_dqn[:-1]:\n    element_bound = {k: [] for k in element_index.keys()}\n    for env in results['exploration']['envs']:\n        bound, layout = env.best[0], env.best_layout\n        for basis_element in layout:\n            element_bound[element2str(basis_element)] += [bound]\n    hist_by_budget.append(element_bound)\n\nNow we can look at the most used constraints. For instance, for the largest budget.\n\nidx_budget = -1\nbounds_by_element = hist_by_budget[idx_budget]\nx = list(element_index.keys())\ncounts = np.array([len(bounds_by_element[k]) for k in x])\nfreq = counts/counts.sum()\n\n\n\nCode\nplt.figure(figsize=(15, 4))\nplt.bar(x, freq)\nplt.xticks(rotation=75);\n\n\n\n\n\nThere are some clear 3-body terms that are overrepresented with respect to the rest: the \\([0, 1, 2]\\), the \\([2, 5, 8]\\) and the \\([3, 4, 5]\\).\nWe can look even further and check the frequency that every 2-body term is captured either by itself or by another 3-body term.\n\ncounts_link = {}\nfor link in x[:18]:\n    counts_link[link] = sum([counts[element_index[k]] for k in element_index.keys()\n                             if np.all([l in k for l in link])])\ncounts_link = {k: v/max(counts_link.values()) for k, v in counts_link.items()}\n\n\n\nCode\nplt.bar(list(counts_link.keys()), list(counts_link.values()));\n\n\n<BarContainer object of 18 artists>\n\n\n\n\n\nAgain, we see a few elements that appear more often in the best obtained relaxations. We can bring the analysis even further by looking only at the top performing agents. Let’s do it!\n\nbounds = [env.best[0] for env in results_dqn[idx_budget]['exploration']['envs']]\nidx_top = np.argsort(bounds)[::-1]\n\n\ntop = 10\nelement_bound = {k: [] for k in element_index.keys()}\nfor i in idx_top[:top]:\n    env = results_dqn[idx_budget]['exploration']['envs'][i]\n    bound, layout = env.best[0], env.best_layout\n    for basis_element in layout:\n        element_bound[element2str(basis_element)] += [bound]\n\n\ncounts_top = np.array([len(element_bound[k]) for k in x])\nfreq_top = counts/counts.sum()\n\n\n\nCode\nplt.figure(figsize=(15, 4))\nplt.bar(x, freq_top)\nplt.xticks(rotation=75);\n\n\n\n\n\n\ncounts_link = {}\nfor link in x[:18]:\n    counts_link[link] = sum([counts_top[element_index[k]] for k in element_index.keys()\n                             if np.all([l in k for l in link])])\ncounts_link = {k: v/max(counts_link.values()) for k, v in counts_link.items()}\n\n\n\nCode\nplt.bar(list(counts_link.keys()), list(counts_link.values()));\n\n\n<BarContainer object of 18 artists>\n\n\n\n\n\nWe do not see any major differences with the previous analysis, meaning that the agents have consistently found similar relaxations. The main difference that we observe is in the representation of the \\([0, 3]\\) term, which is clearly overrepresented by the top 10 performing agents, and it was just shy above the baseline in the previous analysis.\n\n\nConstraint statistics: bounds\nWe can also look at the relationship between each basis element and the obtained bound. This way, we can see whether a certain element results in higher energy bounds on average.\n\nbound_2body = -9.6186 # From before\nbound_avg = np.array([np.mean(bounds_by_element[k]) for k in x])\nbound_max = np.array([np.max(bounds_by_element[k]) \n                      if len(bounds_by_element[k]) > 0 else np.nan for k in x])\nbound_gain = bound_avg - bound_2body\n\nWe start by looking at every single energy bound obtained with a layout that contained each basis element.\n\nx_scatter = [[k]*count for k, count in zip(x, counts)]\nx_scatter = [s for sublist in x_scatter for s in sublist]\ny_scatter = [s for k in x for s in bounds_by_element[k]]\n\n\n\nCode\nplt.figure(figsize=(15, 4))\nplt.scatter(x_scatter, y_scatter)\nplt.grid(alpha=0.5)\nplt.xticks(rotation=75);\n\n\n\n\n\nIn this plot, top row of points corresponds to the best obtained energy bound with the layout \\([[3, 6], [0, 1, 2], [0, 1, 6], [0, 2, 3], \\dots]\\). Interestingly, the term \\([0, 1, 6]\\) only appears in the best layout.\nHowever, with this visualization, it is hard to determine the average performance associated to each element. To get a better picture, we can compute the average energy gain over the full 2-body constraint baseline.\n\n\nCode\nplt.figure(figsize=(15, 4))\nplt.bar(x, bound_gain)\nplt.xticks(rotation=75);\n\n\n\n\n\nIt’s still a bit hard to find the relationship between the different basis elements and the best bounds. Let’s build a regressor that tries to predict the bound we will obtain with a given layout and inspect its feature importances.\n\n\n\n\n\n\nWork in progress\n\n\n\nYeh, well… This is awkward. We have not gathered enough time and energy to do this. If you want to have a look at it together just send us an email."
  },
  {
    "objectID": "tutorial/plots.html",
    "href": "tutorial/plots.html",
    "title": "Plots",
    "section": "",
    "text": "Benchmark plots\nIn the paper we compare the performance of different methods. Here we provide the source code to reproduce the plots (Figure 5).\n\ndef load_benchmark_plot_data(suffix, **kwargs):\n    \"Loads the data required for the plotting.\"\n    name = f\"bench_plot_data_{suffix}.pkl\"\n    bench_path = Path(\"../benchmarks/\")/name\n    if bench_path.exists():\n        with open(bench_path, 'rb') as f:\n            plot_data = pickle.load(f)\n    else:\n        plot_data = process_benchmark_data(suffix, **kwargs)\n    return plot_data\n    \ndef process_benchmark_data(suffix, max_n=16, max_trains=50, algorithms=['DQN', 'MC', 'BFS'], save_path=None):\n    \"Processes the benchmark data for the plots.\"\n    results = {algorithm: {} for algorithm in algorithms}\n    for N in range(5, max_n+1):\n        B, J = 1, [i%3 for i in range(N)]\n        H = XXHamiltonian(Chain1D(N), B, J)\n\n        for algorithm in algorithms:\n            try: \n                benchmark = load_benchmark(H, budget, algorithm, suffix=suffix)\n                if algorithm == 'DQN': \n                    rewards = best_so_far(arrange_shape(benchmark['exploration']['oracle_rewards'][:max_trains]))\n                else: \n                    rewards = best_so_far(arrange_shape(benchmark['oracle_rewards'][:max_trains]))\n                best_mean = np.mean(rewards, axis=0)\n                best_std = np.std(rewards, axis=0)\n                try:    over95 = np.where(best_mean >= 0.95)[0][0]\n                except: over95 = np.inf\n                results[algorithm][N] = (best_mean, best_std, over95)\n            except:\n                pass\n            \n    if save_path is None: save_path = Path(f\"../benchmarks/bench_plot_data_{suffix}.pkl\")\n    with open(save_path, 'wb') as f:\n        pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return results\n\n\ndef plot_benchmark(results, suffix, ylim=(0, 2300), lw=3.5, ms=10, fs=18, ts=16, n_max=17, algorithms=[\"DQN\", \"MC\", \"BFS\"]):\n    linestyles = ['-', ':', '--']\n    markers = ['s', 'o', 'h']\n    fig = plt.figure(figsize=(12, 5))\n    \n    for a, alg in enumerate(results.keys()):\n        if alg not in algorithms: continue\n        plot_vline = False\n        ns, times = [], []\n        for n, (_, _, over95) in results[alg].items(): \n            if n <= n_max: ns.append(n); times.append(over95)\n        if times[-1] == np.inf:  ns.pop(-1); times.pop(-1); plot_vline = True\n        elif ns[-1] < n_max: plot_vline = True\n        label = alg if alg != 'DQN' else 'RL'\n        plt.plot(ns, times, linestyle=linestyles[a], linewidth=lw, marker=markers[a], ms=ms, label=label)\n        if plot_vline: \n            plt.vlines(ns[-1], 0, ylim[1], linestyles='dashed', alpha=0.7)\n            plt.text(ns[-1], 0.93*ylim[1], r\"$\\rightarrow$ Not found\", fontsize=fs)\n            \n    plt.grid(alpha=0.5)\n    plt.legend(fontsize=ts, loc=\"upper left\")\n    plt.xlabel(\"System size\", fontsize=fs)\n    plt.ylabel(\"States to 95% optimality\", fontsize=ts)\n    plt.tick_params(labelsize=ts)\n    plt.savefig(Path(f\"../figures/benchmark_sizes_{suffix}.pdf\"), bbox_inches='tight')\n\n\nsuffix = 'half_3'\nresults_half = load_benchmark_plot_data(suffix)\n\n\nplot_benchmark(results_half, suffix, n_max=16)\n\n\n\n\n\nsuffix = 'all_3'\nresults_all = load_benchmark_plot_data(suffix)\n\n\nplot_benchmark(results_all, suffix, ylim=(0, 3500), n_max=16)\n\n\n\n\n\nn = 11 \n\nlinestyles = ['-.', ':', '--']\nlinewidth = 3.5\nfs = 18\nticksize = 15\nplt.figure(figsize=(10, 5))\nfor a, algorithm in enumerate(algorithms):\n    plt.plot(results[algorithm][n][0], linestyle=linestyles[a], linewidth=linewidth, label=algorithm)\nplt.grid()\nplt.legend(fontsize=ticksize)\nplt.tick_params(labelsize=ticksize)\nplt.xlabel(\"New visited states\", fontsize=fs)\nplt.ylabel(\"Proximity to optimal solution\", fontsize=fs);\n\n\n\n\n\n\nTransfer learning across phases\nWe analyse the effect of transfer learning across different phases of the same Hamiltonian. Here we provide the source code to reproduce the plots (Figure 6).\n\nN = 6\nbudget = 185\nB0 = 5\npath = Path(f\"../benchmarks/TL_N{N}_{budget}_from_B{B0}.pkl\")\nwith open(path, 'rb') as f:\n    TL_evaluation = pickle.load(f)\n\n\ndef convergence_time(results, tol=5e-4, T=50, t_avg=20, return_diffs=False):\n    \"Returns the convergence with criterion of not changing result by `tol` for `T` epochs.\"\n    if 'training' in results.keys(): results = results['training']\n    rewards = arrange_shape(results['rewards'])\n    mean_rewards = np.mean(rewards, axis=0)\n    epochs = len(mean_rewards)\n    moving_avg = np.convolve(np.array([1/t_avg]*t_avg), mean_rewards, mode='valid')\n    diffs = np.abs(moving_avg[1:] - moving_avg[:-1])\n    diff_variation = np.convolve(np.array([1/T]*T), diffs, mode='valid')\n    try:    t = np.where(diff_variation <= tol)[0][0]\n    except: t = len(mean_rewards)\n    if return_diffs: return t + 2*T, moving_avg, diff_variation \n    return t + T\n\ndef indiv_convergence_time(results, max_epochs=800, **kwargs):\n    \"Similar to `convergence_time` but with each agent.\"\n    results = results['rewards']\n    times = [convergence_time({'rewards': [res[:max_epochs]]}, **kwargs) for res in results]\n    return np.array(times)\n\ndef get_indiv_times(tl_eval, convergence_crit=None):\n    \"Provides convergence times from individual ratios.\"\n    default_crit = {'T': 50, 't_avg': 100, 'tol': 2e-4}\n    convergence_crit = {**default_crit, **convergence_crit} if convergence_crit is not None else default_crit\n    Bs = list(TL_evaluation.keys()); Bs.sort()\n    time_ratios, time_err = [], []\n    for b in Bs:\n        ts_tl = indiv_convergence_time(tl_eval[b]['tl'], **convergence_crit)\n        ts_0 = indiv_convergence_time(tl_eval[b]['vanilla'], **convergence_crit)\n        t0, ttl = ts_0.mean(), ts_tl.mean()\n        ratio = ttl/t0\n        ratio_std = np.sqrt((1/t0)**2*ts_tl.var() + (ttl/t0**2)**2*ts_0.var())\n        time_ratios.append(ratio)\n        time_err.append(ratio_std/np.sqrt(len(ts_0)))\n    return Bs, np.array(time_ratios), np.array(time_err)\n\n\ninset_Bs = [0., 1.5, 2., 4.]\nax_width, ax_length = 0.25, 0.2\nmax_epochs = 700\nrefs = [\"(a)\", \"(b)\", \"(c)\", \"(d)\"]\nplot_rewards_tl = [np.mean(TL_evaluation[b]['tl']['rewards'], axis=0)[:max_epochs] for b in inset_Bs]\nplot_rewards_cs = [np.mean(TL_evaluation[b]['vanilla']['rewards'], axis=0)[:max_epochs] for b in inset_Bs]\n\ndef plot_subplot(ax, tl_rewards, cs_rewards, xlabel=False, legend=False, ref=\"(a)\", ylim=[-0.05, 1.05]):\n    ax.plot(cs_rewards, linewidth=3, label=\"CS\")\n    ax.plot(tl_rewards, linewidth=3, label=\"TL\")\n    ax.tick_params(labelbottom=xlabel)\n    ax.text(0.1, 0.7, ref, fontsize=16)\n    if xlabel: ax.set_xlabel(\"Training Episode\", fontsize=20)\n    if legend: ax.legend(fontsize=14, loc='lower right')\n    ax.tick_params(labelsize=16)\n    ax.set_ylim(ylim)\n    ax.grid()\n\nfig = plt.figure(figsize=(14, 5))\ngs0 = gs.GridSpec(1, 3, figure=fig)\n\nax1 = fig.add_subplot(gs0[:-1])\n\n# Times\nBs, time_ratios, time_errs = get_indiv_times(TL_evaluation)\n\nax1.fill_between(Bs, time_ratios-time_errs, time_ratios+time_errs, alpha=0.25)\nax1.plot(Bs, time_ratios, 's-', ms=7, lw=2)\nax1.tick_params(labelsize=16)\nax1.set_xlabel(\"B/J\", fontsize=20)\nax1.set_ylabel(r\"$t_{TL}/t_0$\", fontsize=20);\nax1.grid(alpha=0.5)\nfor b, ref in zip(inset_Bs, refs):\n    if b != 2: dx, dy = 0.1, 0.05\n    else:      dx, dy = 0.23, 0.\n    x, y = b-dx, time_ratios[np.where(np.array(Bs) == b)[0][0]]+dy\n    ax1.text(x, y, ref, fontsize=16)\nymin, ymax = ax1.get_ylim()\nax1.vlines(0.75, ymin*1.2, ymax*0.95, linestyles='dashed', alpha=0.5)\nax1.vlines(1, ymin*1.2, ymax*0.95, linestyles='dashed', alpha=0.5)\nax1.vlines(2, ymin*1.2, ymax*0.95, linestyles='dashed', alpha=0.5)\n    \n\n# Trainings\ngs1 = gs0[-1].subgridspec(4, 1)\n\naxes2 = [fig.add_subplot(gs1[i]) for i in range(4)]\nfor i, ax in enumerate(axes2): \n    plot_subplot(ax, plot_rewards_tl[i], plot_rewards_cs[i], xlabel=i==3, legend=i==0, ref=refs[i])\nfig.text(0.635, 0.5, \"Reward\", va='center', rotation='vertical', fontsize=20);\nplt.savefig(f\"../figures/TL_bench_N{N}_{budget}_from_B{B0}.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\nEnergy bounds at different system sizes\nWe also study the energy bounds provided by the same qualitative solutions at different system sizes in Appendix C. With the following code you can obtain the energy bounds along one phase of the XX Hamiltonian for several sets of constraints.\n\nfrom bounce.environment import SdPEnvironment\nfrom bounce.sdp import SdPEnergySolver\n\n\ndef test_configs(confs, N, Bs):\n    \"Tests a list of configurations for the XX Hamiltonian in the range of Bs (J=1).\"\n    J = 1\n    energies = []\n    for B in tqdm(Bs):\n        H = XXHamiltonian(Chain1D(N), B, J)\n        e = SdPEnvironment(H, SdPEnergySolver(), budget)\n        c_energies, c_params, c_layouts = [], [], []\n        for c in confs: \n            e.reset()\n            e.state[:N] = 1\n            e.state[c] = 1\n            es, ps, _ = e.get_values()\n            c_energies.append(es)\n            c_params.append(ps)\n            c_layouts.append(e.layout)\n        energies.append(c_energies)\n    return np.array(energies), np.array(c_params), c_layouts\n\ndef plot_tests(energies, params, labels, Bs, norm=False, figsize=(8, 4), fontsize=16, labelsize=14):\n    plt.figure(figsize=figsize)\n    for energy, params, label in zip(energies.T, params, labels):\n        plt.plot(Bs, energy, label=label+f\" ({params})\")\n    plt.grid()\n    plt.legend(fontsize=labelsize)\n    plt.xticks(Bs[::2])\n    plt.xlabel(\"B/J\", fontsize=fontsize)\n    plt.ylabel(\"Energy bound per spin\" if norm else \"Energy bound\", fontsize=fontsize)\n    plt.tick_params(labelsize=labelsize)\n    plt.axvspan(0, 0.75, facecolor='C0', alpha=0.1)\n    plt.axvspan(0.75, 1., facecolor='C1', alpha=0.1)\n    plt.axvspan(1., 2., facecolor='C2', alpha=0.1)\n    plt.axvspan(2., 2.1, facecolor='C3', alpha=0.1);\n    \ndef save_tests(energies, params, layouts, N):\n    save_path = Path(f\"../benchmarks/test_{N}.pkl\")\n    with open(save_path, 'wb') as f:\n        pickle.dump((energies, params, layouts), f, protocol=pickle.HIGHEST_PROTOCOL)\n        \ndef load_tests(N):\n    save_path = Path(f\"../benchmarks/test_{N}.pkl\")\n    with open(save_path, 'rb') as f:\n        energies, params, layouts = pickle.load(f)\n    return energies, params, layouts\n\nIn every system size, we compute the bounds obtained with different layouts that we store in the list confs. In the code below, all the configurations that are never optimal are commented out. This way, we can call test_configs to compute the bounds for each layout given several values of the external magnetic field \\(B\\) (assuming \\(J=1\\)).\nWe perform the entire analysis for sizes \\(n=6, 12, 24\\) and \\(36\\).\n\nN = 6\nconfs = [np.arange(0, N, 2)+N,\n         np.concatenate((np.array([0]), np.arange(1, N-1, 2)))+N,\n#          np.arange(0, N//2, 1)+N, # double overlap\n         np.arange(0, N, 3)+N,\n         np.arange(0, 1)]\nBs = np.arange(0, 2.2, 0.1)\n\nenergies_6, params_6, layouts_6 = test_configs(confs, N, Bs)\nsave_tests(energies_6, params_6, layouts_6, N)\n\n\n# Load previous results\nBs = np.arange(0, 2.2, 0.1)\nenergies_6, params_6, layouts_6 = load_tests(6)\n\n\nlabels = ['(a) 3 T, 0 P', '(b) 3 T, 1 P', '(c) 2 T, 2 P', '(d) 0 T, 6 P']\n# Absolute bounds\nplot_tests(energies_6, params_6, labels, Bs)\nplt.title(\"N=6\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N6.pdf\", bbox_inches=\"tight\")\n# Normalized bounds\nplot_tests(energies_6/6, params_6, labels, Bs, norm=True)\nplt.title(\"N=6\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N6_norm.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\nN = 12\nconfs = [np.arange(0, N, 2)+N,\n         np.concatenate((np.array([0]), np.arange(1, N//2-1, 2), np.array([N//2]), np.arange(N//2+1, N-1, 2)))+N,\n         np.arange(0, N, 3)+N,\n         np.arange(0, 1),\n         np.concatenate((np.array([0]), np.arange(1, N-1, 2)))+N,\n#          np.sort(np.concatenate((np.arange(0, N, 4), np.arange(1, N, 4))))+N, # tiplet-triplet-pair\n#          np.arange(0, N, 4)+N, # triplet-pair-pair\n        ]\nBs = np.arange(0, 2.2, 0.1)\n\nenergies_12, params_12, layouts_12 = test_configs(confs, N, Bs)\nsave_tests(energies_12, params_12, layouts_12, N)\n\n\n# Load previous results\nBs = np.arange(0, 2.2, 0.1)\nenergies_12, params_12, layouts_12 = load_tests(12)\n\n\nlabels = ['(a) 6 T,  0  P', '(b) 6 T,  2  P', '(c) 4 T,  4  P', '(d) 0 T, 12 P']\n# Absolute bounds\nplot_tests(energies_12[:, :-1], params_12[:-1], labels, Bs)\nplt.title(\"N=12\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N12.pdf\", bbox_inches=\"tight\")\n# Normalized bounds\nplot_tests(energies_12[:, :-1]/12, params_12[:-1], labels, Bs, norm=True)\nplt.title(\"N=12\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N12_norm.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\nN = 24\nconfs = [np.arange(0, N, 2)+N,\n         np.concatenate([np.arange(p*(N//4)+1, (p+1)*N//4-1, 2) for p in range(4)] + [np.array([p*N//4]) for p in range(4)])+N,         \n         np.arange(0, N, 3)+N,\n         np.arange(0, 1),\n         np.concatenate([np.arange(p*(N//3)+1, (p+1)*N//3-1, 2) for p in range(3)] + [np.array([p*N//3]) for p in range(3)])+N,\n         np.concatenate((np.array([0]), np.arange(1, N//2-1, 2), np.array([N//2]), np.arange(N//2+1, N-1, 2)))+N,\n         np.concatenate((np.array([0]), np.arange(1, N-1, 2)))+N,\n#          np.sort(np.concatenate((np.arange(0, N, 4), np.arange(1, N, 4))))+N, # tiplet-triplet-pair\n#          np.arange(0, N, 4)+N, # triplet-pair-pair\n        ]\nBs = np.arange(0, 2.2, 0.1)\n\nenergies_24, params_24, layouts_24 = test_configs(confs, N, Bs)\nsave_tests(energies_24, params_24, layouts_24, N)\n\n\n\n\n\n\n\n\n# Load previous results\nBs = np.arange(0, 2.2, 0.1)\nenergies_24, params_24, layouts_24 = load_tests(24)\n\n\nlabels = ['(a) 12 T,  0  P', '(b) 12 T,  4  P', '(c)  8  T,  8  P', '(d)  0  T, 24 P']\n# Absolute bounds\nplot_tests(energies_24[:, :-3], params_24[:-3], labels, Bs)\nplt.title(\"N=24\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N24.pdf\", bbox_inches=\"tight\")\n# Normalized bounds\nplot_tests(energies_24[:, :-3]/24, params_24[:-3], labels, Bs, norm=True)\nplt.title(\"N=24\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N24_norm.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\nN = 36\nconfs = [np.arange(0, N, 2)+N,    \n         np.concatenate([np.arange(p*(N//6)+1, (p+1)*N//6-1, 2) for p in range(6)] + [np.array([p*N//6]) for p in range(6)])+N,\n         np.arange(0, N, 3)+N,\n         np.arange(0, 1),\n         np.concatenate([np.arange(p*(N//4)+1, (p+1)*N//4-1, 2) for p in range(4)] + [np.array([p*N//4]) for p in range(4)])+N,\n         np.concatenate([np.arange(p*(N//3)+1, (p+1)*N//3-1, 2) for p in range(3)] + [np.array([p*N//3]) for p in range(3)])+N,\n         np.concatenate((np.array([0]), np.arange(1, N//2-1, 2), np.array([N//2]), np.arange(N//2+1, N-1, 2)))+N,\n         np.concatenate((np.array([0]), np.arange(1, N-1, 2)))+N,\n#          np.sort(np.concatenate((np.arange(0, N, 4), np.arange(1, N, 4))))+N, # tiplet-triplet-pair\n#          np.arange(0, N, 4)+N, # triplet-pair-pair\n        ]\nBs = np.arange(0, 2.2, 0.1)\n\nenergies_36, params_36, layouts_36 = test_configs(confs, N, Bs)\nsave_tests(energies_36, params_36, layouts_36, N)\n\n\n# Load previous results\nBs = np.arange(0, 2.2, 0.1)\nenergies_36, params_36, layouts_36 = load_tests(36)\n\n\nlabels = ['(a) 18 T,  0  P', '(b) 18 T,  6  P', '(c) 12 T, 12 P', '(d)  0  T, 36 P']\n# Absolute bounds\nplot_tests(energies_36[:, :-3], params_36[:-3], labels, Bs)\nplt.title(\"N=36\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N36.pdf\", bbox_inches=\"tight\")\n# Normalized bounds\nplot_tests(energies_36[:, :-3]/36, params_36[:-3], labels, Bs, norm=True)\nplt.title(\"N=36\", fontsize=16);\nplt.savefig(\"../figures/pattern_test_N36_norm.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nComparison between different lower bound methods\nWe compare the performance of our SdP-based approach with other techniques developed to lower bound the ground state energy of many-body Hamiltonians. See the SdP docs for more details about the methods.\nExecuting the following cells you will reproduce Figure 10 from Appendix D.\n\nfrom bounce.sdp import SdPEnergySolver, SdPEnergyAndersonSolver, SdPEnergyUskovLichkovskiySolver\n\n\ndef compare_bounds(N, B_values, rdm_size=5):\n    \"Returns the energy bounds obtained with Anderson, Uskov-Lichkovskiy and our methods.\"\n    layouts = [[np.sort(np.arange(i, i + rdm_size)%N) for i in np.arange(0, N, rdm_size - s)]\n               for s in range(1, rdm_size)]\n    anderson, uskov, our_bounds = [], [], [[] for _ in range(len(layouts))]\n    anderson_solver = SdPEnergyAndersonSolver()\n    uskov_solver = SdPEnergyUskovLichkovskiySolver()\n    our_solver = SdPEnergySolver()\n    \n    J = 1\n    for B in tqdm(B_values):\n        H = XXHamiltonian(Chain1D(N), B, J)\n        anderson.append(anderson_solver.solve(H.to_sdp(), cluster_size=rdm_size))\n        uskov.append(uskov_solver.solve(H.to_sdp, cluster_size=rdm_size))\n        for layout, bounds in zip(layouts, our_bounds):\n            bounds.append(our_solver.solve(H.to_sdp(), layout))\n            \n    return (anderson, uskov, *our_bounds)\n\n\nN = 8\nrdm_size = 5\nBs = np.arange(0, 3.1, 0.1)         # Main plot with large spacing\nBs_inset = np.arange(0, 0.11, 0.01) # Finer grid for the inset\n\n\n# Data for main plot\ncompare_data = compare_bounds(N, Bs, rdm_size=rdm_size)\n\ncomparison_path = Path(f\"../benchmarks/anderson_uskov_comparison_N{N}_cs{rdm_size}.pkl\")\ncomparison_path.mkdir(exist_ok=True)\nwith open(comparison_path, 'wb') as f:\n    pickle.dump(compare_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n# Data for inset\ninset_data = compare_bounds(N, Bs_inset, rdm_size=rdm_size)\n\ninset_path = Path(f\"../benchmarks/anderson_uskov_comparison_N{N}_cs{rdm_size}_inset.pkl\")\ninset_path.mkdir(exist_ok=True)\nwith open(inset_path, 'wb') as f:\n    pickle.dump(inset_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n# Load previous results\nN = 8\nBs = np.arange(0, 3.1, 0.1)\nBs_inset = np.arange(0, 0.11, 0.01)\n\ncomparison_path = Path(f\"../benchmarks/anderson_uskov_comparison_N{N}_cs{rdm_size}.pkl\")\nwith open(comparison_path, 'rb') as f:\n    comparison = pickle.load(f) \n    \ninset_path = Path(f\"../benchmarks/anderson_uskov_comparison_N{N}_cs{rdm_size}_inset.pkl\")\nwith open(inset_path, 'rb') as f:\n    inset = pickle.load(f)\n\n\nfigsize = (8, 5)\nfontsize, labelsize = 16, 14\nplot_data = comparison[:-1]\ninset_data = inset[:-1]\nlabels = ['Anderson bound', 'TI bound', '1-body overlap', '2-body overlap', '3-body overlap']\nfig = plt.figure(figsize=figsize)\nax1 = fig.add_axes([0.1, 0.1, 0.9, 0.8])\nax2 = fig.add_axes([0.19, 0.17, 0.42, 0.45])\nfor bounds, ins, label in zip(plot_data, inset_data, labels):\n    ax1.plot(Bs, np.array(bounds)/N, linewidth=2.2, label=label)\n    ax2.plot(Bs_inset, np.array(ins)/N, linewidth=2)\nax1.grid()\nax1.legend(fontsize=labelsize, loc=\"upper right\")\nax1.set_xticks(Bs[::5])\nax1.set_xlabel(\"B/J\", fontsize=fontsize)\nax1.set_ylabel(\"Energy bound per spin\", fontsize=fontsize)\nax1.tick_params(labelsize=labelsize);\n    \nax2.grid()\nax2.set_ylim([-1.375, -1.33])\nax2.set_xlim([-0.002, 0.075])\n# ax2.set_yticks([-1.5, -1.45, -1.4, -1.35]);\nplt.savefig(\"../figures/comparison_anderson_uskov.pdf\", bbox_inches=\"tight\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOUNCE",
    "section": "",
    "text": "Documentation | Tutorials | Cite us\n\nWe have originally developed this library for the paper Certificates of quantum many-body properties assisted by machine learning. Here, you will find the source code used to obtain the paper results in the tutorials.\n\nInstallation\nIn order to use the library, you will have to clone this repository with git clone https://github.com/BorjaRequena/bounce.git and install it via pip install bounce. In order to edit the source code and adapt it to your particular problems, you may want to install it in editable form with pip install -e bounce.\n\n\n\n\n\n\nNote\n\n\n\nIn order to perform the installation, you can either provide the full path to the repository or be in the immediately higher level. For instance, if the repository is in the directory ~/what/ever/bounce, go to ~/what/ever/ directory to run the pip install -e bounce command, otherwise you can run pip install -e ~/what/ever/bounce. If this ever gets momentum, we’ll put it on pypi :)\n\n\n\n\nCite us\nWhen using this library, please cite us!\nIt is very important for us to cite our original work:\n@article{Requena2023PRR,\n  title     = {Certificates of quantum many-body properties assisted by machine learning},\n  author    = {Requena, Borja and Mu\\~noz-Gil, Gorka and Lewenstein, Maciej and Dunjko, Vedran and Tura, Jordi},\n  journal   = {Phys. Rev. Res.},\n  volume    = {5},\n  issue     = {1},\n  pages     = {013097},\n  numpages  = {26},\n  year      = {2023},\n  month     = {Feb},\n  publisher = {American Physical Society},\n  doi       = {10.1103/PhysRevResearch.5.013097},\n  url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.013097}\n}\nalong this repository:\n@Software{Requena2021Zenodo,\n  author    = {Requena, Borja and Muñoz-Gil, Gorka and Tura, Jordi},\n  doi       = {10.5281/zenodo.4585623},\n  month     = mar,\n  publisher = {Zenodo},\n  title     = {BorjaRequena/BOUNCE},\n  url       = {https://doi.org/10.5281/zenodo.4585623},\n  year      = {2021},\n}"
  },
  {
    "objectID": "source/sdp.html",
    "href": "source/sdp.html",
    "title": "Semidefinite programming",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "source/sdp.html#solving-a-problem-instance",
    "href": "source/sdp.html#solving-a-problem-instance",
    "title": "Semidefinite programming",
    "section": "Solving a problem instance",
    "text": "Solving a problem instance\nNow that we have defined a problem, we use the solve method to define the objective function, which corresponds to the expected energy, and the problem constraints, given by a layout.\n\nsource\n\nSdPEnergySolver.solve\n\n SdPEnergySolver.solve (interactions, layout)\n\nCreates and solves the SdP associated to the given layout and Hamiltonian interactions. The result is a lower bound of the Hamiltonian’s ground state energy.\nThe method takes the Hamiltonian.interactions, provided by the Hamiltonian.to_sdp() method, and a layout with which we define the set of compatibility constraints imposed in the SdP, as we explain in [1].\nGiven the Hamiltonian, let’s define a layout in order to compute an energy bound.\n\nsimple_layout = fill_layout([np.array([0, N-1])], N)\nsimple_layout\n\n[array([0, 5]), array([1]), array([2]), array([3]), array([4])]\n\n\nAnd now we can solve the associated SdP to the Hamiltonian with the given set of constraints.\n\nsolver = SdPEnergySolver()\nsolver.solve(H.to_sdp(), simple_layout)\n\n-15.999999999012134\n\n\nTightening the constraints, we can obtain a better energy bound. For instance, adding a 3-body element to the previous set we will obtain a higher energy.\n\nstronger_layout = fill_layout([np.array([1, 2, 3]), np.array([0, N-1])], N)\nstronger_layout\n\n[array([1, 2, 3]), array([0, 5]), array([4])]\n\n\n\nsolver.solve(H.to_sdp(), stronger_layout)\n\n-12.472135949820899"
  },
  {
    "objectID": "source/sdp.html#cost-estimation",
    "href": "source/sdp.html#cost-estimation",
    "title": "Semidefinite programming",
    "section": "Cost estimation",
    "text": "Cost estimation\nA key element of any optimization problem is its computational cost. We can take any consistent measure of the problem’s cost as a valid estimation for the posterior optimization with the reinforcement learning framework. For example, since the actual complexity of solving an SdP depends on the algorithm that we use, we take the number of free parameters as a metric for the associated cost.\nIn order to get a cost estimation, we rely on the ojimetro method.\n\nsource\n\nSdPEnergySolver.ojimetro\n\n SdPEnergySolver.ojimetro (layout)\n\nEstimates the amount of free parameters involved in the SdP associated to the layout.\nFollowing our example, the first SdP was a wild relaxation of the original problem.\n\nsolver.ojimetro(simple_layout)\n\n31\n\n\nHowever, the second one was a tighter one.\n\nsolver.ojimetro(stronger_layout)\n\n83\n\n\nWith the first set of constraints, the resulting SdP had 31 free variables to optimize, while the second SdP had to deal with 83. Tighter energy bounds usually come at the cost of higher computational costs.\n\nassert solver.ojimetro(simple_layout) < solver.ojimetro(stronger_layout)"
  },
  {
    "objectID": "source/sdp.html#alternative-methods-to-lower-bound-the-ground-state-energy",
    "href": "source/sdp.html#alternative-methods-to-lower-bound-the-ground-state-energy",
    "title": "Semidefinite programming",
    "section": "Alternative methods to lower bound the ground state energy",
    "text": "Alternative methods to lower bound the ground state energy\nWe implement two other methods to obtain lower bounds of many-body Hamiltonians. We rewrite the Anderson bound [2] and the method introduced by Uskov and Lichovskiy in [3] in SdP form to compare with the proposed methodology above. The SdP formulation ensures that the obtained result is the actual global minimum.\nThe main limitation of these methods is not taking into account compatibility constraints between the reduced density matrices spanning the system. Furthermore, they rely on symmetries (such as translational invariance) and can, thus, not be used to solve inhomogeneous systems. Conversely, the approach we introduced above can be nautrally complemented by introducing additional constratints stemming from any previously known symmetry from the system.\n\nsource\n\nSdPEnergyAndersonSolver\n\n SdPEnergyAndersonSolver (solver='cvxopt')\n\nSolver to compute bounds to the ground state energy of local Hamiltonians implementing the methods described in [2] (see references below). Finds the so-called Anderson bound. Assumes the system is one-dimensional.\n\nsource\n\n\nSdPEnergyUskovLichkovskiySolver\n\n SdPEnergyUskovLichkovskiySolver (solver='cvxopt')\n\nSolver to compute ground state energy bounds of local Hamiltonians. It follows the method introduced in [3] (see references below) to improve over the Anderson bound. The method assumes the reduced states have certain symmetries, such as translational invariance.\nWe can quickly reproduce the results from [3] (see Table 2) by properly adjusting the cluster size and using the same Hamiltonian.\n\nN = 8\nchain = Chain1D(N)\nB, J = 0, 1\nH = XYZHamiltonian(chain, B, J)\n\n\nsolver_anderson = SdPEnergyAndersonSolver()\nsolver_uskov = SdPEnergyUskovLichkovskiySolver()\n\n\ncluster_size = 5\nlayout = [np.sort(np.arange(i, i+cluster_size)%N) for i in np.arange(0, N, cluster_size-3)]\n\n\nsolver_anderson.solve(H.to_sdp(), cluster_size)/N, solver_uskov.solve(H.to_sdp(), cluster_size)/N, solver.solve(H.to_sdp(), layout)/N\n\n(-1.9278862525095595, -1.8685170845449277, -1.868517086761974)"
  },
  {
    "objectID": "source/training.html",
    "href": "source/training.html",
    "title": "Training",
    "section": "",
    "text": "Deep Q-learning trainer\nThe DQNTrainer handles the training of the deep reinforcement learning agent.\n\nsource\n\nDQNTrainer\n\n DQNTrainer (environment, n_agents=1, models=None, arch=<class\n             'bounce.agents.DQN'>, n_jobs=1, learning_rate=0.001,\n             criterion=None, optimizer=None, batch_size=128,\n             target_update=5, gamma=0.85, eps_0=1, eps_decay=0.999,\n             eps_min=0.1)\n\nTrainer for DQN agents\nThe DQNTrainer depends entirely on the train_agent function. This, in itself, relies on several inner functionalities shown below.\n\nsource\n\n\ntrain_agent\n\n train_agent (env, agent, episodes, time_steps=20, opt=None,\n              best_ref=None, evaluate=True, break_opt=False,\n              train_id=None, ckp=20, save=True)\n\nTrains an agent given an environment.\nTo train an agent, provide the environment env containing the all the details about the problem that is to be solved and the agent, which is an initialized DQNAgent. The learning process is structured in a number of episodes of length time_steps. Every episode starts from the same initial state and the agent performs a trajectory through the state space of length time_steps. The agent gathers experience throughout the episode that then, once it has gathered enough, it is replayed to learn. The target network for double Q-learning is updated every agent.target_update learning episodes.\nIf evaluate is set to True, the agent performs an evaluation episode after each training episode. With this, we can keep track of the convergence of the agent by looking at the final state of this episode. For the evaluation, there can be provided the optional opt and best_ref, which are used to check whether the agent is reaching the optimal configuration and the relative performance with respect to it. When only considering the exploration capabilities, break_opt is used to stop the learning process once the agent has encountered the optimal state.\nThe optional inputs train_id, ckp and save are meant to handle data storage. With train_id we name the agent to save the training data in a consistent way. This is specially important when training agents in parallel, although DQNTrainer already handles this automatically. If no train_id is provided, the agent is assigned one at random. The training process is backed up every ckp training episodes and, if save is set to True, the resulting trained agent is saved at the end of the training process.\ntrain_agent uses the following functionalities to make the code more readable.\n\nsource\n\n\ncheck_optim\n\n check_optim (opt, B, C)\n\nChecks whether the current bound B and cost C match with the optimal ones.\n\nsource\n\n\nevaluate_agent\n\n evaluate_agent (agent, environment, time_steps)\n\nEvaluates the agent in deterministic policy. Returns final reward, bound and cost.\n\nsource\n\n\nget_reward\n\n get_reward (environment, bound, cost, best_ref=None)\n\nReturns the reward according to the environment.\n\nsource\n\n\nstep\n\n step (state, agent, environment)\n\nTake a step forward from state following the agent’s policy.\nLet’s see how the trainer works. As usual, we start by defining the environment.\n\nN = 6\nchain = Chain1D(N)\nb, j = 1, [i%3 for i in range(N)]\nH = XXHamiltonian(chain, b, j)\nbudget = 300\nenv = SdPEnvironment(H, SdPEnergySolver(), budget)\n\nThen, we define the training parameters. In the training process if we know the optimal set of constraints, we can provide an opt reference consisting of a tuple with the optimal bound and cost. Furthermore, with full knowledge of the problem, we can provide the best_ref in the form of an np.array that contains the optimal bound and cost (like the opt), as well as the highest cost with which the optimal bound can be obtained.\n\nepisodes = 50\ntime_steps = 7\nopt = (-10.94427, 127)\nbest_ref = np.array([*opt, 288])\n\nWith these, we’re all set to create our trainer and start training our agent! Even more, we can train a few agents in parallel.\n\ndqn = DQNTrainer(env, n_agents=2, n_jobs=2)\nresults = dqn.train(episodes=episodes, time_steps=time_steps, opt=opt, best_ref=best_ref)\n\nThe results are split into training and exploration results. The training results focus on the actual training process of the agents, storing information after every training episode.\nWe can quickly visualize those with plot_trainings from bounce.utils.\n\nplot_trainings(results['training'])\n\n\n\n\nHere we see the reward and cost associated to the state at the end of an evaluation trajectory after every episode. We see that the agent needs some time to converge to a solution.\nConversely, the exploration results track the exploration process through the state space. This way, they store relevant data only after the agent visits a new state. This allows us to see how the agent progresses in its quest to find the optimal relaxation.\nWe can visualize those with plot_exploration from bounce.utils.\n\nplot_exploration(results['exploration'])\n\n\n\n\nHere we see the best bound obtained by the agents at every time step, as well as how close the states are to the global optimum. Remember that finding the optimal relaxation not only involves finding the best possible bound, but doing so with the least amount of parameters.\n\n\n\nBreadth first search trainer\n\nsource\n\nBrFSTrainer\n\n BrFSTrainer (environment, n_agents=1, n_jobs=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nexplore_brfs\n\n explore_brfs (env, agent, max_states, opt=None, best_ref=None,\n               break_opt=False)\n\nSpace exploration with Breadth First Search (BrFS)\nNeither the breadth first search nor the Monte Carlo algorithms require any actual training. However, we use the “trainers” to explore the state space with them. Therefore, we only receive exploration results.\n\nbfs = BrFSTrainer(env)\nresults = bfs.train(150, opt=opt, best_ref=best_ref)\n\n\nplot_exploration(results)\n\n\n\n\n\n\n\nMonte-Carlo trainer\n\nsource\n\nMCTrainer\n\n MCTrainer (environment, n_agents=1, n_jobs=1, beta=0.1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nexplore_mc\n\n explore_mc (env, agent, max_states, opt=None, best_ref=None, ckp=20,\n             break_opt=False)\n\nSpace exploration with Monte-Carlo (MC)\n\nbfs = MCTrainer(env, n_agents=4, n_jobs=2)\nresults = bfs.train(350, opt=opt, best_ref=best_ref)\n\n\nplot_exploration(results)"
  },
  {
    "objectID": "source/hamiltonian.html",
    "href": "source/hamiltonian.html",
    "title": "Hamiltonian",
    "section": "",
    "text": "Before going into the actual Hamiltonians, we introduce the Graph class, which is an essential constituent of the Hamiltonian.\n\nsource\n\n\n\n Graph (nodes, edges)\n\nInitialize self. See help(type(self)) for accurate signature.\nWe define the structure of the problem with a Graph, upon which we introduce the interactions. In order to create a Graph, we need to provide a list of nodes and edges, which are the links between nodes.\n\nnodes, edges = np.array([0, 1, 2]), [np.array([0, 1]), np.array([0, 2])]\ng = Graph(nodes, edges)\n\n\nassert (g.nodes == nodes).all()\nassert np.all([e_graph == e for e_graph, e in zip(g.edges, edges)])\n\nFurthermore, Graphs provide some useful properties that provide useful information about the graph.\n\nassert g.n_nodes == len(nodes)\nassert g.n_edges == len(edges)\n\nWe have predefined some common graph structures in typical physical systems, such as a one-dimensional chain or a two-dimensional square lattice, both with or without periodic boundary conditions (denoted as pbc). However, notice that implementing graphs allows us to tackle problems with arbitrary dimensions and connectivities.\n\nsource\n\n\n\n\n Chain1D (N, pbc=True)\n\nCreate a one-dimensional chain of N nodes and periodic boundary conditions pbc.\n\nchain = Chain1D(6)\nchain.nodes, chain.edges\n\n(array([0, 1, 2, 3, 4, 5]),\n [array([0, 1]),\n  array([1, 2]),\n  array([2, 3]),\n  array([3, 4]),\n  array([4, 5]),\n  array([0, 5])])\n\n\n\nassert (chain.nodes == np.arange(6)).all()\nedges = [np.sort([i, (i+1)%6]) for i in range(6)]\nassert np.all([e_chain == e for e_chain, e in zip(chain.edges, edges)])\n\n\nsource\n\n\n\n\n SquareLattice (N, M, pbc=True)\n\nCreate a NxM square lattice with periodic boundary conditions pbc.\n\nlattice = SquareLattice(3, 4, pbc=False)\n\n\nlattice.edges\n\n[array([0, 1]),\n array([0, 4]),\n array([1, 2]),\n array([1, 5]),\n array([2, 3]),\n array([2, 6]),\n array([3, 7]),\n array([4, 5]),\n array([4, 8]),\n array([5, 6]),\n array([5, 9]),\n array([6, 7]),\n array([ 6, 10]),\n array([ 7, 11]),\n array([8, 9]),\n array([ 9, 10]),\n array([10, 11])]"
  },
  {
    "objectID": "source/hamiltonian.html#local-hamiltonians",
    "href": "source/hamiltonian.html#local-hamiltonians",
    "title": "Hamiltonian",
    "section": "Local Hamiltonians",
    "text": "Local Hamiltonians\nFor now, there are issues pickling some picos instances, which we use to define the interactions (see serializing below). We use the LocalHamiltonian class to define a way to pickle the Hamiltonian class assuming 2-local Hamiltonians, which encompasses most physically relevant Hamiltonians.\n\nsource\n\nLocalHamiltonian\n\n LocalHamiltonian (graph, interactions, *aux_args)\n\nLocal Hamiltonian with two-body interactions. This class defines a structure for 2-local Hamiltonians of the form cls(graph, one_body_amplitudes, two_body_amplitudes, *aux_args). This allows us to serialize Hamiltonians with the reduce method, which we do not want to explicitly define for every new Hamiltonian. Let us know if you find any better solution to this.\nLet’s see how to implement a transverse field Ising Hamiltonian and instantiate it in the onedimensional chain.\n\nclass IsingHamiltonian(LocalHamiltonian):\n    def __init__(self, graph, field, coupling):\n        field, coupling = arrayfy(field, graph.n_nodes), arrayfy(coupling, graph.n_edges)\n        one_body_terms = [(np.array([n]), field[n]*self.x) for n in graph.nodes]\n        two_body_terms = [(edge, coupling[e]*self.z@self.z) for e, edge in enumerate(graph.edges)]\n        interactions = one_body_terms + two_body_terms\n        super().__init__(graph, interactions)\n        self.name = 'ising'\n\n\nsource\n\n\nIsingHamiltonian\n\n IsingHamiltonian (graph, field, coupling)\n\nLocal Hamiltonian with two-body interactions. This class defines a structure for 2-local Hamiltonians of the form cls(graph, one_body_amplitudes, two_body_amplitudes, *aux_args). This allows us to serialize Hamiltonians with the reduce method, which we do not want to explicitly define for every new Hamiltonian. Let us know if you find any better solution to this.\n\nb, j = 0.75, 2.5\nH_ising = IsingHamiltonian(chain, b, j)\n\n\nH_ising.interactions\n\n[(array([0]), <2×2 Real Constant: 0.75·x>),\n (array([1]), <2×2 Real Constant: 0.75·x>),\n (array([2]), <2×2 Real Constant: 0.75·x>),\n (array([3]), <2×2 Real Constant: 0.75·x>),\n (array([4]), <2×2 Real Constant: 0.75·x>),\n (array([5]), <2×2 Real Constant: 0.75·x>),\n (array([0, 1]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([1, 2]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([2, 3]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([3, 4]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([4, 5]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([0, 5]), <4×4 Real Constant: 2.5·z⊗z>)]\n\n\n\nH_ising.to_sdp()\n\n[(array([0]), <2×2 Real Constant: 0.75·x>),\n (array([1]), <2×2 Real Constant: 0.75·x>),\n (array([2]), <2×2 Real Constant: 0.75·x>),\n (array([3]), <2×2 Real Constant: 0.75·x>),\n (array([4]), <2×2 Real Constant: 0.75·x>),\n (array([5]), <2×2 Real Constant: 0.75·x>),\n (array([0, 1]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([1, 2]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([2, 3]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([3, 4]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([4, 5]), <4×4 Real Constant: 2.5·z⊗z>),\n (array([0, 5]), <4×4 Real Constant: 2.5·z⊗z>)]\n\n\n\nassert H_ising.to_sdp() == H_ising.interactions"
  },
  {
    "objectID": "source/hamiltonian.html#visualization",
    "href": "source/hamiltonian.html#visualization",
    "title": "Hamiltonian",
    "section": "Visualization",
    "text": "Visualization\nFurthermore, we have provided the Hamiltonian class with a draw method. This provides us with a pictorial representation of the Hamiltonian. We represent the interaction strength with the edge transparency, and the on-site terms with the node colors. The scales are between the smallest to the largest value, so they provide an idea of the relative strengths.\nWe have given the different graphs a layout method that determines the position of the nodes in the plot. You do not need to define a specific layout in your own graphs, as the function will use the networkx.kamada_kawai_layout by default (mainly because the name is cool). For example, we have given a circular layout to the Chain1D using the predefined with networkx.circular_layout.\nLet’s see how it looks in our example Ising Hamiltonian.\n\nH_ising.draw()\n\n\n\n\nIn the case of a lattice, we have created our own custom a square-lattice-like pattern which adapts to periodic boundary conditions. Let’s first see how it looks with inhomogeneous parameters and without periodic boundary conditions.\n\nb, j = [0., 0.5, 1.], [0., 0.75, 1.5]\nH_ising = IsingHamiltonian(lattice, b, j)\n\n\nH_ising.draw()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice here that we do not explicitly define the values of the interacting terms for every node and edge. The Hamiltonian takes care to replicate the pattern along the nodes and edges by order. The behaviour is consistent, although we do not recommend using this property unless you are confident it works as expected. It is mainly intended to perform simple patterns like homogeneous Hamiltonians, such as the one we defined before, or alternating exchange in one dimension.\n\n\nIn case you want to exploit it, we recommend paying close attention to the edge ordering. In the Chain1D the edges go in increasing order. In the SquareLattice, we sequentially parse the lattice rows from left to right, and top to bottom. We start at the first node and create two edges: the horizontal towards the right and the vertical downwards. This takes into account periodic boundary conditions.\nLet’s see now a lattice with periodic boundary conditions! (We’re proud of this plotting :3)\n\nlattice = SquareLattice(3, 4)\nb, j = [0., 0.5, 1.], [0.1, 0.75, 1.5]\nH_ising = IsingHamiltonian(lattice, b, j)\nH_ising.draw()"
  },
  {
    "objectID": "source/hamiltonian.html#heisenberg-models",
    "href": "source/hamiltonian.html#heisenberg-models",
    "title": "Hamiltonian",
    "section": "Heisenberg models",
    "text": "Heisenberg models\nIn the presented work, we study a special case of the Heisenberg XY model. The quantum Heisenberg XY model is given by \\[H=\\sum_{i} J_{i}((1+\\gamma)\\sigma_{i}^x \\sigma_{i+1}^x + (1-\\gamma)\\sigma_{i}^y \\sigma_{i+1}^y) + \\sum_{i}B_{i}\\sigma_{i}^z,\\] where \\(J_i\\) is the pair-wise antiferromagnetic exchange and \\(B_i\\) is the strength of the on-site magnetic field. The parameter \\(\\gamma\\) induces an anisotropic interaction between directions \\(x\\) and \\(y\\). In the limit of \\(\\gamma=0\\), we have the so-called XX model \\[H=\\sum_{i} J_{i}(\\sigma_{i}^x \\sigma_{i+1}^x + \\sigma_{i}^y \\sigma_{i+1}^y) + \\sum_{i}B_{i}\\sigma_{i}^z\\,,\\] whose results we report on the work Certificates of quantum many-body properties assisted by machine learning.\n\nsource\n\nXXHamiltonian\n\n XXHamiltonian (graph, linear, quadratic)\n\nLocal Hamiltonian with two-body interactions. This class defines a structure for 2-local Hamiltonians of the form cls(graph, one_body_amplitudes, two_body_amplitudes, *aux_args). This allows us to serialize Hamiltonians with the reduce method, which we do not want to explicitly define for every new Hamiltonian. Let us know if you find any better solution to this.\n\nsource\n\n\nXYHamiltonian\n\n XYHamiltonian (graph, linear, quadratic, gamma)\n\nLocal Hamiltonian with two-body interactions. This class defines a structure for 2-local Hamiltonians of the form cls(graph, one_body_amplitudes, two_body_amplitudes, *aux_args). This allows us to serialize Hamiltonians with the reduce method, which we do not want to explicitly define for every new Hamiltonian. Let us know if you find any better solution to this.\n\nxy = XYHamiltonian(chain, 0.5, 1., 0.2)\nxy.interactions\n\n[(array([0]), <2×2 Real Constant: 0.5·z>),\n (array([1]), <2×2 Real Constant: 0.5·z>),\n (array([2]), <2×2 Real Constant: 0.5·z>),\n (array([3]), <2×2 Real Constant: 0.5·z>),\n (array([4]), <2×2 Real Constant: 0.5·z>),\n (array([5]), <2×2 Real Constant: 0.5·z>),\n (array([0, 1]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>),\n (array([1, 2]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>),\n (array([2, 3]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>),\n (array([3, 4]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>),\n (array([4, 5]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>),\n (array([0, 5]), <4×4 Complex Constant: 1.2·x⊗x + 0.8·y⊗y>)]\n\n\n\nxx = XXHamiltonian(chain, 0.5, 1.)\nxx.interactions\n\n[(array([0]), <2×2 Real Constant: 0.5·z>),\n (array([1]), <2×2 Real Constant: 0.5·z>),\n (array([2]), <2×2 Real Constant: 0.5·z>),\n (array([3]), <2×2 Real Constant: 0.5·z>),\n (array([4]), <2×2 Real Constant: 0.5·z>),\n (array([5]), <2×2 Real Constant: 0.5·z>),\n (array([0, 1]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([1, 2]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([2, 3]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([3, 4]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([4, 5]), <4×4 Complex Constant: x⊗x + y⊗y>),\n (array([0, 5]), <4×4 Complex Constant: x⊗x + y⊗y>)]\n\n\nFor completeness, we provide an additional example of the Heisenberg Hamiltonian including interaction in the \\(\\sigma^z\\) direction \\[H=\\sum_i J_i(\\sigma_{i}^x\\sigma_{i+1}^x + \\sigma_{i}^y\\sigma_{i+1}^y + \\sigma_{i}^z\\sigma_{i+1}^z) + \\sum_i B_i\\sigma_i^z\\] This model, with \\(B_i=0 \\ \\forall i\\), is used in [1]. See the SdP docs for further details.\n\nsource\n\n\nXYZHamiltonian\n\n XYZHamiltonian (graph, linear, quadratic)\n\nLocal Hamiltonian with two-body interactions. This class defines a structure for 2-local Hamiltonians of the form cls(graph, one_body_amplitudes, two_body_amplitudes, *aux_args). This allows us to serialize Hamiltonians with the reduce method, which we do not want to explicitly define for every new Hamiltonian. Let us know if you find any better solution to this.\n\nxyz = XYZHamiltonian(chain, 0.5, 1.5)\nxyz.interactions\n\n[(array([0]), <2×2 Real Constant: 0.5·z>),\n (array([1]), <2×2 Real Constant: 0.5·z>),\n (array([2]), <2×2 Real Constant: 0.5·z>),\n (array([3]), <2×2 Real Constant: 0.5·z>),\n (array([4]), <2×2 Real Constant: 0.5·z>),\n (array([5]), <2×2 Real Constant: 0.5·z>),\n (array([0, 1]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>),\n (array([1, 2]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>),\n (array([2, 3]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>),\n (array([3, 4]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>),\n (array([4, 5]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>),\n (array([0, 5]), <4×4 Complex Constant: 1.5·(x⊗x + y⊗y + z⊗z)>)]"
  },
  {
    "objectID": "source/utils.html",
    "href": "source/utils.html",
    "title": "Utils",
    "section": "",
    "text": "Training utilities\n\nsource\n\nplot_exploration\n\n plot_exploration (exploration_results, expl_optims=True,\n                   highlight_max=False, figsize=(10, 5), fontsize=18,\n                   ticksize=14)\n\nPlots exploration results. It shows the best bound obtained for every new visited state. If expl_optims=True, it also shows the proximity to the optimal relaxation.\n\nsource\n\n\nbest_so_far\n\n best_so_far (m)\n\nReturns best value up to each point along the second axis of a matrix for each row.\n\nsource\n\n\narrange_shape\n\n arrange_shape (obj, mode='expand')\n\nMakes all lists within a list have the same length. Can be mode ‘cut’ or ‘expand’\n\nsource\n\n\nplot_trainings\n\n plot_trainings (training_results, eval_optims=True, expl_optims=False,\n                 figsize=(10, 5), fontsize=18, ticksize=14)\n\nPlots the training results.\n\nsource\n\n\nget_indiv_times\n\n get_indiv_times (tl_eval, convergence_crit=None)\n\nProvides convergence times from individual ratios.\n\nsource\n\n\nindiv_convergence_time\n\n indiv_convergence_time (results, max_epochs=800, tol=0.0005, T=50,\n                         t_avg=20, return_diffs=False)\n\nSimilar to convergence_time but with each agent.\n\nsource\n\n\nconvergence_time\n\n convergence_time (results, tol=0.0005, T=50, t_avg=20,\n                   return_diffs=False)\n\nReturns the convergence with criterion of not changing result by tol for T epochs.\n\n\n\nLoad and save data\n\nsource\n\nCPU_Unpickler\n\n CPU_Unpickler (file, fix_imports=True, encoding='ASCII', errors='strict',\n                buffers=())\n\nUnpickles from GPU to CPU\n\nsource\n\n\ncheckpoint2results\n\n checkpoint2results (N, model, budget=None, IDs=None, episode=None)\n\nLoads checkpoints from IDs and converts them to result format.\n\nsource\n\n\nload_checkpoint\n\n load_checkpoint (problem, budget, ID, episode)\n\n\nsource\n\n\nload_benchmark\n\n load_benchmark (problem, budget, method, suffix=None)\n\nLoads benchmark data.\n\nsource\n\n\nsave_benchmark\n\n save_benchmark (benchmark, problem, budget, method, suffix=None)\n\nSaves benchmark data.\n\nsource\n\n\nload_model\n\n load_model (problem, budget, ID)\n\nLoads agent model and parameters.\n\nsource\n\n\nsave_model\n\n save_model (agent, problem, budget, ID)\n\nSaves agent model and parameters.\n\nsource\n\n\nget_checkpoint_file_name\n\n get_checkpoint_file_name (problem, budget, ID, episode)\n\nReturns a consistent file naming for checkpoints.\n\nsource\n\n\nget_benchmark_file_name\n\n get_benchmark_file_name (problem, budget, method)\n\nReturns a consistent file naming for benchmarks.\n\nsource\n\n\nget_memory_file_name\n\n get_memory_file_name (problem, solver)\n\nReturns a consistent file naming for memories.\n\nsource\n\n\nget_agent_file_name\n\n get_agent_file_name (problem, budget, ID)\n\nReturns a consistent file naming for agents.\n\nsource\n\n\nget_problem_file_name\n\n get_problem_file_name (problem)\n\nReturns a consistent file naming from a problem instance.\n\n\n\nState transformations\n\nsource\n\naction_mask\n\n action_mask (state, basis)\n\nMask of the actions that can be performed. Better to use SdPEnvironment.action_mask if possible, it’s much faster.\n\nsource\n\n\ncontained_constraints\n\n contained_constraints (state, basis)\n\nReturns a mask indicating which constraints in the state are contained by larger elements of the basis in the same state, excluding themselves. Better to use the SdPEnvironment.contained_constraints method when possible (it’s faster).\n\nsource\n\n\nadd_subgraph_size\n\n add_subgraph_size (subgraphs)\n\nTakes a list of subgraphs up to any given size and adds a sublist with all the possible subgraphs of a larger size. It assumes the graph edges as minimal graph size.\n\nsource\n\n\nflip\n\n flip (state, i)\n\nFlips constraint i from state.\n\nsource\n\n\nT\n\n T (state)\n\nTakes state and converts it to torch.FloatTensor\n\nsource\n\n\nstate2str\n\n state2str (state)\n\nTakes state vector and outputs a (spaceless) string.\n\nsource\n\n\nstate2int\n\n state2int (state)\n\nTakes state vector as binary encoding of an integer.\n\nsource\n\n\nfill_layout\n\n fill_layout (L, N)\n\nFills layout with single-body terms that are missing.\n\nsource\n\n\nsimplify_layout\n\n simplify_layout (L)\n\nSimplifies the given layout of the form [np.array([0, 1]), np.array([0, 1, 2])].\n\n\n\nArray operations\n\nsource\n\narrayfy\n\n arrayfy (inp, size)\n\nTransforms input into an array of the desired size.\n\nsource\n\n\narray_in_list\n\n array_in_list (array, array_list)\n\nChecks whether an array is in a list of arrays.\n\n\n\nMathematical operations\n\nsource\n\nbinomial\n\n binomial (n, k)\n\n\nsource\n\n\ndist_poly\n\n dist_poly (x, xmin, xmax, d)\n\nComputes a linear decay bound from 1 to 0 given a minimum and maximum value (in terms of absolute value). Exponentiate the line to get better shapes\n\nsource\n\n\ndist_exp\n\n dist_exp (xmin, x)\n\nComputes an eponential distance given a minimum value (in terms of absolute value)."
  },
  {
    "objectID": "source/environment.html",
    "href": "source/environment.html",
    "title": "Environment",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "source/environment.html#exploration-boundaries-and-errors",
    "href": "source/environment.html#exploration-boundaries-and-errors",
    "title": "Environment",
    "section": "Exploration boundaries and errors",
    "text": "Exploration boundaries and errors\nIf we were to exceed the allowed computational budget with an action, we would obtain an error indicator. The erorr code is: - 0: all good! - 1: there could not be found a solution for the associated SdP - 2: the associated SdP cost exceeds the computational budget\n\nenv.perform_action(N+3)\nnew_state, bound, cost, err = env.perform_action(N+4)\nenv.show_constraints()\n\n2: [1 1 1 1 1]\n3: [0 0 0 1 1]\n\n\n\nerr, cost, budget\n\n(2, 132, 100)\n\n\n\nassert err == 2\nassert cost > budget\n\nErrors are the way through which we establish “walls” in the state space. This way, agents do not need to learn the boundaries of the feasible region nor need to keep track of it.\nThe environment prevents threspassing the state space boundaries with the step method. Rather than taking a single action, such as the perform_action method.\n\nsource\n\nSdPEnvironment.step\n\n SdPEnvironment.step (actions)\n\nReceives a list of actions (priority ordered) and executes them until one succeeds. Input: - actions: collection of actions (iterable of ints). Output: - next_state: new state after performing the chosen action. - action: action that was actually performed among the input ones. - bound: bound associated to the new state. - cost: cost associated to the new state. - err: error code (should be zero).\nThe step method takes a list of priority-ordered actions. The method performes them in the given order until one succeeds, ensuring that we never land on an error-flagged state.\nIn order to test this out, let’s first reset the environment to the initial state.\n\nenv.reset()\nenv.show_constraints()\n\n2: [0 0 0 0 0]\n3: [0 0 0 0 0]\n\n\n\nnew_state, action, bound, cost, error = env.step([5, 6, 7, 0, 1])\nprint(f\"The performed action was {action}.\")\nenv.show_constraints()\n\nThe performed action was 5.\n2: [1 1 0 0 0]\n3: [1 0 0 0 0]\n\n\n\nassert action == 5\nassert error == 0\n\nIf we now try to add an additional large constraint, we will exceed the computational budget. However, the step method will perform the first suitable action.\n\nactions_to_try = [8, 6, 3, 5]\nnew_state, action, bound, cost, error = env.step(actions_to_try)\nprint(f\"The performed action was {action}.\")\nenv.show_constraints()\n\nThe performed action was 3.\n2: [1 1 0 1 0]\n3: [1 0 0 0 0]\n\n\n\nerror\n\n0\n\n\n\nassert action == 3\n\nThe step method has skipped the first two actions in the priority list to avoid exceeding the budget. In the extreme case in which none of the actions provided can be executed, the function returns the resutls for the last action it tried, although the environment has not advanced. This should never happen if we provide all possible actions ranked and the state-space is properly designed. Intuitively, there should never be rabbit holes the agent can’t escape from."
  },
  {
    "objectID": "source/agents.html",
    "href": "source/agents.html",
    "title": "Agents",
    "section": "",
    "text": "Reinforcement learning\nThe agents based on reinforcement learning implement a value-based algorithm called Q-learning. More precisely, the agent implemented in this framework is based on deep double Q-learning.\n\nsource\n\nDQNAgent\n\n DQNAgent (model, learning_rate=0.001, criterion=None, optimizer=None,\n           batch_size=128, target_update=5, gamma=0.85, eps_0=1,\n           eps_decay=0.999, eps_min=0.1)\n\nAgent based on a deep Q-Network (DQN): Input: - model: torch.nn.Module with the DQN model. Dimensions must be consistent - criterion: loss criterion (e.g., torch.nn.SmoothL1Loss) - optimizer: optimization algorithm (e.g., torch.nn.Adam) - eps_0: initial epsilon value for an epsilon-greedy policy - eps_decay: exponential decay factor for epsilon in the epsilon-greedy policy - eps_min: minimum saturation value for epsilon - gamma: future reward discount factor for Q-value estimation\nWe provide a default architecture for the neural network that encodes the Q-values, usually referred to as deep Q-Network (DQN).\n\nsource\n\n\nDQN\n\n DQN (state_size, action_size)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nBlind-search\nThe agents based on tree search currently only implement blind-search techniques, such as breadth first search.\n\nsource\n\nBrFSAgent\n\n BrFSAgent (initial_state)\n\nAgent based on Breadth First Search (BrFS).\n\nagent = BrFSAgent(np.array([1, 1, 1, 0, 0, 0]))\n\n\nagent.expand()\n\n[array([0, 1, 1, 0, 0, 0]),\n array([1, 0, 1, 0, 0, 0]),\n array([1, 1, 0, 0, 0, 0]),\n array([1, 1, 1, 1, 0, 0]),\n array([1, 1, 1, 0, 1, 0]),\n array([1, 1, 1, 0, 0, 1])]\n\n\n\n\n\nMonte-Carlo\nThe agents based on Monte-Carlo sampling follow the Metropolis-Hastings algorithm to move between states. A random action (new state) is proposed and the move is accepted or rejected with a certain probability.\n\nsource\n\nMCAgent\n\n MCAgent (beta=0.1)\n\nInitialize self. See help(type(self)) for accurate signature."
  }
]