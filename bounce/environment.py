# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/source/00_environment.ipynb.

# %% auto 0
__all__ = ['SdPEnvironment']

# %% ../nbs/source/00_environment.ipynb 1
import numpy as np
import torch
from copy import deepcopy
from pathlib import Path
import pickle

from .utils import (state2int, state2str, add_subgraph_size, simplify_layout, 
                          fill_layout, dist_poly, get_memory_file_name)

# %% ../nbs/source/00_environment.ipynb 4
class SdPEnvironment:
    "Environment for constraint-space exploration."
    
    def __init__(self, problem, sdp_solver, budget, reward_criterion="bound_norm", 
                 layout_basis=None, max_basis_size=None, initial_size=1, bound_tol=1e-3):
        
        self.N = problem.graph.n_nodes
        self.problem = problem
        self.solver = sdp_solver
        self.budget = budget
        
        # Agent state-action basis         
        self.layout_basis = (self._get_layout_basis(max_basis_size) 
                             if layout_basis is None else layout_basis)
        self.contained_map = self._get_contained_map()
        self.basis_sizes = np.array([l.shape[0] for l in self.layout_basis])
        
        # Reward function
        self.reward_fun = getattr(self, reward_criterion+"_reward")
        self.dist_d = 5
        
        # Memory of visited states. It is a lookup table for computation speedup.
        self.memory_limit = 2e6
        self._get_memory()
        
        # Initialize the environment
        self.initial_size = initial_size
        self.reset()        
        
        # Memories           
        self.bound_tol = bound_tol
        self.max_bound = -np.inf   # Maximum bound ever obtained
        self.min_bound = np.inf    # Minimum bound ever obtained
        self.max_cost  = -np.inf   # Maximum amount of costs ever obtained
        self.min_cost  = np.inf    # Minimum amount of costs ever obtained
        self.best = np.array([-np.inf, np.inf, -np.inf])  # Maximum bound, best and worst cost
        self.best_layout = deepcopy(self.layout)
        
        # Initial state reference
        bound, cost, err = self.get_values()
        if not err: self._min_max_update(bound, cost)
        else:       raise ValueError(f"Something went wrong. Initial state {self.state} provides error.")
        
        
    def reset(self):       
        '''Resets the environment state to the simplest possible relaxation.'''
        self.state = np.zeros_like(self.layout_basis, dtype=int)
        self.state[self.basis_sizes == self.initial_size] = 1
        self._fill_contained()
        return self.state
    
    @property
    def layout(self):
        layout = simplify_layout([np.array(sites) 
                                  for sites in self.layout_basis[self.state.astype(bool)]])
        return fill_layout(layout, self.N)
    
    @property
    def bound(self): return self.get_values()[0]
            
    def show_constraints(self, state=None):
        if state is None: state = self.state       
        for size in np.unique(self.basis_sizes):
            print(f"{size}: {state[self.basis_sizes == size]}")
            
    ## agent - environment interaction ##
    def step(self, actions):
        """Receives a list of actions (priority ordered) and executes them until one succeeds.
        Input:  - actions: collection of actions (iterable of ints).
        Output: - next_state: new state after performing the chosen action.
                - action: action that was actually performed among the input ones.
                - bound: bound associated to the new state.
                - cost: cost associated to the new state.
                - err: error code (should be zero)."""
                
        state_0 = deepcopy(self.state)
        mask = self.action_mask()
        for a in actions:
            if mask[a]:
                next_state, bound, cost, err  = self.perform_action(a) # Try action
                if err: 
                    self.state = deepcopy(state_0)
                    _, _, err = self.get_values()
                    if err: raise Exception(f"Error found undoing an action. Returning from " +
                                            f"\n{next_state}\nto\n{self.state}\nwith action {a}." +
                                            "\nRef state\n{state_0}")
                else: 
                    break 
        return next_state, a, bound, cost, err
    
    def perform_action(self, action):
        ''' Perform action over the current state and returns the resulting associated values.
        Inputs: - action: integer indicating the index of the state to be flipped
        Outputs: - Resulting state
                 - Solution of the associated SdP:
                    - Resulting bound
                    - Associated problem cost
                    - Error code'''   
        
        if action < len(self.state):   self.state[action] = -self.state[action] + 1
        elif action > len(self.state): raise ValueError(f"Action {action} exceeds state" +
                                                        f" size {len(self.state)}")
        # Case that action == len(self.state) the action is to remain in the current state
        
        self._fill_contained() # Include the smaller contained constraints
        bound, cost, err = self.get_values() # Calculate the features
        if not err: self._min_max_update(bound, cost)         
                
        return self.state, bound, cost, err  
    
    ## SdP results ## 
    def get_values(self):
        "Solve the associated SdP to the state and return the results."
        binary = state2int(self.state)
        if binary in self.memory.keys():
            bound, cost, err = self._remember(binary)
            bound, cost, err = self._check_current_limit(binary, bound, cost, err)  
        else:
            bound, cost, err = self._solve_sdp()
            if len(self.memory) < self.memory_limit:
                self._memorize(binary, [bound, cost, err])

        return bound, cost, err
    
    def _solve_sdp(self):
        "Solves the associated SdP to the current sate and returns the output."
        cost = self.solver.ojimetro(self.layout)
        if cost > self.budget: 
            bound = None
            err = 2
        else:
            bound = self.solver.solve(self.problem.to_sdp(), self.layout)
            if bound == None: err = 1
            else:             err = 0
        return bound, cost, err
    
    def _check_current_limit(self, binary, bound, cost, err):
        "Checks whether pre-computed results fit in the current conditions."
        if not err and cost > self.budget:
            # Pre-computed costs are larger than current limit
            err, bound = 2, 0.
        elif err == 2 and cost <= self.budget or err==1: 
            # If the error was due to excess of costs but it fits now, recompute the SdP
            bound, cost, err = self._solve_sdp()
            self._memorize(binary, [bound, cost, err])
                
        return bound, cost, err

    def _min_max_update(self, bound, cost):
        """Given a set of bound and cost, compares them to the previous max and min references
        and updates them accordingly."""
        if cost < self.min_cost:   self.min_cost  = cost
        if cost > self.max_cost:   self.max_cost  = cost
        if bound < self.min_bound: self.min_bound = bound
        if bound > self.max_bound: self.max_bound = bound
        
        # Recall in self.best we have [best_bound, best_cost, worst_cost]    
        if bound > self.best[0] and np.abs(bound-self.best[0]) > self.bound_tol:
            # If bound beyond threshold, keep it all
            self.best = np.array([bound, cost, cost])
            self.best_layout = deepcopy(self.layout)
            
        elif np.abs(bound-self.best[0]) < self.bound_tol:
            # If bound within threshold
            if   cost < self.best[1]: 
                self.best[0], self.best[1] = bound, cost 
                self.best_layout = deepcopy(self.layout)
            elif cost > self.best[2]: 
                self.best[2] = cost
    
    ## State and action space methods ##
    def contained_constraints(self, state=None):
        "Returns an array indicating which constraints are contained by larger ones in the state."
        state = self.state if state is None else state
        return self.contained_map[state.astype(bool)].sum(0).astype(bool)
    
    def action_mask(self, state=None):
        "Returns a boolean mask indicating which actions can be performed in the current state."
        return np.concatenate((~self.contained_constraints(state), np.array([True])))
    
    def _fill_contained(self):
        "Fills state vector to account for the constraints contained in larger ones."
        self.state[self.contained_constraints()] = 1
        
    def _get_layout_basis(self, max_size=None):
        "Builds layout basis relating the physical constraints with the state vector."
        graph = self.problem.graph
        subgraphs = [graph.edges]
        constr_cost = self.solver.ojimetro(fill_layout([subgraphs[-1][0]], graph.n_nodes))
        while constr_cost <= self.budget:
            subgraphs = add_subgraph_size(subgraphs)
            constr_cost = self.solver.ojimetro(fill_layout([subgraphs[-1][0]], graph.n_nodes))
            if len(subgraphs[-1]) == 1: break
            if len(subgraphs[-1][0]) == max_size: break
        
        basis = [c for sub in (subgraphs if constr_cost <= self.budget else subgraphs[:-1])
                 for c in sub]
        if len(basis) == 0: 
            raise ValueError(f"Unable to fit a single 2-body constraint with cost {constr_cost} " +
                             f"for budget {self.budget}. We'll need beefier computers for this!")
        return np.array(basis, dtype=object)
    
    def _get_contained_map(self):
        """Builds a map indicating which basis elements are contained into larger ones excluding
        themselves. The map is a matrix such that `map[state.astype(bool)].sum(0)` returns the 
        boolean indicators of the contained elements."""
        size = len(self.layout_basis)
        contained_map = np.zeros((size, size), dtype=bool)
        for i, big_supp in enumerate(self.layout_basis):
            for j, small_supp in enumerate(self.layout_basis[:i]):
                common = np.intersect1d(big_supp, small_supp)
                if len(common) == len(small_supp):
                    contained_map[i, j] = True
                    
        return contained_map
    
    ## Reward functions ##
    def bound_reward(self, bounds, costs, best_ref=None):
        "The reward is the bound of the state."
        best = self.best if best_ref is None else best_ref
        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol
        reward = deepcopy(bounds)
        reward[bounds == 0] = self.min_bound*1.1                 # Errors
        reward[costs > self.budget] = self.min_bound*1.1         # Over parameter limit
        reward[thresh_mask] = costs[thresh_mask]/best[2].float() # Reweight threshold states
        return reward
    
    def bound_norm_reward(self, bounds, costs, best_ref=None):
        "The reward is a normalized function from 0 to 1 as function of the bound and cost."
        best = self.best if best_ref is None else best_ref
        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol
        reward = dist_poly(deepcopy(bounds), best[0], self.min_bound, d=self.dist_d)
        reward[bounds == 0] = 0                                  # Errors
        reward[costs > self.budget] = 0                          # Over parameter limit
        reward[thresh_mask] = best[2]/costs[thresh_mask].float() # Reweight threshold states
        return reward*best[1]/best[2]
    
    def bound_improve_reward(self, bounds, costs, best_ref=None):
        "The reward is the bound improvement (+1) with respect to the minimum bound."
        best = self.best if best_ref is None else best_ref
        thresh_mask = torch.abs(bounds - best[0]) < self.bound_tol
        reward = bounds - self.min_bound + 1
        reward[bounds == 0] = 0                                  # Errors
        reward[costs > self.budget] = 0                          # Over parameter limit
        reward[thresh_mask] = best[2]/costs[thresh_mask].float() # Reweight threshold states
        return reward               
         
    ## Memory methods ##
    def save_memory(self):
        old_memory = self._read_memory()
        full_memory = {**old_memory, **self.memory}
        with open(self.memory_path, "wb") as f:
            pickle.dump(full_memory, f, protocol=pickle.HIGHEST_PROTOCOL)
        self.memory = self._read_memory()
        
    def _get_memory(self):
        "Reads the corresponding memory file"
        memory_dir = Path("../memories/")
        memory_dir.mkdir(exist_ok=True)
        self.memory_path = memory_dir/f"{get_memory_file_name(self.problem, self.solver)}.pkl" 
        self.memory = self._read_memory()

    def _memorize(self, state_idx, values):
        "Add to memory the state visited (associated binary index) and the values of the SdP."
        bound, cost, err = values

        if state_idx in self.memory.keys() and cost > self.budget and err != 2:
            _, _, old_err = self._remember(state_idx)
            if old_err != 1:
                raise Exception(f"Trying to memorize constraint with binary index {state_idx}" +
                                " already in memory")
        elif not isinstance(state_idx, int):
            raise ValueError(f"Constraint is not a binary integer {state_idx}")
        else:
            self.memory[state_idx] = values       
    
    def _remember(self, state_idx):
        "Recalls the results values associated to a previously visited state."         
        return self.memory[state_idx]     
    
    def _read_memory(self):
        "Reads the memory corresponding to the environemnt's current problem."
        try:
            with open(self.memory_path, "rb") as f:
                memory = pickle.load(f)
        except: memory = {}
        return memory
