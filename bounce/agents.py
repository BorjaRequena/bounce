# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_agents.ipynb (unless otherwise specified).

__all__ = ['DQNAgent', 'DQN', 'BrFSAgent', 'MCAgent']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from collections import deque, namedtuple
import random
from copy import deepcopy
from .utils import state2int, state2str, state_in_list, flip
from .utils import action_mask, contained_constraints, T

# Cell
class DQNAgent:
    def __init__(self, N, model, learning_rate=1e-3, criterion=None, optimizer=None, batch_size=120,
                 target_update=5, gamma=0.85, eps_0=1, eps_decay=0.999, eps_min=0.1):
        """Agent based on a deep Q-Network (DQN):
        On input:
            - N: Number of parties to consider
            - model: torch.nn.Module with the DQN model. Dimensions must be consistent
            - criterion: loss criterion (e.g., torch.nn.SmoothL1Loss)
            - optimizer: optimization algorithm (e.g., torch.nn.Adam)
            - eps_0: initial epsilon value for an epsilon-greedy policy
            - eps_decay: exponential decay factor for epsilon in the epsilon-greedy policy
            - eps_min: minimum saturation value for epsilon
            - gamma: future reward discount factor for Q-value estimation"""

        self.N = N

        # Model
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.model = model.to(self.device)
        self._build_target_net()
        self.target_update = target_update

        # Parameters
        self.lr = learning_rate
        self.gamma = gamma    # discount factor
        self.epsilon, self.epsilon_min, self.epsilon_decay = eps_0, eps_min, eps_decay
        self._get_criterion(criterion)
        self._get_optimizer(optimizer)
        self.batch_size = batch_size
        self.memory = deque(maxlen=10000) # Replay memory
        self.Transition = namedtuple('Transition', ('state', 'action', 'energy', 'params', 'next_state'))

    def try_actions(self, state):
        "Given a state, return ordered chosen actions by priority."
        mask = action_mask(state, self.N)

        if np.random.rand() <= self.epsilon:
            return np.random.permutation(np.where(mask==True)[0])
        else:
            with torch.no_grad():
                Q = self.q_values(state)
                Q[mask==False] = torch.min(Q) - 1. # Remove value from impossible actions
                return torch.argsort(Q, descending=True)[:sum(mask)]

    def q_values(self, state):
        "Returns the Q values of each action given a state."
        state = T(state).reshape(1, self.state_size).to(self.device)
        return self.model(state).squeeze()

    def replay(self, env):
        batch_size = min(len(self.memory), self.batch_size)
        transitions = random.sample(self.memory, batch_size)
#         transitions = random.sample(self.memory, self.batch_size)
        batch = self.Transition(*zip(*transitions))

        state_batch, action_batch, next_states = torch.cat(batch.state), torch.cat(batch.action), torch.cat(batch.next_state)
        energy_batch, param_batch = torch.cat(batch.energy), torch.cat(batch.params)
        reward_batch = env.reward_fun(energy_batch, param_batch)

        if torch.isnan(reward_batch).any():
            shit_idx = torch.isnan(reward_batch)
            raise ValueError(f"Shit was fooked, got nan in {next_states[shit_idx]}\nEnergy {energy_batch[shit_idx]}"+
                            f"\nParams {param_batch[shit_idx]}"+
                            f"\nReward {reward_batch[shit_idx]}"+
                            f"\nReward batch {reward_batch}")

        # Q-values
        state_action_values = self.model(state_batch).gather(1, action_batch.reshape(batch_size, 1))
        # Expected Q-values
        next_state_values = self.target_net(next_states).max(1)[0].detach()*self.gamma + reward_batch

        # Optimize the model
        self.optimizer.zero_grad()
        loss = self.criterion(state_action_values, next_state_values.unsqueeze(1))
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay
        if self.epsilon < self.epsilon_min: self.epsilon = self.epsilon_min

    def memorize(self, state, action, energy, params, next_state):
        """Remember a state-action-state-reward transition."""
        info = [torch.FloatTensor(state).reshape(1, self.state_size).to(self.device),
                torch.tensor([action], device = self.device),
    #                 torch.FloatTensor([reward]).to(self.device),
                torch.FloatTensor([energy]).to(self.device),
                torch.FloatTensor([params]).to(self.device),
                torch.FloatTensor(next_state).reshape(1, self.state_size).to(self.device)
                ]
        self.memory.append(self.Transition(*info))

    def act(self, state):
        """Take an action according to the epsilon-greedy policy"""
        mask = action_mask(state, self.N)  # Possible actions

        if np.random.rand() <= self.epsilon:
            return np.random.choice(np.where(mask==True)[0])
        else:
            with torch.no_grad():
                Q = self.q_values(state)
                Q[mask==False] = torch.min(Q) - 1. # remove value from impossible actions
                return int(torch.argmax(Q))

    def _build_target_net(self):
        model_params = list(self.model.parameters())
        self.state_size = model_params[0].size()[-1]
        self.action_size = model_params[-1].size()[0]
        self.target_net = self.model.__class__(self.state_size, self.action_size).to(self.device)
        self.target_net.load_state_dict(self.model.state_dict())
        self.target_net.eval()

    def _get_criterion(self, criterion=None):
        if criterion is None: self.criterion = nn.SmoothL1Loss(reduction='sum')
        else:                 self.criterion = criterion

    def _get_optimizer(self, optimizer=None):
        if optimizer is None: self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        else:                 self.optimizer = optimizer

# Cell
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super().__init__()
        self.fc1 = nn.Linear(state_size, 3*state_size)
        self.fc2 = nn.Linear(3*state_size, 2*action_size)
        self.fc3 = nn.Linear(2*action_size, 2*action_size)
        self.fc4 = nn.Linear(2*action_size, action_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Cell
class BrFSAgent:
    def __init__(self, N, initial_state):
        "Agent based on Breadth First Search (BrFS)."
        self.N = N
        self.state_size = len(initial_state)
        self.open = deque([initial_state])
        self.closed = set()

    def expand(self):
        "Expands the first node of the open"
        try :
            state = self.open.popleft()
            self.add_closed(state)
            state[contained_constraints(state, self.N)] = 1
            return np.random.permutation([flip(state, i) for i in range(self.state_size)])
        except:
            return []

    def in_open(self, state):
        "Boolean indicating whether state is in open"
        return state_in_list(state, self.open)

    def in_closed(self, state):
        "Boolean indicating whether state is in closed"
        return state2str(state) in self.closed

    def add_open(self, state):
        "Adds state to open"
        self.open.append(state)

    def add_closed(self, state):
        "Adds state to closed"
        self.closed.update([state2str(state)])

# Cell
class MCAgent:
    def __init__(self, N, beta=0.1):
        self.N = N
        self.beta = beta
        self.accepted = 0

    def try_actions(self, state):
        "Try random actions changing one constraint."
        mask = action_mask(state, self.N)[:-1]
        return np.random.permutation(np.where(mask==True)[0])

    def accept(self, r1, r2):
        "Boolean indicating whether to accept or not the movement from 1 to 2."
        a = np.random.random() <= min(1, np.exp((r2-r1)/self.beta))
        self.accepted += a
        return a